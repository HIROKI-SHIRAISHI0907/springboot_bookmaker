# -*- coding: utf-8 -*-
"""
Flashscore çµ‚äº†æ¸ˆï¼ˆRESULTSï¼‰è©¦åˆ -> midãƒªãƒ³ã‚¯çªåˆ -> (stats / summary meta / standings) ã‚’å–å¾—ã—ã€
1è©¦åˆ=1è¡Œã®CSVã‚’ S3 ã«é€æ¬¡ä¿å­˜ã™ã‚‹å…¨å‡¦ç†ç‰ˆï¼ˆæ–¹å¼Aï¼‰

âœ… æ–¹å¼Aï¼ˆæ¨å¥¨ï¼‰
- S3ã¯è¿½è¨˜ã§ããªã„ã®ã§ã€Œ1è¡Œ=1ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ã§ä¿å­˜
- å¤±æ•—ã—ã¦ã‚‚ä»–è¡Œã«å½±éŸ¿ãªã—
- SEQMAPï¼ˆè©¦åˆIDã”ã¨ã®é€šç•ªï¼‰ã‚‚S3ã«ä¿å­˜ã—ã¦æ°¸ç¶šåŒ–ï¼ˆECSã§ã‚‚é€£ç•ªãŒç¶šãï¼‰

âœ… é‡è¦ä»•æ§˜ï¼ˆã‚ãªãŸã®ãƒ©ã‚¤ãƒ–ç‰ˆã‚’è¸è¥²ï¼‰
- stats ã®å€¤ã¯ã€Œ34%ï¼ˆ31/90ï¼‰ã€ãªã©â€œãã®ã¾ã¾â€å–å¾—ï¼ˆåŠ å·¥ã—ãªã„ï¼‰
- "ã‚»ã‚¯ã‚·ãƒ§ãƒ³:ãƒ©ãƒ™ãƒ«" -> canonicalã‚­ãƒ¼ -> STAT_KEY_MAP -> HEADERåˆ—ã¸æŠ•å…¥
- VERIFYãƒ­ã‚°ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°ä¸€è‡´/æœªä¸€è‡´ã€rowåŸ‹ã¾ã‚ŠçŠ¶æ³ï¼‰ã‚ã‚Š

âœ… å¤‰æ›´ç‚¹ï¼ˆãƒ©ã‚¤ãƒ–ç‰ˆ â†’ çµ‚äº†æ¸ˆç‰ˆï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ãƒ©ã‚¤ãƒ–ã‹ã‚‰çµ‚äº†æ¸ˆï¼ˆRESULTSï¼‰ã¸å¤‰æ›´
- çµ‚äº†æ¸ˆè©¦åˆãƒšãƒ¼ã‚¸ã‚’é–‹ã„ãŸå¾Œã€ãƒšãƒ¼ã‚¸å†…ã® <a href*="mid="> ã‚’èµ°æŸ»ã—
  S3 JSONã® matchId(mid) ã«ä¸€è‡´ã™ã‚‹ãƒªãƒ³ã‚¯ã®ã¿ã€Œç›´æ¥é–‹ãã€
- ä¸€è‡´ã—ãªã„è©¦åˆã¯ã‚¹ã‚­ãƒƒãƒ—

æƒ³å®š
- ECS(Fargate) ã§å®Ÿè¡Œ
- Playwright(Chromium) ã¯ã‚³ãƒ³ãƒ†ãƒŠå†…
- ã‚¿ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§ S3 PutObject/GetObject æ¨©é™
"""

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
import time
import re
import os
import pickle
import datetime
import traceback
from urllib.parse import urlsplit, urlunsplit, urlparse, parse_qs
from typing import Optional, List, Dict, Tuple, Any
import multiprocessing as mp
import queue as pyqueue
import io
import csv
import json

import boto3
from botocore.exceptions import ClientError


# =========================
# Env helpers (ECS envã¯å…¨éƒ¨æ–‡å­—åˆ—)
# =========================
def env_bool(name: str, default: bool = True) -> bool:
    v = os.environ.get(name)
    if v is None:
        return default
    return v.strip().lower() in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int = 0) -> int:
    v = os.environ.get(name)
    if v is None or v.strip() == "":
        return default
    try:
        return int(float(v.strip()))
    except ValueError:
        return default

def env_float(name: str, default: float = 0.0) -> float:
    v = os.environ.get(name)
    if v is None or v.strip() == "":
        return default
    try:
        return float(v.strip())
    except ValueError:
        return default

def env_str(name: str, default: str = "") -> str:
    v = os.environ.get(name)
    return default if v is None else v


# =========================
# Settings
# =========================
FLASHCORE_URL = "https://www.flashscore.co.jp/"
TIMEZONE_ID   = "Asia/Tokyo"
LOCALE        = "ja-JP"
USER_AGENT    = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"

HEADLESS      = env_bool("HEADLESS", False)
SLOW_MO_MS    = env_float("SLOW_MO_MS", 0.0)

WORKER_TIMEOUT_SEC = env_int("WORKER_TIMEOUT_SEC", 220)  # å­ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ï¼ˆ1è©¦åˆï¼‰
NAV_TIMEOUT_MS     = env_int("NAV_TIMEOUT_MS", 25000)    # safe_goto 1st try
WAIT_TIMEOUT_MS    = env_int("WAIT_TIMEOUT_MS", 12000)   # wait_for_selector
MAX_DAY_STEPS = env_int("MAX_DAY_STEPS", 120)  # prev/next ã‚¯ãƒªãƒƒã‚¯æœ€å¤§å›æ•°

VERBOSE = env_bool("VERBOSE", True)

BOT_WALL_PAT = r"Just a moment|Access Denied|verify you are human|ãƒã‚§ãƒƒã‚¯|ç¢ºèª"

# S3 settingsï¼ˆæ–¹å¼Aï¼‰
S3_BUCKET_OUTPUTS = "aws-s3-outputs-csv"
S3_REGION         = "ap-northeast-1"
S3_PREFIX         = ""  # ä¾‹: "outputs/"ã€‚ãƒã‚±ãƒƒãƒˆç›´ä¸‹ãªã‚‰ç©ºã€‚

# SEQMAPï¼ˆS3ã«pickleä¿å­˜ï¼‰
SEQMAP_S3_KEY     = "seqmap/seqmap.pkl"

# 1è¡ŒCSVã«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å«ã‚ã‚‹ã‹ï¼ˆ1ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ=1è¡Œé‹ç”¨ãªã‚‰ True ãŒä¾¿åˆ©ï¼‰
INCLUDE_HEADER_IN_EACH_ROW = env_bool("INCLUDE_HEADER_IN_EACH_ROW", True)

# mp start methodï¼ˆLinuxã§spawnã«ã—ãŸã„å ´åˆã®ã¿ï¼‰
FORCE_SPAWN = env_bool("FORCE_SPAWN", True)

# RESULTSä¸€è¦§ã§ã€Œã‚‚ã£ã¨è¡¨ç¤ºã€æŠ¼ä¸‹å›æ•°ï¼ˆå¯¾è±¡midãŒå¤šã„ãªã‚‰å¢—ã‚„ã™ï¼‰
MAX_LOAD_MORE_CLICKS = env_int("MAX_LOAD_MORE_CLICKS", 25)

# JSONã«ã‚ã‚‹midã‚’å…¨éƒ¨è¦‹ã¤ã‘ãŸã‚‰åˆ—æŒ™ã‚’æ‰“ã¡åˆ‡ã‚‹ï¼ˆè¦ªå´ã§æ—©æœŸçµ‚äº†ï¼‰
STOP_WHEN_ALL_MIDS_FOUND = env_bool("STOP_WHEN_ALL_MIDS_FOUND", True)

def log(msg: str):
    if VERBOSE:
        print(msg, flush=True)


# =========================
# HEADERï¼ˆCSVåˆ—ï¼‰
# =========================
HEADER = [
    "ãƒ›ãƒ¼ãƒ é †ä½","è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª","è©¦åˆæ™‚é–“","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ","ãƒ›ãƒ¼ãƒ ã‚¹ã‚³ã‚¢","ã‚¢ã‚¦ã‚§ãƒ¼é †ä½","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ",
    "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ã‚³ã‚¢","ãƒ›ãƒ¼ãƒ æœŸå¾…å€¤","ã‚¢ã‚¦ã‚§ãƒ¼æœŸå¾…å€¤","ãƒ›ãƒ¼ãƒ æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤","ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤",
    "ãƒ›ãƒ¼ãƒ ãƒœãƒ¼ãƒ«æ”¯é…ç‡","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒ¼ãƒ«æ”¯é…ç‡","ãƒ›ãƒ¼ãƒ ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "ãƒ›ãƒ¼ãƒ æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ãƒ›ãƒ¼ãƒ æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "ãƒ›ãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ","ãƒ›ãƒ¼ãƒ ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹",
    "ãƒ›ãƒ¼ãƒ ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯","ã‚¢ã‚¦ã‚§ãƒ¼ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯","ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ",
    "ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ","ãƒ›ãƒ¼ãƒ ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ","ãƒ›ãƒ¼ãƒ ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«",
    "ãƒ›ãƒ¼ãƒ ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–","ã‚¢ã‚¦ã‚§ãƒ¼ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–","ãƒ›ãƒ¼ãƒ ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯",
    "ãƒ›ãƒ¼ãƒ ã‚ªãƒ•ã‚µã‚¤ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ã‚ªãƒ•ã‚µã‚¤ãƒ‰","ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¦ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¦ãƒ«",
    "ãƒ›ãƒ¼ãƒ ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰","ãƒ›ãƒ¼ãƒ ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰","ãƒ›ãƒ¼ãƒ ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³",
    "ãƒ›ãƒ¼ãƒ ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ","ã‚¢ã‚¦ã‚§ãƒ¼ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ","ãƒ›ãƒ¼ãƒ ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‘ã‚¹","ãƒ›ãƒ¼ãƒ ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹","ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹",
    "ãƒ›ãƒ¼ãƒ ã‚¯ãƒ­ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒ­ã‚¹","ãƒ›ãƒ¼ãƒ ã‚¿ãƒƒã‚¯ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¿ãƒƒã‚¯ãƒ«","ãƒ›ãƒ¼ãƒ ã‚¯ãƒªã‚¢","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒªã‚¢","ãƒ›ãƒ¼ãƒ ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°",
    "ãƒ›ãƒ¼ãƒ ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ",
    "ã‚¹ã‚³ã‚¢æ™‚é–“","å¤©æ°—","æ°—æ¸©","æ¹¿åº¦","å¯©åˆ¤å","ãƒ›ãƒ¼ãƒ ç›£ç£å","ã‚¢ã‚¦ã‚§ãƒ¼ç›£ç£å","ãƒ›ãƒ¼ãƒ ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³",
    "ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ","åå®¹äººæ•°","è¦³å®¢æ•°","å ´æ‰€","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…å‡ºå ´çŠ¶æ³","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…å‡ºå ´çŠ¶æ³",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹",
    "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹","é€šçŸ¥ãƒ•ãƒ©ã‚°","è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—","ã‚´ãƒ¼ãƒ«æ™‚é–“","é¸æ‰‹å","åˆ¤å®šçµæœ","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¹ã‚¿ã‚¤ãƒ«","ã‚¢ã‚¦ã‚§ã‚¤ãƒãƒ¼ãƒ ã‚¹ã‚¿ã‚¤ãƒ«",
    "ã‚´ãƒ¼ãƒ«ç¢ºç‡","å¾—ç‚¹äºˆæƒ³æ™‚é–“","è©¦åˆID","é€šç•ª","ã‚½ãƒ¼ãƒˆç”¨ç§’"
]


# =========================
# çµ±è¨ˆã‚­ãƒ¼ -> HEADERåˆ—å¯¾å¿œï¼ˆcanonicalã‚­ãƒ¼å‰æï¼‰
# =========================
STAT_KEY_MAP = {
    "ã‚¢ã‚¿ãƒƒã‚¯:æœŸå¾…å€¤ï¼ˆxGï¼‰": ("ãƒ›ãƒ¼ãƒ æœŸå¾…å€¤", "ã‚¢ã‚¦ã‚§ãƒ¼æœŸå¾…å€¤"),
    "ã‚¢ã‚¿ãƒƒã‚¯:æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤": ("ãƒ›ãƒ¼ãƒ æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤", "ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤"),
    "ãƒã‚¼ãƒƒã‚·ãƒ§ãƒ³:ãƒœãƒ¼ãƒ«æ”¯é…ç‡": ("ãƒ›ãƒ¼ãƒ ãƒœãƒ¼ãƒ«æ”¯é…ç‡", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒ¼ãƒ«æ”¯é…ç‡"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ã‚·ãƒ¥ãƒ¼ãƒˆæ•°": ("ãƒ›ãƒ¼ãƒ ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆæ•°"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°": ("ãƒ›ãƒ¼ãƒ æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", "ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°": ("ãƒ›ãƒ¼ãƒ æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", "ã‚¢ã‚¦ã‚§ãƒ¼æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ": ("ãƒ›ãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ"),
    "ã‚¢ã‚¿ãƒƒã‚¯:ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹": ("ãƒ›ãƒ¼ãƒ ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹"),
    "ã‚»ãƒƒãƒˆãƒ—ãƒ¬ãƒ¼:ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯": ("ãƒ›ãƒ¼ãƒ ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ": ("ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ"),  # â€»å…ƒã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã‚‹ãªã‚‰ã“ã“ã¯è¦ç¢ºèª
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ": ("ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒã‚¹ãƒˆãƒ’ãƒƒãƒˆ": ("ãƒ›ãƒ¼ãƒ ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«": ("ãƒ›ãƒ¼ãƒ ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«"),

    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–": ("ãƒ›ãƒ¼ãƒ ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯": ("ãƒ›ãƒ¼ãƒ ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚ªãƒ•ã‚µã‚¤ãƒ‰": ("ãƒ›ãƒ¼ãƒ ã‚ªãƒ•ã‚µã‚¤ãƒ‰", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚ªãƒ•ã‚µã‚¤ãƒ‰"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ•ã‚¡ã‚¦ãƒ«": ("ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¦ãƒ«", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¦ãƒ«"),
    "ã‚«ãƒ¼ãƒ‰:ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰": ("ãƒ›ãƒ¼ãƒ ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰"),
    "ã‚«ãƒ¼ãƒ‰:ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰": ("ãƒ›ãƒ¼ãƒ ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³": ("ãƒ›ãƒ¼ãƒ ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³"),

    "ãƒ‘ã‚¹:ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ": ("ãƒ›ãƒ¼ãƒ ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ", "ã‚¢ã‚¦ã‚§ãƒ¼ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ"),
    "ãƒ‘ã‚¹:ç·ãƒ‘ã‚¹æ•°": ("ãƒ›ãƒ¼ãƒ ãƒ‘ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‘ã‚¹"),
    "ãƒ‘ã‚¹:ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹": ("ãƒ›ãƒ¼ãƒ ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹"),
    "ãƒ‘ã‚¹:ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹": ("ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹"),
    "ãƒ‘ã‚¹:ã‚¯ãƒ­ã‚¹": ("ãƒ›ãƒ¼ãƒ ã‚¯ãƒ­ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒ­ã‚¹"),

    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¿ãƒƒã‚¯ãƒ«": ("ãƒ›ãƒ¼ãƒ ã‚¿ãƒƒã‚¯ãƒ«", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¿ãƒƒã‚¯ãƒ«"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¯ãƒªã‚¢": ("ãƒ›ãƒ¼ãƒ ã‚¯ãƒªã‚¢", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒªã‚¢"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©": ("ãƒ›ãƒ¼ãƒ ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ": ("ãƒ›ãƒ¼ãƒ ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ"),
}

LABEL_TO_CANON: Dict[str, str] = {
    "ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤ï¼ˆxGï¼‰": "ã‚¢ã‚¿ãƒƒã‚¯:æœŸå¾…å€¤ï¼ˆxGï¼‰",
    "æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤ï¼ˆxGOTï¼‰": "ã‚¢ã‚¿ãƒƒã‚¯:æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤",

    "ãƒœãƒ¼ãƒ«æ”¯é…ç‡": "ãƒã‚¼ãƒƒã‚·ãƒ§ãƒ³:ãƒœãƒ¼ãƒ«æ”¯é…ç‡",
    "åˆè¨ˆã‚·ãƒ¥ãƒ¼ãƒˆ": "ã‚·ãƒ¥ãƒ¼ãƒˆ:ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆ": "ã‚·ãƒ¥ãƒ¼ãƒˆ:æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ": "ã‚·ãƒ¥ãƒ¼ãƒˆ:æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "ã‚·ãƒ¥ãƒ¼ãƒˆãƒ–ãƒ­ãƒƒã‚¯": "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ",
    "ãƒœãƒƒã‚¯ã‚¹å†…ã‹ã‚‰ã®ã‚·ãƒ¥ãƒ¼ãƒˆ": "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ",
    "ãƒœãƒƒã‚¯ã‚¹å¤–ã‹ã‚‰ã®ã‚·ãƒ¥ãƒ¼ãƒˆ": "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ",
    "ã‚´ãƒ¼ãƒ«æ ã«å½“ãŸã‚‹": "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒã‚¹ãƒˆãƒ’ãƒƒãƒˆ",
    "ã‚´ãƒ¼ãƒ«æ ã«å½“ãŸã‚‹ã‚·ãƒ¥ãƒ¼ãƒˆ": "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒã‚¹ãƒˆãƒ’ãƒƒãƒˆ",

    "ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹": "ã‚¢ã‚¿ãƒƒã‚¯:ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹",
    "ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯": "ã‚»ãƒƒãƒˆãƒ—ãƒ¬ãƒ¼:ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯",

    "ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰": "ã‚«ãƒ¼ãƒ‰:ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰",
    "ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰": "ã‚«ãƒ¼ãƒ‰:ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰",
    "ãƒ•ã‚¡ã‚¦ãƒ«": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ•ã‚¡ã‚¦ãƒ«",
    "ã‚ªãƒ•ã‚µã‚¤ãƒ‰": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚ªãƒ•ã‚µã‚¤ãƒ‰",
    "ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯",
    "ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³",
    "ã‚¿ãƒƒã‚¯ãƒ«": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¿ãƒƒã‚¯ãƒ«",
    "ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©",
    "ã‚¯ãƒªã‚¢ãƒªãƒ³ã‚°": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¯ãƒªã‚¢",
    "ã‚¯ãƒªã‚¢": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¯ãƒªã‚¢",
    "ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ",
    "ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–": "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–",

    "ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹å†…ã§ã®ã‚¿ãƒƒãƒ": "ãƒ‘ã‚¹:ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ",
    "ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹å†…ã‚¿ãƒƒãƒ": "ãƒ‘ã‚¹:ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ",
    "ãƒ‘ã‚¹": "ãƒ‘ã‚¹:ç·ãƒ‘ã‚¹æ•°",
    "ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹": "ãƒ‘ã‚¹:ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹",
    "ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ã§ã®ãƒ‘ã‚¹": "ãƒ‘ã‚¹:ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹",
    "ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ã®ãƒ‘ã‚¹": "ãƒ‘ã‚¹:ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹",
    "ã‚¯ãƒ­ã‚¹": "ãƒ‘ã‚¹:ã‚¯ãƒ­ã‚¹",
}

SECTION_PREFER = ["ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¿ãƒƒã‚¯", "ãƒ‘ã‚¹", "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹", "ã‚´ãƒ¼ãƒ«ã‚­ãƒ¼ãƒ‘ãƒ¼"]
def _section_rank(section: str) -> int:
    try:
        return SECTION_PREFER.index(section)
    except ValueError:
        return 999


# =========================
# VERIFY
# =========================
def verify_header_and_stat_map():
    bad = []
    for k, (hcol, acol) in STAT_KEY_MAP.items():
        if hcol not in HEADER:
            bad.append(("HOME", k, hcol))
        if acol not in HEADER:
            bad.append(("AWAY", k, acol))
    if bad:
        log("âŒ [VERIFY] STAT_KEY_MAP ãŒå‚ç…§ã—ã¦ã„ã‚‹åˆ—ãŒ HEADER ã«ç„¡ã„")
        for side, k, col in bad:
            log(f"   - {side} key='{k}' col='{col}'")
        raise RuntimeError("STAT_KEY_MAP column mismatch with HEADER")
    log("âœ… [VERIFY] STAT_KEY_MAP ã®åˆ—åã¯ HEADER ã¨æ•´åˆã—ã¦ã„ã¾ã™")

def verify_stats_mapping(keys_title: str, stats_pairs: Dict[str, Tuple[str, str]]):
    scraped_keys = set(stats_pairs.keys())
    mapped_keys  = set(STAT_KEY_MAP.keys())
    direct_hit   = sorted(scraped_keys & mapped_keys)
    miss_scraped = sorted(scraped_keys - mapped_keys)

    log(f"âœ… [VERIFY:{keys_title}] ç›´æ¥ä¸€è‡´ã‚­ãƒ¼æ•°: {len(direct_hit)}")
    for k in direct_hit[:40]:
        hv, av = stats_pairs.get(k, ("",""))
        log(f"   âœ“ {k} = ({hv}, {av})")

    log(f"âš ï¸ [VERIFY:{keys_title}] ãƒãƒƒãƒ—ã«ç„¡ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã‚­ãƒ¼æ•°: {len(miss_scraped)}")
    for k in miss_scraped[:60]:
        hv, av = stats_pairs.get(k, ("",""))
        log(f"   ? {k} = ({hv}, {av})")

def verify_row_filled(row: Dict[str, Any], limit: int = 120):
    filled = []
    for col in HEADER:
        v = row.get(col, "")
        if v not in ("", None):
            filled.append((col, v))
    log(f"âœ… [VERIFY] row ã«å€¤ãŒå…¥ã£ãŸåˆ—æ•°: {len(filled)}")
    for col, v in filled[:limit]:
        log(f"   â€¢ {col} = {v}")


# =========================
# URL builders / helpers
# =========================
_MATCH_ROOT_RE = re.compile(r"^(/match/[^/]+/[^/]+/[^/]+/)")

def text_clean(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def extract_mid(any_url: str) -> Optional[str]:
    if not any_url:
        return None
    qs = parse_qs(urlparse(any_url).query)
    return qs.get("mid", [None])[0]

def _match_root_from_any_url(any_url: str) -> Tuple[Tuple[str, str, str], str]:
    parts = urlsplit(any_url)
    path = parts.path or ""
    m = _MATCH_ROOT_RE.search(path)
    if not m:
        raise ValueError(f"match root not found: {any_url}")
    match_root_path = m.group(1)
    mid = extract_mid(any_url) or ""
    return (parts.scheme, parts.netloc, match_root_path), mid

def build_stats_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "summary/stats/overall/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_summary_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "summary/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_live_standings_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "standings/live-standings/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_overall_standings_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "standings/standings/overall/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))


# =========================
# STOPå¯¾ç­–ï¼šsafe_goto
# =========================
def safe_goto(pg, url: str, timeout_ms: int = NAV_TIMEOUT_MS, tag: str = "") -> bool:
    ttag = f" {tag}" if tag else ""
    log(f"ğŸ§­ [GOTO]{ttag} try1 commit: {url}")
    try:
        pg.goto(url, timeout=timeout_ms, wait_until="commit")
        return True
    except PWTimeout as e:
        log(f"â±ï¸ [GOTO]{ttag} try1 timeout: {e}")
    except Exception as e:
        log(f"âš ï¸ [GOTO]{ttag} try1 error: {type(e).__name__}: {e}")

    try:
        log(f"ğŸ›‘ [GOTO]{ttag} window.stop()")
        pg.evaluate("() => window.stop()")
    except Exception as e:
        log(f"âš ï¸ [GOTO]{ttag} window.stop error: {type(e).__name__}: {e}")

    log(f"ğŸ§­ [GOTO]{ttag} try2 domcontentloaded: {url}")
    try:
        pg.goto(url, timeout=8000, wait_until="domcontentloaded")
        return True
    except PWTimeout as e:
        log(f"â±ï¸ [GOTO]{ttag} try2 timeout: {e}")
    except Exception as e:
        log(f"âš ï¸ [GOTO]{ttag} try2 error: {type(e).__name__}: {e}")

    try:
        log(f"ğŸ§¼ [GOTO]{ttag} about:blank")
        pg.goto("about:blank", timeout=3000, wait_until="commit")
    except Exception:
        pass

    log(f"ğŸ§­ [GOTO]{ttag} try3 domcontentloaded(after blank): {url}")
    try:
        pg.goto(url, timeout=8000, wait_until="domcontentloaded")
        return True
    except Exception as e:
        log(f"âŒ [GOTO]{ttag} try3 failed: {type(e).__name__}: {e}")
        return False

def _get_breadcrumb_country_league(pg) -> Tuple[str, str, List[str]]:
    """
    æ–°UIã®ãƒ‘ãƒ³ããš:
      1: ã‚µãƒƒã‚«ãƒ¼
      2: æ—¥æœ¬
      3: J2ãƒ»J3 ãƒªãƒ¼ã‚° - ãƒ©ã‚¦ãƒ³ãƒ‰ 1
    ã‚’ç¢ºå®Ÿã«å–ã‚‹
    """
    # 3ã¤æƒã†ã¾ã§å¾…ã¤ï¼ˆé‡è¦ï¼‰
    try:
        pg.wait_for_function("""
        () => {
          const xs = Array.from(
            document.querySelectorAll("li[data-testid='wcl-breadcrumbsItem'] [itemprop='name']")
          ).map(e => (e.textContent || '').replace(/\\s+/g,' ').trim()).filter(Boolean);
          return xs.length >= 3;
        }
        """, timeout=WAIT_TIMEOUT_MS)
    except Exception:
        pass

    # JSã§ä¸€æ‹¬æŠ½å‡ºï¼ˆlocator ãƒ«ãƒ¼ãƒ—ã‚ˆã‚Šå®‰å®šï¼‰
    crumbs = []
    try:
        crumbs = pg.evaluate("""
        () => Array.from(
          document.querySelectorAll("li[data-testid='wcl-breadcrumbsItem'] [itemprop='name']")
        ).map(e => (e.textContent || '').replace(/\\s+/g,' ').trim()).filter(Boolean)
        """) or []
    except Exception:
        crumbs = []

    # æƒ³å®š: ["ã‚µãƒƒã‚«ãƒ¼", "æ—¥æœ¬", "J2ãƒ»J3 ãƒªãƒ¼ã‚° - ãƒ©ã‚¦ãƒ³ãƒ‰ 1"]
    country = crumbs[1] if len(crumbs) >= 2 else ""
    league  = crumbs[2] if len(crumbs) >= 3 else ""

    return text_clean(country), text_clean(league), crumbs


# =========================
# consent / bot wall
# =========================
def kill_onetrust(page):
    try:
        btn = page.locator("#onetrust-accept-btn-handler")
        if btn.count():
            btn.click(timeout=1200, force=True)
    except:
        pass
    try:
        page.evaluate("""
        () => {
          const ids = ["onetrust-consent-sdk", "onetrust-banner-sdk"];
          ids.forEach(id => document.getElementById(id)?.remove());
          document.querySelectorAll(".ot-sdk-container, .ot-sdk-row, .otOverlay, .ot-pc-footer, .ot-pc-header")
            .forEach(el => el.remove());
        }""", timeout=1200)
    except:
        pass

def kill_consent_banners(page):
    kill_onetrust(page)
    candidates = [
        "button:has-text('ã™ã¹ã¦æ‹’å¦ã™ã‚‹')",
        "button:has-text('å…¨ã¦æ‹’å¦ã™ã‚‹')",
        "button:has-text('æ‹’å¦ã™ã‚‹')",
        "button:has-text('åŒæ„ã—ã¾ã™')",
        "button:has-text('åŒæ„ã™ã‚‹')",
        "button:has-text('Reject all')",
        "button:has-text('Accept all')",
        "[role='button']:has-text('ã™ã¹ã¦æ‹’å¦ã™ã‚‹')",
        "[role='button']:has-text('åŒæ„ã—ã¾ã™')",
    ]
    for sel in candidates:
        try:
            b = page.locator(sel).first
            if b.count() and b.is_visible(timeout=500):
                b.click(timeout=1200, force=True)
                break
        except:
            pass

    try:
        page.evaluate("""
        () => {
          const kill = [
            "#qc-cmp2-container",
            "#didomi-host",
            "#sp_message_container_",
            ".fc-consent-root",
            ".message-component",
            ".pmConsentWall"
          ];
          kill.forEach(s => document.querySelectorAll(s).forEach(el => el.remove()));
        }""", timeout=1200)
    except:
        pass

def is_bot_wall(pg) -> bool:
    try:
        return pg.locator(f"text=/{BOT_WALL_PAT}/i").first.is_visible(timeout=900)
    except:
        return False


# =========================
# route blockingï¼ˆè»½é‡åŒ–ï¼‰
# =========================
def setup_route_blocking(ctx):
    def _route(route):
        try:
            rtype = route.request.resource_type
            url = (route.request.url or "").lower()
            if rtype in ("image", "media", "font"):
                return route.abort()
            if any(x in url for x in ("doubleclick", "googlesyndication", "adservice", "adsystem", "taboola", "outbrain")):
                return route.abort()
        except:
            pass
        return route.continue_()
    ctx.route("**/*", _route)


# =========================
# match page: teams/scores/time
# =========================
def get_home_away_names(pg) -> Tuple[str, str]:
    try:
        cont = pg.locator("div.duelParticipant__container").first
        if not cont.count():
            cont = pg
        h = cont.locator(".duelParticipant__home a.participant__participantName").first
        a = cont.locator(".duelParticipant__away a.participant__participantName").first
        home = text_clean(h.text_content()) if h.count() else ""
        away = text_clean(a.text_content()) if a.count() else ""
        if not home:
            img = cont.locator(".duelParticipant__home img.participant__image").first
            home = text_clean(img.get_attribute("alt") or "")
        if not away:
            img = cont.locator(".duelParticipant__away img.participant__image").first
            away = text_clean(img.get_attribute("alt") or "")
        return home, away
    except:
        return "", ""

def get_scores(pg) -> Tuple[str, str]:
    def _clean(s: str) -> str:
        return re.sub(r"\s+", "", (s or "").replace("\u00A0", " ")).strip()

    def _from_container(el):
        try:
            spans = el.locator("span")
            vals = []
            for i in range(spans.count()):
                sp = spans.nth(i)
                cls = (sp.get_attribute("class") or "")
                if "divider" in cls:
                    continue
                txt = _clean(sp.text_content() or "")
                if re.fullmatch(r"\d+", txt):
                    vals.append(txt)
            if len(vals) >= 2:
                return vals[0], vals[-1]
        except:
            pass

        try:
            t = _clean(el.inner_text() or "")
            m = re.search(r"(\d+)\s*[\-\u2212\u2012\u2013\u2014\u2015]\s*(\d+)", t)
            if m:
                return m.group(1), m.group(2)
        except:
            pass
        return "", ""

    try:
        fx = pg.locator(".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedScore").first
        if fx.count():
            h, a = _from_container(fx)
            if h and a:
                return h, a
    except:
        pass

    try:
        live = pg.locator("div.detailScore__wrapper.detailScore__live").first
        if live.count():
            h, a = _from_container(live)
            if h and a:
                return h, a
    except:
        pass

    try:
        wrap = pg.locator("div.detailScore__wrapper").first
        if wrap.count():
            h, a = _from_container(wrap)
            if h and a:
                return h, a
    except:
        pass

    return "", ""

def get_match_time_text(pg) -> str:
    def _txt(locator) -> str:
        try:
            if locator.count():
                t = (locator.first.text_content() or "").strip().replace("\u00A0", " ")
                return t
        except:
            pass
        return ""

    event_selectors = [
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .eventAndAddedTime .eventTime",
        "div.detailScore__status .eventAndAddedTime .eventTime",
        ".eventAndAddedTime .eventTime",
    ]
    for s in event_selectors:
        t = _txt(pg.locator(f"{s} >> visible=true"))
        if t:
            return t

    testid_selectors = [
        "[data-testid='wcl-time']",
    ]
    for s in testid_selectors:
        t = _txt(pg.locator(f"{s} >> visible=true"))
        if t:
            return t

    status_selectors = [
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedHeaderDuel__detailStatus",
        "div.detailScore__status .fixedHeaderDuel__detailStatus",
        "div.detailScore__status",
    ]
    for s in status_selectors:
        t = _txt(pg.locator(f"{s} >> visible=true"))
        if t:
            return t

    return ""

def parse_live_time_to_seconds(tstr: str) -> int:
    if not tstr:
        return 0
    t = tstr.strip()
    if "çµ‚äº†" in t or "FT" in t.upper():
        return 5400
    if "å‰åŠ" in t:
        num = re.sub(r"[^0-9]", "", t)
        return int(num) * 60 if num else 0
    if "å¾ŒåŠ" in t:
        num = re.sub(r"[^0-9]", "", t)
        return 2700 + int(num) * 60 if num else 2700
    if re.match(r"^\d+\+\d+$", t):
        a, b = t.split("+")
        return (int(a) + int(b)) * 60
    if t.isdigit():
        return int(t) * 60
    return 0


# =========================
# RowBuilderï¼ˆdictï¼‰
# =========================
class RowBuilder:
    def __init__(self, header: List[str], match_tag: str = ""):
        self.header = header
        self.d = {col: "" for col in header}
        self.match_tag = match_tag

    def put(self, key: str, value: Any):
        if key not in self.d:
            return
        self.d[key] = "" if value is None else value

    def put_pair(self, home_key: str, away_key: str, home_val: Any, away_val: Any):
        self.put(home_key, home_val)
        self.put(away_key, away_val)

def apply_stats_to_row(rb: RowBuilder, canon_pairs: Dict[str, Tuple[str, str]]):
    for stat_key, (hcol, acol) in STAT_KEY_MAP.items():
        if stat_key in canon_pairs:
            hv, av = canon_pairs[stat_key]
            rb.put_pair(hcol, acol, hv, av)


# =========================
# stats rawï¼ˆåŠ å·¥ãªã—ï¼‰
# =========================
STAT_ROW_SELECTOR = "[data-testid='wcl-statistics']"

def goto_statistics_page(pg) -> bool:
    stats_url = build_stats_url(pg.url)

    kill_consent_banners(pg)

    if "/summary/stats/" not in pg.url:
        log(f"â¡ï¸ [STATS] goto: {stats_url}")
        ok = safe_goto(pg, stats_url, timeout_ms=NAV_TIMEOUT_MS, tag="STATS")
        if not ok:
            log("âš ï¸ [STATS] gotoå¤±æ•—ï¼ˆsafe_gotoã§ã‚‚ï¼‰")
            return False
        kill_consent_banners(pg)

    if is_bot_wall(pg):
        log("ğŸ§± [STATS] BOTå£ã£ã½ã„ â†’ statsã‚¹ã‚­ãƒƒãƒ—")
        return False

    log("â³ [STATS] wait_for_selector wcl-statistics ...")
    for attempt in range(2):
        try:
            pg.wait_for_selector(STAT_ROW_SELECTOR, timeout=WAIT_TIMEOUT_MS)
            log("âœ… [STATS] statistics selector appeared")
            return True
        except Exception as e:
            log(f"âš ï¸ [STATS] wait_for_selector failed({attempt+1}/2): {type(e).__name__}: {e}")
            kill_consent_banners(pg)

    log("âš ï¸ [STATS] çµ±è¨ˆå‡ºãªã„ï¼ˆåŒæ„é™¤å»å¾Œã‚‚ï¼‰")
    return False

def scrape_stats_raw(pg) -> Dict[str, List[str]]:
    """
    out: {"ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„:ãƒ‘ã‚¹": ["34%ï¼ˆ31/90ï¼‰","51%ï¼ˆ24/47ï¼‰"], ...}
    â€» å€¤ã¯åŠ å·¥ã—ãªã„
    """
    if not goto_statistics_page(pg):
        return {}

    log("ğŸ“Š [STATS] JS extraction start")
    try:
        pg.wait_for_selector('[data-testid="wcl-statistics"]', timeout=WAIT_TIMEOUT_MS)
    except Exception:
        log("âš ï¸ [STATS] statistics selector not found")
        return {}

    stats = pg.evaluate("""
    () => {
      const out = {};
      const wrapper = document.querySelector('.sectionsWrapper');
      if (!wrapper) return out;

      const clean = (s) =>
        (s || '')
          .replace(/\\u00A0/g, ' ')
          .replace(/\\s+/g, ' ')
          .trim();

      const pickValue = (cell) => {
        if (!cell) return '';
        const spans = cell.querySelectorAll("span[data-testid='wcl-scores-simple-text-01']");
        if (!spans || spans.length === 0) return '';
        const parts = [];
        for (const sp of spans) {
          const t = clean(sp.textContent);
          if (t) parts.push(t);
        }
        return parts.join(' ');
      };

      const sections = wrapper.querySelectorAll('.section');
      for (const sec of sections) {
        const sectionTitle = clean(sec.querySelector('.sectionHeader')?.textContent);
        if (!sectionTitle) continue;

        const rows = sec.querySelectorAll('[data-testid="wcl-statistics"]');
        for (const row of rows) {
          const label = clean(
            row.querySelector('[data-testid="wcl-statistics-category"] span')?.textContent
          );
          if (!label) continue;

          const values = row.querySelectorAll('[data-testid="wcl-statistics-value"]');
          if (!values || values.length < 2) continue;

          const home = pickValue(values[0]);
          const away = pickValue(values[values.length - 1]);

          out[`${sectionTitle}:${label}`] = [home, away];
        }
      }
      return out;
    }
    """) or {}

    log(f"ğŸ“Š [STATS] extracted items = {len(stats)}")
    return stats

def normalize_stats_raw_to_canon(stats_raw: Dict[str, Any]) -> Dict[str, Tuple[str, str]]:
    """
    stats_raw: {"ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„:åˆè¨ˆã‚·ãƒ¥ãƒ¼ãƒˆ":[home,away], ...}
    -> {"ã‚·ãƒ¥ãƒ¼ãƒˆ:ã‚·ãƒ¥ãƒ¼ãƒˆæ•°":(home,away), ...}
    """
    if not stats_raw:
        return {}

    best_by_label: Dict[str, Tuple[int, Tuple[str, str]]] = {}

    for k, v in stats_raw.items():
        if not isinstance(v, (list, tuple)) or len(v) < 2:
            continue
        if ":" not in k:
            continue

        section, label = k.split(":", 1)
        section = (section or "").strip()
        label = (label or "").strip()

        canon = LABEL_TO_CANON.get(label)
        if not canon:
            continue

        hv, av = str(v[0]), str(v[1])
        rank = _section_rank(section)

        if (label not in best_by_label) or (rank < best_by_label[label][0]):
            best_by_label[label] = (rank, (hv, av))

    out: Dict[str, Tuple[str, str]] = {}
    for label, (_, pair) in best_by_label.items():
        canon = LABEL_TO_CANON.get(label)
        if canon:
            out[canon] = pair
    return out


# =========================
# metaï¼ˆsummaryï¼‰
# =========================
def get_match_meta(pg) -> Dict[str, str]:
    meta: Dict[str, str] = {}
    summary_url = build_summary_url(pg.url)

    if "/summary/" not in pg.url or "/summary/stats" in pg.url:
        log(f"â¡ï¸ [META] goto: {summary_url}")
        ok = safe_goto(pg, summary_url, timeout_ms=NAV_TIMEOUT_MS, tag="META")
        if not ok:
            meta["è©¦åˆæ™‚é–“"] = get_match_time_text(pg)
            meta["å–å¾—æ™‚åˆ»"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            return meta
        kill_consent_banners(pg)

    if is_bot_wall(pg):
        meta["è©¦åˆæ™‚é–“"] = get_match_time_text(pg)
        meta["å–å¾—æ™‚åˆ»"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return meta

    try:
        pg.wait_for_selector(
            "ol li span[itemprop='name'], [data-testid*='breadcrumbs'], .tournamentHeader, .duelParticipant",
            timeout=WAIT_TIMEOUT_MS
        )
    except:
        pass

    country = ""
    league = ""

    try:
        crumb_txts = []
        cand = pg.locator("ol li span[itemprop='name'], [data-testid*='breadcrumbs'] span[itemprop='name']")
        for i in range(cand.count()):
            t = text_clean(cand.nth(i).text_content() or "")
            if t:
                crumb_txts.append(t)
        if len(crumb_txts) >= 2:
            country = crumb_txts[1]
        if len(crumb_txts) >= 3:
            league = crumb_txts[2]
    except:
        pass

    if not country or not league:
        try:
            c = pg.locator(".tournamentHeader__country, .tournamentHeader__category, [class*='tournamentHeader__category']").first
            t = text_clean(c.text_content() or "")
            if t:
                if ":" in t and (not country or not league):
                    a, b = [x.strip() for x in t.split(":", 1)]
                    if not country:
                        country = a
                    if not league:
                        league = b
                elif not country:
                    country = t
        except:
            pass

        try:
            l = pg.locator(".tournamentHeader__name, [class*='tournamentHeader__name']").first
            t = text_clean(l.text_content() or "")
            if t and not league:
                league = t
        except:
            pass

    # ã“ã“ã¯è»½ãå¾…ã¤ï¼ˆcommitç›´å¾Œå¯¾ç­–ï¼‰
    try:
        pg.wait_for_selector("li[data-testid='wcl-breadcrumbsItem']", timeout=WAIT_TIMEOUT_MS)
    except:
        pass

    # â˜… ã“ã“ã§æ–°æ–¹å¼ï¼šãƒ‘ãƒ³ããšã‚’ç¢ºå®Ÿã«å–ã‚‹
    country, league, crumbs = _get_breadcrumb_country_league(pg)
    if crumbs:
        log(f"ğŸ§© [META] breadcrumbs={crumbs}")

    meta["å›½"] = country
    meta["ãƒªãƒ¼ã‚°"] = league

    label_aliases = {
        "ãƒ¬ãƒ•ã‚§ãƒªãƒ¼": "ãƒ¬ãƒ•ã‚§ãƒªãƒ¼",
        "å¯©åˆ¤": "ãƒ¬ãƒ•ã‚§ãƒªãƒ¼",
        "ä¸»å¯©": "ãƒ¬ãƒ•ã‚§ãƒªãƒ¼",
        "é–‹å‚¬åœ°": "é–‹å‚¬åœ°",
        "ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ": "é–‹å‚¬åœ°",
        "ä¼šå ´": "é–‹å‚¬åœ°",
        "åå®¹äººæ•°": "åå®¹äººæ•°",
        "ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£": "åå®¹äººæ•°",
        "è¦³å®¢": "è¦³å®¢",
        "è¦³å®¢æ•°": "è¦³å®¢",
        "å‚åŠ ": "è¦³å®¢",
    }
    want = set(label_aliases.keys())

    def put_meta(label: str, value: str):
        label = text_clean(label).replace(":", "").replace("ï¼š", "").strip()
        value = text_clean(value)
        if not label or not value:
            return
        norm = label_aliases.get(label, label)
        if norm not in meta:
            meta[norm] = value

    try:
        dts = pg.locator("dl dt")
        n = dts.count()
        for i in range(n):
            dt = dts.nth(i)
            lab = text_clean(dt.text_content() or "")
            if lab not in want:
                continue
            dd = dt.locator("xpath=following-sibling::dd[1]").first
            val = text_clean(dd.text_content() or "") if dd.count() else ""
            put_meta(lab, val)
    except:
        pass

    meta["è©¦åˆæ™‚é–“"] = get_match_time_text(pg)
    meta["å–å¾—æ™‚åˆ»"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return meta

def jst_today_date() -> datetime.date:
    return (datetime.datetime.utcnow() + datetime.timedelta(hours=9)).date()

def parse_iso_date(dk: str) -> datetime.date:
    return datetime.datetime.strptime(dk, "%Y-%m-%d").date()

def fmt_ddmm(d: datetime.date) -> str:
    # dayPickerButton ã¯ "25/02 æ°´" ã®ã‚ˆã†ã« dd/mm
    return d.strftime("%d/%m")

def get_day_picker_text(page) -> str:
    try:
        # visibleè¦ä»¶ãªã—ã§DOMã‹ã‚‰å–å¾—ï¼ˆæœ€å¼·ï¼‰
        t = page.evaluate("""
        () => {
          const b = document.querySelector("[data-testid='wcl-dayPickerButton']");
          return b ? (b.textContent || "").replace(/\\s+/g, " ").trim() : "";
        }
        """)
        return text_clean(t or "")
    except:
        return ""

def click_day_arrow(page, direction: str) -> bool:
    # direction: 'prev' or 'next'
    sel = f"button[data-day-picker-arrow='{direction}']"
    try:
        btn = page.locator(sel).first
        if btn.count():
            btn.scroll_into_view_if_needed(timeout=1500)
            btn.click(timeout=3000, force=True)
            return True
    except Exception as e:
        log(f"âš ï¸ [DATE] arrow click error({direction}): {type(e).__name__}: {e}")

    # aria fallback
    aria = "å‰æ—¥" if direction == "prev" else "ç¿Œæ—¥"
    try:
        btn = page.locator(f"button[aria-label='{aria}']").first
        if btn.count():
            btn.scroll_into_view_if_needed(timeout=1500)
            btn.click(timeout=3000, force=True)
            return True
    except Exception as e:
        log(f"âš ï¸ [DATE] arrow(aria) click error({direction}): {type(e).__name__}: {e}")

    return False

def goto_results_date_by_arrows(page, current_date: datetime.date, target_date: datetime.date) -> datetime.date:
    """
    ç¾åœ¨æ—¥ä»˜ current_dateï¼ˆJSTæƒ³å®šï¼‰ã‹ã‚‰ target_date ã«ã€prev/next ã§ç§»å‹•ã€‚
    æˆ»ã‚Šå€¤: åˆ°é”ã—ãŸã¨ã¿ãªã™æ—¥ä»˜ï¼ˆåŸºæœ¬ã¯ target_dateã€å¤±æ•—æ™‚ã¯é€”ä¸­ï¼‰
    """
    if current_date == target_date:
        return current_date

    delta = (target_date - current_date).days
    direction = "next" if delta > 0 else "prev"
    steps = abs(delta)

    if steps > MAX_DAY_STEPS:
        log(f"âš ï¸ [DATE] steps={steps} > MAX_DAY_STEPS={MAX_DAY_STEPS} ãªã®ã§æ‰“ã¡åˆ‡ã‚Šï¼ˆENVã§å¢—ã‚„ã›ã¾ã™ï¼‰")
        steps = MAX_DAY_STEPS

    want_ddmm = fmt_ddmm(target_date)

    for i in range(steps):
        kill_consent_banners(page)
        ok = click_day_arrow(page, direction)
        if not ok:
            log(f"âŒ [DATE] arrow click failed direction={direction}")
            break

        # ä¸€è¦§ãŒæ›´æ–°ã•ã‚Œã‚‹ã®ã§å°‘ã—å¾…ã¤ï¼ˆé‡ã„æ—¥ã¯é•·ã‚ã«ï¼‰
        page.wait_for_timeout(600)

        # è¡¨ç¤ºç¢ºèªï¼ˆ"25/02 æ°´" ãªã©ã« want_ddmm ãŒå«ã¾ã‚Œã‚‹ã‹ï¼‰
        txt = get_day_picker_text(page)
        if want_ddmm and (want_ddmm in txt):
            return target_date

    # æœ€å¾Œã«ã‚‚ã†ä¸€åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ°é”ã—ã¦ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
    txt = get_day_picker_text(page)
    if want_ddmm and (want_ddmm in txt):
        return target_date

    # åˆ°é”ã§ããªã‹ã£ãŸå ´åˆã¯ã€current_date ã‚’è¿‘ã¥ã‘ãŸåˆ†ã ã‘é€²ã‚ãŸæ‰±ã„ï¼ˆæ¦‚ç®—ï¼‰
    moved = (steps if direction == "next" else -steps)
    return current_date + datetime.timedelta(days=moved)

# =========================
# standingsï¼ˆé †ä½ï¼‰
# =========================
def goto_standings_page(pg) -> Optional[str]:
    url1 = build_live_standings_url(pg.url)
    url2 = build_overall_standings_url(pg.url)

    log(f"â¡ï¸ [RANK] goto(live): {url1}")
    ok = safe_goto(pg, url1, timeout_ms=NAV_TIMEOUT_MS, tag="RANK-LIVE")
    if ok:
        kill_consent_banners(pg)
        try:
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=WAIT_TIMEOUT_MS)
            return url1
        except Exception as e:
            log(f"âš ï¸ [RANK] live wait failed: {type(e).__name__}: {e}")

    log(f"â¡ï¸ [RANK] goto(overall): {url2}")
    ok = safe_goto(pg, url2, timeout_ms=NAV_TIMEOUT_MS, tag="RANK-OVERALL")
    if ok:
        kill_consent_banners(pg)
        try:
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=WAIT_TIMEOUT_MS)
            return url2
        except Exception as e:
            log(f"âš ï¸ [RANK] overall wait failed: {type(e).__name__}: {e}")

    return None

def get_match_standings(pg, home_name: str, away_name: str) -> Dict[str, Any]:
    url = goto_standings_page(pg)
    if not url:
        return {}

    rows = pg.locator(".ui-table__body .ui-table__row")
    try:
        n = rows.count()
    except:
        n = 0
    if n == 0:
        return {}

    table = {}
    for i in range(n):
        r = rows.nth(i)
        rank_txt = text_clean(r.locator(".table__cell--rank .tableCellRank").first.text_content() or "")
        team_name = text_clean(r.locator(".table__cell--participant .tableCellParticipant__name").first.text_content() or "")
        pts_txt = text_clean(r.locator(".table__cell--value").last.text_content() or "")

        try:
            rank = int(rank_txt.strip().rstrip("."))
        except:
            rank = None
        try:
            pts = int(pts_txt)
        except:
            pts = None

        if team_name:
            table[team_name] = {"rank": rank, "pts": pts}

    h = text_clean(home_name)
    a = text_clean(away_name)

    home = next((v for k, v in table.items() if (h and (h in k or k in h))), None)
    away = next((v for k, v in table.items() if (a and (a in k or k in a))), None)

    return {
        "url": url,
        "home_rank": home["rank"] if home else None,
        "home_pts": home["pts"] if home else None,
        "away_rank": away["rank"] if away else None,
        "away_pts": away["pts"] if away else None,
    }


# =========================
# S3 utils
# =========================
def s3_client():
    return boto3.client("s3", region_name=(S3_REGION or None))

def s3_put_bytes(bucket: str, key: str, data: bytes, content_type: str) -> bool:
    s3 = s3_client()
    try:
        s3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type)
        log(f"âœ… S3 put: s3://{bucket}/{key}")
        return True
    except ClientError as e:
        log(f"âŒ S3 put failed: {e}")
        return False

def s3_get_bytes(bucket: str, key: str) -> Optional[bytes]:
    s3 = s3_client()
    try:
        obj = s3.get_object(Bucket=bucket, Key=key)
        return obj["Body"].read()
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code in ("NoSuchKey", "404", "NotFound"):
            return None
        log(f"âŒ S3 get failed: {e}")
        return None

def collect_match_roots_on_current_results_page(page) -> List[str]:
    hrefs = set()

    # eventRowLinkå„ªå…ˆ
    try:
        loc = page.locator("a.eventRowLink[href*='/match/']")
        n = loc.count()
        for i in range(n):
            h = loc.nth(i).get_attribute("href") or ""
            if "/match/" in h:
                hrefs.add(h)
    except:
        pass

    # fallback
    try:
        loc = page.locator("a[href*='/match/']")
        n = loc.count()
        for i in range(n):
            h = loc.nth(i).get_attribute("href") or ""
            if "/match/" in h:
                hrefs.add(h)
    except:
        pass

    out: List[str] = []
    for h in sorted(hrefs):
        if h.startswith("http"):
            out.append(h)
        elif h.startswith("/"):
            out.append("https://www.flashscore.co.jp" + h)
        else:
            out.append("https://www.flashscore.co.jp/" + h)
    return out

def collect_finished_match_roots_by_dates(date_keys: List[str]) -> Dict[str, List[str]]:
    out: Dict[str, List[str]] = {}

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(
            user_agent=USER_AGENT,
            locale=LOCALE,
            timezone_id=TIMEZONE_ID
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        page = ctx.new_page()

        log("ğŸŒ Flashscore football RESULTS ã‚’é–‹ãã¾ã™...")

        ok = goto_results_tab(page)
        if not ok:
            try: browser.close()
            except: pass
            return {}

        # cursor_date ã¯ã€ŒdayPickerãŒç¤ºã™æ—¥ä»˜ã€ã‹ã‚‰æ¨å®šã™ã‚‹ã®ãŒæœ¬å½“ã¯ç†æƒ³ã§ã™ãŒã€
        # ã¾ãšã¯ JST ä»Šæ—¥ã¨ã—ã¦é€²ã‚ã¾ã™ï¼ˆã‚ãªãŸã®ç”»é¢ã ã¨ 28/02 åœŸ ã«ãªã£ã¦ã„ã‚‹ã®ã§ä¸€è‡´ã—ã‚„ã™ã„ï¼‰
        cursor_date = jst_today_date()
        log(f"ğŸ“… [DATE] initial picker text='{get_day_picker_text(page)}' cursor_date(assumed)={cursor_date}")

        for dk in date_keys:
            target_date = parse_iso_date(dk)
            log(f"\nğŸ“… [DATE] move to {dk} (dd/mm={fmt_ddmm(target_date)})")

            cursor_date = goto_results_date_by_arrows(page, cursor_date, target_date)
            kill_consent_banners(page)

            # ãã®æ—¥ãƒšãƒ¼ã‚¸ã§ã•ã‚‰ã«ä¸€è¦§ã‚’å±•é–‹
            click_show_more_results(page, max_load_more_clicks=MAX_LOAD_MORE_CLICKS)

            roots = collect_match_roots_on_current_results_page(page)
            out[dk] = roots
            log(f"âœ… [RESULTS] {dk} collected roots: {len(roots)}")

        try: browser.close()
        except: pass

    return out

# =========================
# FIN JSON (target mids)
# =========================
FIN_JSON_BUCKET = "aws-s3-outputs-csv"
FIN_JSON_KEY    = "fin/b008_fin_getting_data.json"

def _collect_match_ids_from_obj(obj: Any) -> List[str]:
    if obj is None:
        return []
    if isinstance(obj, str):
        return [obj.strip()] if obj.strip() else []
    if isinstance(obj, list):
        out = []
        for x in obj:
            out.extend(_collect_match_ids_from_obj(x))
        return out
    if isinstance(obj, dict):
        for k in ("matchId", "mid", "match_id", "gameId"):
            v = obj.get(k)
            if isinstance(v, str) and v.strip():
                return [v.strip()]
        out = []
        for v in obj.values():
            out.extend(_collect_match_ids_from_obj(v))
        return out
    return []

def _as_date_key(s: str) -> Optional[str]:
    """ '2026-02-25' ã‚’ date key ã¨ã—ã¦æ¡ç”¨ã€‚ãƒ€ãƒ¡ãªã‚‰ None """
    if not isinstance(s, str):
        return None
    s = s.strip()
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}", s):
        return s
    return None

def _extract_mid_value(obj: Any) -> Optional[str]:
    if isinstance(obj, str) and obj.strip():
        return obj.strip()
    if isinstance(obj, dict):
        for k in ("matchId", "mid", "match_id", "gameId"):
            v = obj.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    return None

def _extract_date_value(obj: Any) -> Optional[str]:
    """ dictã®ä¸­ã‹ã‚‰æ—¥ä»˜ã‚‰ã—ãã‚­ãƒ¼ã‚’æ‹¾ã† """
    if not isinstance(obj, dict):
        return None
    for k in ("date", "matchDate", "match_date", "ymd", "day"):
        v = obj.get(k)
        dk = _as_date_key(v) if isinstance(v, str) else None
        if dk:
            return dk
    return None

def load_target_mids_by_date_from_s3() -> Dict[str, List[str]]:
    """
    è¿”ã‚Šå€¤: {"2026-02-25": ["mid1","mid2",...], ...}
    JSONå½¢çŠ¶ã®ä¾‹:
      A) {"2026-02-25":["mid1","mid2"], "2026-02-26":[...]}
      B) [{"date":"2026-02-25","matchId":"mid1"}, ...]
      C) [["2026-02-25","mid1"], ["2026-02-25","mid2"]]
    """
    b = s3_get_bytes(FIN_JSON_BUCKET, FIN_JSON_KEY)
    if not b:
        log(f"âŒ [FIN] not found: s3://{FIN_JSON_BUCKET}/{FIN_JSON_KEY}")
        return {}

    try:
        obj = json.loads(b.decode("utf-8"))
    except Exception as e:
        log(f"âŒ [FIN] json parse failed: {e}")
        return {}

    out: Dict[str, List[str]] = {}

    # A) date-keyed dict
    if isinstance(obj, dict) and any(_as_date_key(k) for k in obj.keys()):
        for k, v in obj.items():
            dk = _as_date_key(k)
            if not dk:
                continue
            mids: List[str] = []
            if isinstance(v, list):
                for it in v:
                    mid = _extract_mid_value(it) or (it.strip() if isinstance(it, str) else None)
                    if mid:
                        mids.append(mid)
            elif isinstance(v, dict):
                mid = _extract_mid_value(v)
                if mid:
                    mids.append(mid)
            out[dk] = list(dict.fromkeys(mids))
        log(f"âœ… [FIN] loaded date-keyed dict: days={len(out)} total={sum(len(v) for v in out.values())}")
        return out

    # B/C) list
    if isinstance(obj, list):
        for it in obj:
            # C) pair
            if isinstance(it, (list, tuple)) and len(it) >= 2:
                dk = _as_date_key(it[0] if isinstance(it[0], str) else "")
                mid = _extract_mid_value(it[1])
                if dk and mid:
                    out.setdefault(dk, []).append(mid)
                continue

            # B) object
            if isinstance(it, dict):
                dk = _extract_date_value(it)
                mid = _extract_mid_value(it)
                if dk and mid:
                    out.setdefault(dk, []).append(mid)
                continue

        for dk in list(out.keys()):
            out[dk] = list(dict.fromkeys([m for m in out[dk] if m]))
        log(f"âœ… [FIN] loaded list: days={len(out)} total={sum(len(v) for v in out.values())}")
        return out

    # fallbackï¼ˆæœ€æ‚ªï¼šæ—¥ä»˜ãŒå–ã‚Œãªã„â†’å½“æ—¥æ‰±ã„ã§å…¨midï¼‰
    mids = []
    if isinstance(obj, dict):
        mid = _extract_mid_value(obj)
        if mid:
            mids.append(mid)
    log("âš ï¸ [FIN] could not infer dates; fallback to today only")
    jst_today = (datetime.datetime.utcnow() + datetime.timedelta(hours=9)).date().strftime("%Y-%m-%d")
    return {jst_today: list(dict.fromkeys(mids))}

# =========================
# SEQMAP (S3)
# =========================
SEQMAP: Dict[str, int] = {}

def load_seqmap_from_s3():
    global SEQMAP
    b = s3_get_bytes(S3_BUCKET_OUTPUTS, SEQMAP_S3_KEY)
    if not b:
        SEQMAP = {}
        log(f"ğŸ†• [SEQ] S3ã«æ—¢å­˜ãªã—: s3://{S3_BUCKET_OUTPUTS}/{SEQMAP_S3_KEY}")
        return
    try:
        SEQMAP = pickle.loads(b) or {}
        log(f"ğŸ” [SEQ] S3ã‹ã‚‰èª­ã¿è¾¼ã¿: ä»¶æ•°={len(SEQMAP)}")
    except Exception as e:
        log(f"âš ï¸ [SEQ] S3 loadå¤±æ•—: {e}")
        SEQMAP = {}

def save_seqmap_to_s3():
    try:
        data = pickle.dumps(SEQMAP)
        ok = s3_put_bytes(
            S3_BUCKET_OUTPUTS,
            SEQMAP_S3_KEY,
            data,
            content_type="application/octet-stream"
        )
        if ok:
            log(f"ğŸ’¾ [SEQ] S3ã¸ä¿å­˜: ä»¶æ•°={len(SEQMAP)}")
    except Exception as e:
        log(f"âš ï¸ [SEQ] S3 saveå¤±æ•—: {e}")


# =========================
# row -> CSV bytes, key builder
# =========================
def row_dict_to_csv_bytes(row: Dict[str, Any], include_header: bool = True) -> bytes:
    buf = io.StringIO()
    w = csv.writer(buf, lineterminator="\n")
    if include_header:
        w.writerow(HEADER)
    w.writerow([row.get(col, "") for col in HEADER])
    return buf.getvalue().encode("utf-8")

def _normalize_prefix(prefix: str) -> str:
    if not prefix:
        return ""
    return prefix if prefix.endswith("/") else (prefix + "/")

def build_row_s3_key(target_date: str, mid: str, seq: int) -> str:
    ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    prefix = _normalize_prefix(S3_PREFIX)
    return f"{prefix}{target_date}/mid={mid}/seq={seq:06d}_{ts}.csv"

def upload_row_csv_to_s3(row: Dict[str, Any], mid: str, seq: int, output_date: Optional[str] = None) -> bool:
    # output_date ã‚’å„ªå…ˆï¼ˆä¾‹: "2026-02-25"ï¼‰
    target_date = output_date or datetime.date.today().strftime("%Y-%m-%d")
    key = build_row_s3_key(target_date, mid, seq)
    body = row_dict_to_csv_bytes(row, include_header=INCLUDE_HEADER_IN_EACH_ROW)
    return s3_put_bytes(
        S3_BUCKET_OUTPUTS,
        key,
        body,
        content_type="text/csv; charset=utf-8"
    )

def upload_matched_urls_json_to_s3(matched: List[str]) -> bool:
    target_date = datetime.date.today().strftime("%Y-%m-%d")
    prefix = _normalize_prefix(S3_PREFIX)
    key = f"{prefix}{target_date}/matched_mid_urls_{datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json"
    body = json.dumps(matched, ensure_ascii=False, indent=2).encode("utf-8")
    return s3_put_bytes(S3_BUCKET_OUTPUTS, key, body, content_type="application/json; charset=utf-8")


# =========================
# Resultsåˆ—æŒ™ï¼ˆçµ‚äº†æ¸ˆï¼‰
# =========================
def goto_results_tab(page) -> bool:
    kill_consent_banners(page)

    ok = safe_goto(page, "https://www.flashscore.co.jp/football/", timeout_ms=45000, tag="FOOTBALL")
    if not ok:
        return False
    kill_consent_banners(page)

    if is_bot_wall(page):
        log("ğŸ§± [RESULTS] BOT wall on football page")
        return False

    # â‘  ã‚¿ãƒ–ã‚¯ãƒªãƒƒã‚¯å„ªå…ˆï¼ˆæ—¥æœ¬èªUIæƒ³å®šï¼‰
    tab_candidates = [
        "div.filters__tab:has-text('çµæœ')",
        "div.filters__tab:has-text('RESULTS')",
        "div.filters__tab:has-text('Results')",
        "a:has-text('çµæœ')",
        "button:has-text('çµæœ')",
    ]
    clicked = False
    for sel in tab_candidates:
        try:
            loc = page.locator(sel).first
            if loc.count():
                loc.scroll_into_view_if_needed(timeout=1500)
                loc.click(timeout=3000, force=True)
                clicked = True
                break
        except:
            pass

    # â‘¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šhashç›´å©ã
    if not clicked:
        ok = safe_goto(page, "https://www.flashscore.co.jp/football/#/results", timeout_ms=45000, tag="RESULTS")
        if not ok:
            return False

    kill_consent_banners(page)

    # SPAæç”»å¾…ã¡ï¼ˆnetworkidleã¯åŠ¹ã‹ãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§è»½ãå¾…ã¤ï¼‰
    page.wait_for_timeout(1500)
    kill_consent_banners(page)

    # dayPicker ã¯ visible ã§å¾…ã¤ã¨è½ã¡ã‚‹ã®ã§ attached ã§OK
    try:
        page.wait_for_selector("[data-testid='wcl-dayPicker']", timeout=30000, state="attached")
        page.wait_for_selector("[data-testid='wcl-dayPickerButton']", timeout=30000, state="attached")
        page.wait_for_selector("button[data-day-picker-arrow='prev']", timeout=30000, state="attached")
        page.wait_for_selector("button[data-day-picker-arrow='next']", timeout=30000, state="attached")
        log(f"âœ… [RESULTS] dayPicker attached. text='{get_day_picker_text(page)}'")
        return True
    except Exception as e:
        log(f"âš ï¸ [RESULTS] dayPicker not found: {type(e).__name__}: {e}")
        log(f"[DEBUG] url={page.url}")
        try:
            html_head = (page.content() or "")[:1200]
            log("[DEBUG] html_head=" + html_head.replace("\n", " ")[:1200])
        except:
            pass
        return False

def click_show_more_results(page, max_load_more_clicks: int):
    for _ in range(max_load_more_clicks):
        kill_consent_banners(page)
        page.wait_for_timeout(350)

        btn_candidates = [
            "text=ã‚‚ã£ã¨è¡¨ç¤º",
            "text=ã•ã‚‰ã«è¡¨ç¤º",
            "text=Show more matches",
            "text=Show more",
            "button.event__more",
            ".event__more",
            ".event__more--static",
        ]
        clicked = False
        for sel in btn_candidates:
            try:
                b = page.locator(sel).first
                if b.count() and b.is_visible(timeout=900):
                    b.scroll_into_view_if_needed(timeout=900)
                    b.click(timeout=2500, force=True)
                    clicked = True
                    break
            except:
                pass
        if not clicked:
            break

def collect_finished_match_links(max_load_more_clicks: int) -> List[str]:
    out: List[str] = []

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(
            user_agent=USER_AGENT,
            locale=LOCALE,
            timezone_id=TIMEZONE_ID
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        page = ctx.new_page()

        log("ğŸŒ Flashscore football ã‚’é–‹ãã¾ã™...")
        ok = safe_goto(page, "https://www.flashscore.co.jp/football/", timeout_ms=45000, tag="FOOTBALL")
        if not ok:
            log("âŒ footballãƒšãƒ¼ã‚¸ãŒé–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ")
            try: browser.close()
            except: pass
            return []

        goto_results_tab(page)

        try:
            page.wait_for_selector("a[href*='/match/']", timeout=20000)
        except:
            pass

        click_show_more_results(page, MAX_LOAD_MORE_CLICKS)

        hrefs = set()
        # eventRowLinkå„ªå…ˆ
        try:
            loc = page.locator("a.eventRowLink[href*='/match/']")
            n = loc.count()
            for i in range(n):
                h = loc.nth(i).get_attribute("href") or ""
                if "/match/" in h:
                    hrefs.add(h)
        except:
            pass

        # fallback
        try:
            loc = page.locator("a[href*='/match/']")
            n = loc.count()
            for i in range(n):
                h = loc.nth(i).get_attribute("href") or ""
                if "/match/" in h:
                    hrefs.add(h)
        except:
            pass

        for h in sorted(hrefs):
            if h.startswith("http"):
                out.append(h)
            elif h.startswith("/"):
                out.append("https://www.flashscore.co.jp" + h)
            else:
                out.append("https://www.flashscore.co.jp/" + h)

        log(f"âœ… [RESULTS] collected match links: {len(out)}")

        try: browser.close()
        except: pass

    return out


# =========================
# midãƒªãƒ³ã‚¯çªåˆï¼ˆçµ‚äº†æ¸ˆãƒšãƒ¼ã‚¸å†…ï¼‰
# =========================
def find_matching_mid_url_on_page(pg, target_mids_set: set) -> Optional[str]:
    kill_consent_banners(pg)

    # ã¾ãšURLè‡ªä½“ã«midãŒä»˜ã„ã¦ã„ã¦ä¸€è‡´ã™ã‚‹ãªã‚‰ãã‚Œã§OK
    cur_mid = extract_mid(pg.url) or ""
    if cur_mid and cur_mid in target_mids_set:
        return pg.url

    # ãƒšãƒ¼ã‚¸å†…ã® a[href*="mid="] ã‚’èµ°æŸ»
    try:
        links = pg.locator("a[href*='mid=']")
        n = links.count()
    except:
        n = 0

    for i in range(n):
        try:
            href = links.nth(i).get_attribute("href") or ""
            mid = extract_mid(href) or ""
            if mid and (mid in target_mids_set):
                if href.startswith("http"):
                    return href
                if href.startswith("/"):
                    return "https://www.flashscore.co.jp" + href
                return "https://www.flashscore.co.jp/" + href
        except:
            pass

    return None


# =========================
# Workerï¼š1è©¦åˆå‡¦ç†ï¼ˆçµ‚äº†æ¸ˆï¼‰
# =========================
def process_one_match_in_worker(match_root_url: str, target_mids: List[str]) -> Dict[str, Any]:
    """
    match_root_url: Resultsä¸€è¦§ã‹ã‚‰æ‹¾ã£ãŸ /match/... ï¼ˆmidç„¡ã—ã§ã‚‚OKï¼‰
    target_mids: S3 JSON ã® matchId(mid) é›†åˆï¼ˆè¦ªã§ pending ã‚’æ¸¡ã™ï¼‰
    """
    result: Dict[str, Any] = {"ok": False, "url": match_root_url}
    target_set = set(target_mids or [])

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(
            user_agent=USER_AGENT,
            locale=LOCALE,
            timezone_id=TIMEZONE_ID
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        pg = ctx.new_page()

        try:
            log("ğŸ§© [WORKER] open finished match root")
            ok = safe_goto(pg, match_root_url, timeout_ms=NAV_TIMEOUT_MS, tag="MATCH(ROOT)")
            if not ok:
                return {"ok": False, "url": match_root_url, "error": "match_goto_failed"}

            kill_consent_banners(pg)
            if is_bot_wall(pg):
                return {"ok": False, "url": match_root_url, "error": "bot_wall"}

            # midãƒªãƒ³ã‚¯çªåˆ
            mid_url = find_matching_mid_url_on_page(pg, target_set)
            if not mid_url:
                return {"ok": False, "url": match_root_url, "error": "no_matching_mid_link"}

            log(f"ğŸ¯ [WORKER] matched mid link -> {mid_url}")
            ok = safe_goto(pg, mid_url, timeout_ms=NAV_TIMEOUT_MS, tag="MATCH(MID)")
            if not ok:
                return {"ok": False, "url": match_root_url, "mid_url": mid_url, "error": "mid_goto_failed"}

            kill_consent_banners(pg)
            if is_bot_wall(pg):
                return {"ok": False, "url": match_root_url, "mid_url": mid_url, "error": "bot_wall_after_mid"}

            mid = extract_mid(pg.url) or ""
            rb = RowBuilder(HEADER, match_tag=mid)
            rb.put("è©¦åˆID", mid)
            rb.put("è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—", pg.url)

            # teams/scores/time
            log("ğŸ” [WORKER] read teams/scores/time")
            home, away = get_home_away_names(pg)
            hs, aw = get_scores(pg)
            ttxt = get_match_time_text(pg)

            rb.put("ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ", home)
            rb.put("ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ", away)
            rb.put("ãƒ›ãƒ¼ãƒ ã‚¹ã‚³ã‚¢", hs)
            rb.put("ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ã‚³ã‚¢", aw)
            rb.put("è©¦åˆæ™‚é–“", ttxt)

            # meta
            meta = get_match_meta(pg)
            country = (meta.get("å›½") or "").strip()
            league  = (meta.get("ãƒªãƒ¼ã‚°") or "").strip()

            if country and league:
                meta_category = f"{country}: {league}"
            elif league:
                meta_category = league
            elif country:
                meta_category = country
            else:
                meta_category = ""  # æœ€å¾Œã¾ã§å–ã‚Œãªã‹ã£ãŸå ´åˆã®ã¿ç©º

            rb.put("è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª", meta_category)
            rb.put("ã‚¹ã‚³ã‚¢æ™‚é–“", meta.get("å–å¾—æ™‚åˆ»", ""))

            if meta.get("é–‹å‚¬åœ°"):
                rb.put("ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ", meta.get("é–‹å‚¬åœ°"))
            if meta.get("åå®¹äººæ•°"):
                rb.put("åå®¹äººæ•°", meta.get("åå®¹äººæ•°"))
            if meta.get("è¦³å®¢"):
                rb.put("è¦³å®¢æ•°", meta.get("è¦³å®¢"))
            if meta.get("ãƒ¬ãƒ•ã‚§ãƒªãƒ¼"):
                rb.put("å¯©åˆ¤å", meta.get("ãƒ¬ãƒ•ã‚§ãƒªãƒ¼"))

            # standings
            log("ğŸ“Œ [WORKER] standings")
            st = get_match_standings(pg, home, away)
            if st:
                if st.get("home_rank") is not None:
                    rb.put("ãƒ›ãƒ¼ãƒ é †ä½", st.get("home_rank"))
                if st.get("away_rank") is not None:
                    rb.put("ã‚¢ã‚¦ã‚§ãƒ¼é †ä½", st.get("away_rank"))

            # statsï¼ˆåŠ å·¥ãªã—ï¼‰
            log("ğŸ“ˆ [WORKER] scrape stats")
            stats_raw = scrape_stats_raw(pg)
            canon_pairs = normalize_stats_raw_to_canon(stats_raw)

            verify_stats_mapping("CANON", canon_pairs)
            apply_stats_to_row(rb, canon_pairs)

            # é€šç•ªã¯è¦ªã§ä»˜ä¸
            rb.put("é€šç•ª", "")
            rb.put("ã‚½ãƒ¼ãƒˆç”¨ç§’", parse_live_time_to_seconds(rb.d.get("è©¦åˆæ™‚é–“", "")))

            verify_row_filled(rb.d)

            result.update({
                "ok": True,
                "mid": mid,
                "mid_url": pg.url,
                "category": meta_category,
                "row": rb.d,
            })
            return result

        except Exception as e:
            result["error"] = f"{type(e).__name__}: {e}"
            result["trace"] = traceback.format_exc(limit=10)
            log(f"ğŸ’¥ [WORKER] exception: {result['error']}")
            log(result["trace"])
            return result

        finally:
            try: pg.close()
            except: pass
            try: browser.close()
            except: pass


def _worker_entry(match_root_url: str, target_mids: List[str], q: "mp.Queue"):
    res = process_one_match_in_worker(match_root_url, target_mids=target_mids)
    try:
        q.put(res)
    except:
        pass

def run_match_with_timeout(match_root_url: str, target_mids: List[str], timeout_sec: int = WORKER_TIMEOUT_SEC) -> Dict[str, Any]:
    q: mp.Queue = mp.Queue(maxsize=1)
    p = mp.Process(target=_worker_entry, args=(match_root_url, target_mids, q), daemon=True)
    p.start()
    p.join(timeout=timeout_sec)

    if p.is_alive():
        log(f"ğŸ§¨ [TIMEOUT] å­ãƒ—ãƒ­ã‚»ã‚¹å¼·åˆ¶çµ‚äº†: {match_root_url} ({timeout_sec}s)")
        try:
            p.terminate()
        except:
            pass
        p.join(3)
        return {"ok": False, "url": match_root_url, "error": f"timeout({timeout_sec}s)"}

    try:
        return q.get(timeout=2)
    except pyqueue.Empty:
        return {"ok": False, "url": match_root_url, "error": "no_result_from_worker"}

def load_targets_by_date_from_s3() -> Dict[str, List[Dict[str, str]]]:
    """
    return:
      {
        "2026-02-25": [
          {"matchId":"vmwaNZho", "matchUrl":"https://.../?mid=vmwaNZho"},
          {"matchId":"xxxx"}
        ],
        ...
      }
    """
    b = s3_get_bytes(FIN_JSON_BUCKET, FIN_JSON_KEY)
    if not b:
        log(f"âŒ [FIN] not found: s3://{FIN_JSON_BUCKET}/{FIN_JSON_KEY}")
        return {}

    try:
        obj = json.loads(b.decode("utf-8"))
    except Exception as e:
        log(f"âŒ [FIN] json parse failed: {e}")
        return {}

    out: Dict[str, List[Dict[str, str]]] = {}

    if isinstance(obj, dict):
        for dk, arr in obj.items():
            if not _as_date_key(dk):
                continue
            targets: List[Dict[str, str]] = []
            if isinstance(arr, list):
                for it in arr:
                    if isinstance(it, dict):
                        mid = (it.get("matchId") or it.get("mid") or "").strip()
                        url = (it.get("matchUrl") or it.get("url") or "").strip()
                        if mid:
                            t = {"matchId": mid}
                            if url:
                                t["matchUrl"] = url
                            targets.append(t)
                    elif isinstance(it, str) and it.strip():
                        targets.append({"matchId": it.strip()})
            out[dk] = list(dict.fromkeys([json.dumps(t, sort_keys=True, ensure_ascii=False) for t in targets]))
            # â†‘ ä¸€æ—¦JSONæ–‡å­—åˆ—ã§é‡è¤‡é™¤å» â†’ å…ƒã«æˆ»ã™
            out[dk] = [json.loads(s) for s in out[dk]]

        log(f"âœ… [FIN] loaded targets: days={len(out)} total={sum(len(v) for v in out.values())}")
        return out

    log("âš ï¸ [FIN] JSON shape not supported (expected date-keyed dict).")
    return {}

# =========================
# Main
# =========================
def main():
    try:
        if FORCE_SPAWN:
            mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass

    log(f"ENV HEADLESS={os.environ.get('HEADLESS')} -> {HEADLESS}, SLOW_MO_MS={os.environ.get('SLOW_MO_MS')} -> {SLOW_MO_MS}")
    log(f"S3_BUCKET_OUTPUTS={S3_BUCKET_OUTPUTS}, S3_PREFIX='{S3_PREFIX}', SEQMAP_S3_KEY={SEQMAP_S3_KEY}")

    verify_header_and_stat_map()
    load_seqmap_from_s3()

    targets_by_date = load_targets_by_date_from_s3()
    if not targets_by_date:
        log("âŒ targets_by_date ãŒç©ºã§ã™ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return

    date_keys = sorted(targets_by_date.keys())
    log(f"ğŸ—“ï¸ target days = {len(date_keys)} : {date_keys[:10]}{'...' if len(date_keys)>10 else ''}")

    matched_mid_urls: List[str] = []

    for dk in date_keys:
        targets = targets_by_date.get(dk, [])
        if not targets:
            continue

        # 1) matchUrl ãŒã‚ã‚‹ã‚‚ã®ã¯ã€RESULTSåˆ—æŒ™ãªã—ã§ç›´æ¥å‡¦ç†ï¼ˆæœ€é€Ÿãƒ»å®‰å®šï¼‰
        direct = [t for t in targets if t.get("matchUrl")]
        indirect = [t for t in targets if not t.get("matchUrl")]

        log("\n==============================")
        log(f"ğŸ“Œ DATE {dk}: direct={len(direct)} indirect={len(indirect)}")
        log("==============================")

        # --- direct ---
        for t in direct:
            mid = t["matchId"]
            url = t["matchUrl"]
            res = run_match_with_timeout(url, target_mids=[mid], timeout_sec=WORKER_TIMEOUT_SEC)
            if not res.get("ok"):
                log(f"âš ï¸ [DIRECT] fail mid={mid} err={res.get('error')}")
                continue

            row = res.get("row", {}) or {}
            mid_url = res.get("mid_url", "") or url

            # é€šç•ªç¢ºå®šï¼ˆè¦ªã®ã¿æ›´æ–°ï¼‰
            last_seq = int(SEQMAP.get(mid, 0)) if mid else 0
            seq = last_seq + 1
            if mid:
                SEQMAP[mid] = seq

            row["é€šç•ª"] = seq
            row["è©¦åˆID"] = mid

            ok = upload_row_csv_to_s3(row, mid=mid, seq=seq, output_date=dk)
            if not ok:
                log(f"âŒ [DIRECT] upload failed mid={mid}")
                continue

            matched_mid_urls.append(mid_url)

        # --- indirectï¼ˆmidã ã‘ï¼‰---
        if indirect:
            # ã“ã®æ—¥ä»˜ã® pending mids
            pending = set([t["matchId"] for t in indirect if t.get("matchId")])
            if not pending:
                continue

            # RESULTS ãã®æ—¥ã«ç§»å‹•ã—ã¦ /match/... ã‚’åˆ—æŒ™ï¼ˆã‚ãªãŸã®å®Ÿè£…ã‚’ä½¿ã†ï¼‰
            roots_by_date = collect_finished_match_roots_by_dates([dk])
            roots = roots_by_date.get(dk, []) or []

            log(f"ğŸ“„ [INDIRECT] roots={len(roots)} pending={len(pending)}")

            for match_root_url in roots:
                if STOP_WHEN_ALL_MIDS_FOUND and not pending:
                    log(f"âœ… [INDIRECT] pending empty -> stop date={dk}")
                    break

                res = run_match_with_timeout(match_root_url, target_mids=list(pending), timeout_sec=WORKER_TIMEOUT_SEC)
                if not res.get("ok"):
                    continue

                mid = (res.get("mid", "") or "").strip()
                if not mid:
                    continue

                row = res.get("row", {}) or {}
                mid_url = res.get("mid_url", "") or ""

                if mid in pending:
                    pending.remove(mid)

                # é€šç•ªç¢ºå®šï¼ˆè¦ªã®ã¿æ›´æ–°ï¼‰
                last_seq = int(SEQMAP.get(mid, 0)) if mid else 0
                seq = last_seq + 1
                if mid:
                    SEQMAP[mid] = seq

                row["é€šç•ª"] = seq
                row["è©¦åˆID"] = mid

                ok = upload_row_csv_to_s3(row, mid=mid, seq=seq, output_date=dk)
                if not ok:
                    log(f"âŒ [INDIRECT] upload failed mid={mid}")
                    continue

                if mid_url:
                    matched_mid_urls.append(mid_url)

            if pending:
                log(f"âš ï¸ [INDIRECT] not found mids (first 30): {sorted(list(pending))[:30]}")

    # matched urls ã‚’ã¾ã¨ã‚ã¦S3ã¸ï¼ˆä»»æ„ï¼‰
    if matched_mid_urls:
        matched_mid_urls = list(dict.fromkeys(matched_mid_urls))
        upload_matched_urls_json_to_s3(matched_mid_urls)

    save_seqmap_to_s3()
    log("ğŸ‰ å®Œäº†")


if __name__ == "__main__":
    main()
