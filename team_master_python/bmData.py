# -*- coding: utf-8 -*-
from playwright.sync_api import sync_playwright
import time, re
# è¿½åŠ 
from urllib.parse import urlsplit, urlunsplit
import datetime, os
import pandas as pd
from typing import Optional, List

BOT_WALL_PAT = re.compile(r"(Just a moment|Access Denied|verify you are human|ãƒã‚§ãƒƒã‚¯|ç¢ºèª)", re.I)
STAT_CONTAINER = "div.section"

SAVE_DIR = "/Users/shiraishitoshio/bookmaker/outputs"

# ====== ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ======
SEQMAP = {}  # è©¦åˆIDã”ã¨ã®é€£ç•ªç®¡ç†ç”¨

# ===== HEADER =====
HEADER = [
    "ãƒ›ãƒ¼ãƒ é †ä½","è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª","è©¦åˆæ™‚é–“","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ","ãƒ›ãƒ¼ãƒ ã‚¹ã‚³ã‚¢","ã‚¢ã‚¦ã‚§ãƒ¼é †ä½","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ",
    "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ã‚³ã‚¢","ãƒ›ãƒ¼ãƒ æœŸå¾…å€¤","ã‚¢ã‚¦ã‚§ãƒ¼æœŸå¾…å€¤","ãƒ›ãƒ¼ãƒ æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤","ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤",
    "ãƒ›ãƒ¼ãƒ ãƒœãƒ¼ãƒ«æ”¯é…ç‡","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒ¼ãƒ«æ”¯é…ç‡","ãƒ›ãƒ¼ãƒ ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "ãƒ›ãƒ¼ãƒ æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ãƒ›ãƒ¼ãƒ æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°",
    "ãƒ›ãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ","ãƒ›ãƒ¼ãƒ ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹",
    "ãƒ›ãƒ¼ãƒ ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯","ã‚¢ã‚¦ã‚§ãƒ¼ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯","ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ",
    "ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ","ãƒ›ãƒ¼ãƒ ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ","ãƒ›ãƒ¼ãƒ ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«",
    "ãƒ›ãƒ¼ãƒ ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–","ã‚¢ã‚¦ã‚§ãƒ¼ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–","ãƒ›ãƒ¼ãƒ ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯",
    "ãƒ›ãƒ¼ãƒ ã‚ªãƒ•ã‚µã‚¤ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ã‚ªãƒ•ã‚µã‚¤ãƒ‰","ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¦ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¦ãƒ«",
    "ãƒ›ãƒ¼ãƒ ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰","ãƒ›ãƒ¼ãƒ ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰","ãƒ›ãƒ¼ãƒ ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³",
    "ãƒ›ãƒ¼ãƒ ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ","ã‚¢ã‚¦ã‚§ãƒ¼ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ","ãƒ›ãƒ¼ãƒ ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‘ã‚¹","ãƒ›ãƒ¼ãƒ ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹","ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹",
    "ãƒ›ãƒ¼ãƒ ã‚¯ãƒ­ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒ­ã‚¹","ãƒ›ãƒ¼ãƒ ã‚¿ãƒƒã‚¯ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¿ãƒƒã‚¯ãƒ«","ãƒ›ãƒ¼ãƒ ã‚¯ãƒªã‚¢","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒªã‚¢","ãƒ›ãƒ¼ãƒ ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°",
    "ãƒ›ãƒ¼ãƒ ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ",
    "ã‚¹ã‚³ã‚¢æ™‚é–“","å¤©æ°—","æ°—æ¸©","æ¹¿åº¦","å¯©åˆ¤å","ãƒ›ãƒ¼ãƒ ç›£ç£å","ã‚¢ã‚¦ã‚§ãƒ¼ç›£ç£å","ãƒ›ãƒ¼ãƒ ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³",
    "ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ","åå®¹äººæ•°","è¦³å®¢æ•°","å ´æ‰€","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…å‡ºå ´çŠ¶æ³","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…å‡ºå ´çŠ¶æ³",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹",
    "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹","é€šçŸ¥ãƒ•ãƒ©ã‚°","è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—","ã‚´ãƒ¼ãƒ«æ™‚é–“","é¸æ‰‹å","åˆ¤å®šçµæœ","ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¹ã‚¿ã‚¤ãƒ«","ã‚¢ã‚¦ã‚§ã‚¤ãƒãƒ¼ãƒ ã‚¹ã‚¿ã‚¤ãƒ«",
    "ã‚´ãƒ¼ãƒ«ç¢ºç‡","å¾—ç‚¹äºˆæƒ³æ™‚é–“","è©¦åˆID","é€šç•ª","ã‚½ãƒ¼ãƒˆç”¨ç§’"
]

STAT_KEY_MAP = {
    # æ”»æ’ƒãƒ»å¾—ç‚¹é–¢é€£
    "ã‚¢ã‚¿ãƒƒã‚¯:æœŸå¾…å€¤ï¼ˆxGï¼‰": ("ãƒ›ãƒ¼ãƒ æœŸå¾…å€¤", "ã‚¢ã‚¦ã‚§ãƒ¼æœŸå¾…å€¤"),
    "ã‚¢ã‚¿ãƒƒã‚¯:æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤": ("ãƒ›ãƒ¼ãƒ æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤", "ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤"),
    "ãƒã‚¼ãƒƒã‚·ãƒ§ãƒ³:ãƒœãƒ¼ãƒ«æ”¯é…ç‡": ("ãƒ›ãƒ¼ãƒ ãƒœãƒ¼ãƒ«æ”¯é…ç‡", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒ¼ãƒ«æ”¯é…ç‡"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ã‚·ãƒ¥ãƒ¼ãƒˆæ•°": ("ãƒ›ãƒ¼ãƒ ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆæ•°"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°": ("ãƒ›ãƒ¼ãƒ æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", "ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°": ("ãƒ›ãƒ¼ãƒ æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", "ã‚¢ã‚¦ã‚§ãƒ¼æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ": ("ãƒ›ãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ"),
    "ã‚¢ã‚¿ãƒƒã‚¯:ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹": ("ãƒ›ãƒ¼ãƒ ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹"),
    "ã‚»ãƒƒãƒˆãƒ—ãƒ¬ãƒ¼:ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯": ("ãƒ›ãƒ¼ãƒ ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ": ("ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ": ("ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒã‚¹ãƒˆãƒ’ãƒƒãƒˆ": ("ãƒ›ãƒ¼ãƒ ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ"),
    "ã‚·ãƒ¥ãƒ¼ãƒˆ:ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«": ("ãƒ›ãƒ¼ãƒ ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«"),

    # ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹ãƒ»åå‰‡
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–": ("ãƒ›ãƒ¼ãƒ ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯": ("ãƒ›ãƒ¼ãƒ ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚ªãƒ•ã‚µã‚¤ãƒ‰": ("ãƒ›ãƒ¼ãƒ ã‚ªãƒ•ã‚µã‚¤ãƒ‰", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚ªãƒ•ã‚µã‚¤ãƒ‰"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ•ã‚¡ã‚¦ãƒ«": ("ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¦ãƒ«", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¦ãƒ«"),
    "ã‚«ãƒ¼ãƒ‰:ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰": ("ãƒ›ãƒ¼ãƒ ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰"),
    "ã‚«ãƒ¼ãƒ‰:ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰": ("ãƒ›ãƒ¼ãƒ ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³": ("ãƒ›ãƒ¼ãƒ ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³"),

    # ãƒ‘ã‚¹ãƒ»ãƒ“ãƒ«ãƒ‰ã‚¢ãƒƒãƒ—
    "ãƒ‘ã‚¹:ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ": ("ãƒ›ãƒ¼ãƒ ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ", "ã‚¢ã‚¦ã‚§ãƒ¼ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ"),
    "ãƒ‘ã‚¹:ç·ãƒ‘ã‚¹æ•°": ("ãƒ›ãƒ¼ãƒ ãƒ‘ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‘ã‚¹"),
    "ãƒ‘ã‚¹:ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹": ("ãƒ›ãƒ¼ãƒ ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹"),
    "ãƒ‘ã‚¹:ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹": ("ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹"),
    "ãƒ‘ã‚¹:ã‚¯ãƒ­ã‚¹": ("ãƒ›ãƒ¼ãƒ ã‚¯ãƒ­ã‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒ­ã‚¹"),

    # å®ˆå‚™
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¿ãƒƒã‚¯ãƒ«": ("ãƒ›ãƒ¼ãƒ ã‚¿ãƒƒã‚¯ãƒ«", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¿ãƒƒã‚¯ãƒ«"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¯ãƒªã‚¢": ("ãƒ›ãƒ¼ãƒ ã‚¯ãƒªã‚¢", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒªã‚¢"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©": ("ãƒ›ãƒ¼ãƒ ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°"),
    "ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹:ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ": ("ãƒ›ãƒ¼ãƒ ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ", "ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ"),
}

VERBOSE = True  # ãƒ­ã‚°ã‚’ãŸãã•ã‚“å‡ºã™å ´åˆã¯ True

def log(msg: str):
    if VERBOSE:
        print(msg)

# ================= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =================

def text_clean(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def wait_netidle(pg, ms=2500):
    try:
        pg.wait_for_load_state("networkidle", timeout=ms)
    except:
        pass

# ============== è©¦åˆãƒšãƒ¼ã‚¸ï¼šåŸºæœ¬æƒ…å ± ==============

def get_display_time(pg):
    sels = [
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) [data-testid='wcl-time']",
        ".duelParticipant__container [data-testid='wcl-time']",
        "[data-testid='wcl-time']",
    ]
    for s in sels:
        try:
            el = pg.locator(s).first
            if el.count():
                t = text_clean(el.text_content() or "")
                if t:
                    return re.sub(r"\s+", "", t)
        except:
            pass
    return ""

def get_home_away_names(pg):
    try:
        cont = pg.locator("div.duelParticipant__container").first
        if not cont.count():
            cont = pg
        # ãƒ†ã‚­ã‚¹ãƒˆå„ªå…ˆ
        h = cont.locator(".duelParticipant__home a.participant__participantName").first
        a = cont.locator(".duelParticipant__away a.participant__participantName").first
        home = text_clean(h.text_content()) if h.count() else ""
        away = text_clean(a.text_content()) if a.count() else ""
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚¨ãƒ³ãƒ–ãƒ¬ãƒ ã® alt
        if not home:
            img = cont.locator(".duelParticipant__home img.participant__image").first
            home = text_clean(img.get_attribute("alt") or "")
        if not away:
            img = cont.locator(".duelParticipant__away img.participant__image").first
            away = text_clean(img.get_attribute("alt") or "")
        return home, away
    except:
        return "", ""

def get_scores(pg):
    try:
        fx = pg.locator(".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedScore")
        nums = fx.locator("span:not(.fixedScore__divider)") if fx.count() else pg.locator(".detailScore__wrapper span:not(.detailScore__divider)")
        if nums.count() >= 2:
            hs = text_clean(nums.nth(0).text_content() or "")
            aw = text_clean(nums.nth(1).text_content() or "")
            return hs, aw
    except:
        pass
    return "", ""

# ============== çµ±è¨ˆã‚¿ãƒ–ï¼šé·ç§»ã¨å¾…æ©Ÿ =============
STAT_CONTAINER = "[data-analytics-context='tab-match-statistics']"

def goto_statistics_match_tab(pg):
    """è©¦åˆè©³ç´°URLã‹ã‚‰çµ±è¨ˆãƒšãƒ¼ã‚¸ï¼ˆ/summary/stats/0/ï¼‰ã¸ç›´æ¥é·ç§»"""
    import re
    cur = pg.url
    # ä¾‹: https://www.flashscore.co.jp/match/soccer/imabari-0fQDWIvJ/v-varen-nagasaki-hl74HAcF/?mid=4SyKJEwn
    # â†’ https://www.flashscore.co.jp/match/soccer/imabari-0fQDWIvJ/v-varen-nagasaki-hl74HAcF/summary/stats/0/?mid=4SyKJEwn
    if "/summary/stats" not in cur:
        base, mid_part = cur.split("?", 1) if "?" in cur else (cur, "")
        if not base.endswith("/"):
            base += "/"
        stats_url = re.sub(r"/summary/[^/?]*/?", "/summary/stats/0/", base)
        if stats_url == base:
            # summary ãŒå­˜åœ¨ã—ãªã„URL â†’ è¿½è¨˜
            stats_url = base + "summary/stats/0/"
        if mid_part:
            stats_url = stats_url + "?" + mid_part
        # çµ±è¨ˆãƒšãƒ¼ã‚¸ã«é·ç§»
        pg.goto(stats_url, timeout=45000, wait_until="domcontentloaded")

    # çµ±è¨ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæç”»ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
    pg.wait_for_selector("[data-testid='wcl-statistics']", timeout=20000)
    # å¿µã®ãŸã‚å°‘ã—å¾…ã¤ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰å¯¾ç­–ï¼‰
    time.sleep(1.2)


def scrape_stats_pairs(pg):
    """
    { "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å:æŒ‡æ¨™å": ("ãƒ›ãƒ¼ãƒ å€¤","ã‚¢ã‚¦ã‚§ãƒ¼å€¤") } ã‚’è¿”ã™
    å€¤ã¯ <strong> ã®ãƒ¡ã‚¤ãƒ³ã¨ç›´ä¸‹ã® <span>ï¼ˆæ‹¬å¼§ã¯é™¤å»ï¼‰ã‚’é€£çµã™ã‚‹
    """
    goto_statistics_match_tab(pg)

    def strip_paren(s: str) -> str:
        s = (s or "").strip()
        return s[1:-1] if s.startswith("(") and s.endswith(")") else s

    def get_val(side_el):
        if not side_el or not side_el.count():
            return ""
        main = ""
        sub = ""
        try:
            main = (side_el.locator("strong").first.inner_text() or "").strip()
        except: pass
        try:
            sub = (side_el.locator(":scope > span").first.inner_text() or "").strip()
            sub = strip_paren(sub)
        except: pass
        return f"{main} {sub}".strip()

    stats = {}
    sections = pg.locator(f"{STAT_CONTAINER} div.section")
    for si in range(sections.count()):
        sec = sections.nth(si)
        try:
            sec_title = (sec.locator(".sectionHeader").first.inner_text() or "").strip()
        except:
            sec_title = ""
        rows = sec.locator("[data-testid='wcl-statistics']")
        for i in range(rows.count()):
            r = rows.nth(i)
            # æŒ‡æ¨™å
            try:
                label = (r.locator("[data-testid='wcl-statistics-category'] strong").first.inner_text() or "").strip()
            except:
                label = ""
            if not label:
                continue
            # å·¦å³ã®æ•°å€¤ï¼ˆâ€» data-testid ã ã‘ã§å–å¾—ã€‚å…ˆé ­=ãƒ›ãƒ¼ãƒ ã€æœ«å°¾=ã‚¢ã‚¦ã‚§ãƒ¼ï¼‰
            vals = r.locator("[data-testid='wcl-statistics-value']")
            if vals.count() < 2:
                continue
            home = get_val(vals.first)
            away = get_val(vals.last)

            key = f"{sec_title}:{label}" if sec_title else label
            stats[key] = (home, away)

    return stats

def _read_value_box(box):
    """ãƒ›ãƒ¼ãƒ ï¼ã‚¢ã‚¦ã‚§ã‚¤ã®å€¤ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§æŠ½å‡º"""
    try:
        strongs = box.locator("strong")
        texts = [s.inner_text().strip() for s in strongs.all() if s.inner_text().strip()]
        return " ".join(texts)
    except:
        return ""

def wait_statistics_rows(pg, max_tries=20):
    """[data-testid='wcl-statistics'] ãŒå‡ºã‚‹ã¾ã§ã€ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‹å¾…æ©Ÿã§ç²˜ã‚‹"""
    rows = pg.locator("[data-testid='wcl-statistics']")
    for _ in range(max_tries):
        try:
            c = rows.count()
            if c > 0:
                return True
        except: pass
        try: pg.mouse.wheel(0, 1400)
        except: pass
        try: pg.wait_for_load_state("networkidle", timeout=1200)
        except: pass
        time.sleep(0.25)
    return rows.count() > 0

def _read_value_box(box):
    """1ã¤ã®å´(ãƒ›ãƒ¼ãƒ /ã‚¢ã‚¦ã‚§ãƒ¼)ã‚’èª­ã‚€: strong + (span) -> '82% 264/323'"""
    try:
        main = (box.locator("strong").first.inner_text() or "").strip()
    except: main = ""
    try:
        sub  = (box.locator(":scope > span").first.inner_text() or "").strip()
        # (264/323) â†’ 264/323
        if sub.startswith("(") and sub.endswith(")"):
            sub = sub[1:-1].strip()
    except: sub = ""
    if sub and main:
        return f"{main} {sub}"
    return main or sub

def scrape_stats_pairs(pg):
    """
    ã™ã¹ã¦ã® section(ä¾‹: ã‚·ãƒ¥ãƒ¼ãƒˆ/ã‚¢ã‚¿ãƒƒã‚¯/ãƒ‘ã‚¹/ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹/ã‚­ãƒ¼ãƒ‘ãƒ¼) å†…ã‹ã‚‰
    data-testid='wcl-statistics' ã‚’æŠ½å‡ºã€‚
    """
    goto_statistics_match_tab(pg)
    wait_statistics_rows(pg)

    stats = {}
    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’èµ°æŸ»
    sections = pg.locator("div.section")
    nsec = sections.count()

    for si in range(nsec):
        sec = sections.nth(si)
        try:
            sec_title = (sec.locator(".sectionHeader").first.inner_text() or "").strip()
        except:
            sec_title = ""
        # è¡Œå˜ä½ã§çµ±è¨ˆã‚’èª­ã‚€
        rows = sec.locator("[data-testid='wcl-statistics']")
        nrow = rows.count()
        for i in range(nrow):
            r = rows.nth(i)
            try:
                label = (r.locator("[data-testid='wcl-statistics-category'] strong").first.inner_text() or "").strip()
            except:
                label = ""
            if not label:
                continue
            vals = r.locator("[data-testid='wcl-statistics-value']")
            home = _read_value_box(vals.first)
            away = _read_value_box(vals.last)

            key = label
            if sec_title:
                key = f"{sec_title}:{label}"
            stats[key] = (home, away)

    return stats

def wait_statistics_rows(pg, max_tries=12):
    """
    çµ±è¨ˆè¡ŒãŒæç”»ã•ã‚Œã‚‹ã¾ã§ã€ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« & å¾…æ©Ÿã§ç²˜ã‚‹ã€‚
    """
    rows = pg.locator("[data-analytics-context='tab-match-statistics'] [data-testid='wcl-statistics']")
    for _ in range(max_tries):
        try:
            if rows.count() > 0:
                return True
        except:
            pass
        try:
            pg.mouse.wheel(0, 1600)
        except:
            pass
        time.sleep(0.25)
        try:
            pg.wait_for_load_state("networkidle", timeout=2000)
        except:
            pass
    try:
        return rows.count() > 0
    except:
        return False

# ============== çµ±è¨ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ ==============

def scrape_stats_pairs(pg):
    """
    { "ãƒ©ãƒ™ãƒ«": ("ãƒ›ãƒ¼ãƒ å€¤","ã‚¢ã‚¦ã‚§ãƒ¼å€¤") } ã‚’è¿”ã™ã€‚
    å€¤ã¯ <strong>ã®ãƒ¡ã‚¤ãƒ³å€¤ + <span>ã®è£œåŠ©å€¤ï¼ˆã‚ã‚Œã°ï¼‰ã‚’ã€Œæ‹¬å¼§ã‚’å¤–ã—ãŸå½¢ã€ã§é€£çµã€‚
    ä¾‹: ã€Œ82% 232/282ã€
    """
    goto_statistics_match_tab(pg)
    ok = wait_statistics_rows(pg, max_tries=10)
    if not ok:
        return {}

    js = r"""
(() => {
  const clean = s => (s || "").replace(/\u00A0/g, " ").replace(/\s+/g, " ").trim();
  const stripParen = s => {
    const m = /^\((.*)\)$/.exec((s || "").trim());
    return m ? m[1] : (s || "");
  };
  const getVal = (el) => {
    if (!el) return "";
    const main = clean(el.querySelector("strong")?.textContent || "");
    const sub = clean(el.querySelector(":scope > span")?.textContent || "");
    const sub2 = stripParen(sub);
    return sub2 ? (main ? `${main} (${sub2})` : sub2) : main;
  };

  const out = {};
  document.querySelectorAll("div.section").forEach(section => {
    const title = clean(section.querySelector(".sectionHeader")?.textContent || "");
    if (!title) return;
    const catMap = {};
    section.querySelectorAll("div.wcl-row_2oCpS[data-testid='wcl-statistics']").forEach(r => {
      const label = clean(r.querySelector("[data-testid='wcl-statistics-category'] strong")?.textContent || "");
      if (!label) return;
      const home = getVal(r.querySelector(".wcl-homeValue_3Q-7P"));
      const away = getVal(r.querySelector(".wcl-awayValue_Y-QR1"));
      catMap[label] = [home, away];
    });
    if (Object.keys(catMap).length > 0) {
      out[title] = catMap;
    }
  });
  return out;
})();
"""

    try:
        return pg.evaluate(js, STAT_CONTAINER) or {}
    except:
        return {}

def save_to_excel(match_results, output_dir="."):
    """è©¦åˆãƒ‡ãƒ¼ã‚¿ï¼ˆlist[dict]ï¼‰ã‚’ output_x.xlsx å½¢å¼ã§ä¿å­˜"""
    os.makedirs(output_dir, exist_ok=True)
    existing = [f for f in os.listdir(output_dir) if f.startswith("output_") and f.endswith(".xlsx")]
    next_num = len(existing) + 1
    out_path = os.path.join(output_dir, f"output_{next_num}.xlsx")

    # DataFrame åŒ– & HEADER é †ã«
    df = pd.DataFrame(match_results)
    for col in HEADER:
        if col not in df.columns:
            df[col] = ""
    df = df[HEADER]

    # åˆ—ã”ã¨ã®éç©ºä»¶æ•°ã‚’ãƒ­ã‚°
    try:
        non_empty_counts = df.apply(lambda s: s.astype(str).str.strip().ne("").sum())
        log("ğŸ“„ [EXCEL] åˆ—ã”ã¨ã®éç©ºä»¶æ•°ï¼ˆä¸Šä½10åˆ—ï¼‰:")
        top10 = non_empty_counts.sort_values(ascending=False).head(10)
        for col, cnt in top10.items():
            log(f"   - {col}: {cnt}")
        log(f"ğŸ“„ [EXCEL] ç·è¡Œæ•°: {len(df)} / ç·åˆ—æ•°: {len(df.columns)}")
    except Exception as e:
        log(f"âš ï¸ [EXCEL] éç©ºä»¶æ•°è¨ˆç®—ã§ä¾‹å¤–: {e}")

    # Excelä¿å­˜
    df.to_excel(out_path, index=False)
    print(f"ğŸ’¾ Excelä¿å­˜å®Œäº†: {out_path}")

# ============== ãƒ¡ã‚¤ãƒ³ï¼ˆãƒ©ã‚¤ãƒ–æ¤œå‡ºâ†’è©¦åˆå·¡å›ï¼‰ ==============

def main():
    global SEQMAP
    results = []  # å…¨è©¦åˆçµæœã®ãƒªã‚¹ãƒˆ

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, slow_mo=70)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"),
            locale="ja-JP", timezone_id="Asia/Tokyo"
        )
        page = ctx.new_page()

        print("ğŸŒ Flashscoreãƒˆãƒƒãƒ—ã‚’é–‹ãã¾ã™...")
        page.goto("https://www.flashscore.co.jp/", timeout=45000, wait_until="domcontentloaded")

        # CookieãƒãƒŠãƒ¼å¯¾å¿œ
        try:
            page.locator("#onetrust-accept-btn-handler").click(timeout=1500)
        except:
            pass

        # ãƒ©ã‚¤ãƒ–ã‚¿ãƒ–ã¸
        try:
            live_sel = "div.filters__tab:has(div.filters__text--short:has-text('ãƒ©ã‚¤ãƒ–')), div.filters__tab:has(div.filters__text--long:has-text('é–‹å‚¬ä¸­ã®è©¦åˆ'))"
            page.locator(live_sel).first.click(timeout=4000)
        except:
            pass

        try:
            page.wait_for_selector("div.event__match.event__match--live", timeout=20000)
        except:
            pass

        # =========================
        # ğŸ”¹ ãƒ©ã‚¤ãƒ–ã‚¿ãƒ–ã‚’ç¢ºå®Ÿã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
        # =========================
        try:
            # ã€Œãƒ©ã‚¤ãƒ– / é–‹å‚¬ä¸­ã®è©¦åˆã€ã®ã‚¿ãƒ–ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒƒã‚¯ï¼ˆè¤‡æ•°å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
            live_tab = page.locator("div.filters__tab").filter(
                has_text=re.compile(r"(ãƒ©ã‚¤ãƒ–|é–‹å‚¬ä¸­ã®è©¦åˆ)")
            ).first

            for _ in range(3):
                cls = (live_tab.get_attribute("class") or "")
                if "filters__tab--active" in cls:
                    break
                live_tab.click(timeout=2000)
                # ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆå¾Œã®æç”»å¾…ã¡
                try:
                    page.wait_for_selector("div.event__match", timeout=4000)
                except:
                    pass
                time.sleep(0.4)

            # çŠ¶æ…‹ãƒ­ã‚°
            total_rows = page.locator("div.event__match").count()
            live_rows  = page.locator("div.event__match.event__match--live").count()
            print(f"ğŸ“‹ è¡Œæ¤œå‡º: live={live_rows} / total={total_rows}")
        except Exception as e:
            print(f"âš ï¸ ãƒ©ã‚¤ãƒ–ã‚¿ãƒ–åˆ‡æ›¿ã§ä¾‹å¤–: {e}")

        # =========================
        # ğŸ”¹ é–‰ã˜ã¦ã„ã‚‹å¤§ä¼š/ãƒªãƒ¼ã‚°ã‚’å±•é–‹
        # =========================
        try:
            # ãƒšãƒ¼ã‚¸ä¸Šã®ã€ŒæŠ˜ã‚ŠãŸãŸã¿å±•é–‹ãƒœã‚¿ãƒ³ã€ã‚’å…¨éƒ¨æŠ¼ã™
            # ï¼ˆãƒ©ã‚¤ãƒ–ã‚¿ãƒ–å†…ã«é™ã‚‰ãšå­˜åœ¨ã™ã‚‹ãŒã€æŠ¼ã—ã¦ã‚‚å•é¡Œãªã—ï¼‰
            expanders = page.locator("div.event__expander")
            n_closed = expanders.count()
            if n_closed > 0:
                print(f"ğŸ“‚ é–‰ã˜ã¦ã„ã‚‹è©¦åˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’ {n_closed} ä»¶é–‹ãã¾ã™...")
                for i in range(n_closed):
                    try:
                        ex = expanders.nth(i)
                        ex.scroll_into_view_if_needed(timeout=1000)
                        ex.click(timeout=1000)
                        time.sleep(0.25)
                    except Exception as e:
                        print(f"âš ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³å±•é–‹å¤±æ•—: {e}")
                # å±•é–‹åæ˜ ã®ãŸã‚å°‘ã—å¾…ã¤
                time.sleep(0.8)
            else:
                print("âœ… é–‰ã˜ã¦ã„ã‚‹è©¦åˆã‚°ãƒ«ãƒ¼ãƒ—ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"âš ï¸ å±•é–‹å‡¦ç†ä¾‹å¤–: {e}")

        # =========================
        # ğŸ”¹ ãƒ©ã‚¤ãƒ–è¡Œã®ã¿ã‹ã‚‰URLæŠ½å‡ºï¼ˆå®‰å…¨ï¼‰
        # =========================
        links = page.eval_on_selector_all(
            "div.event__match.event__match--live a.eventRowLink[href*='/match/'][href*='?mid=']",
            "els => els.map(e => e.href)"
        ) or []
        links = list(dict.fromkeys(links))  # é‡è¤‡é™¤å»
        print(f"ğŸ¯ ãƒ©ã‚¤ãƒ–è©¦åˆ æ¤œå‡º:{len(links)}ä»¶")


        for i, url in enumerate(links[:], 1):
            print(f"\n[{i}/{len(links[:])}] {url}")
            gp_page = ctx.new_page()
            try:
                gp_page.goto(url, timeout=45000, wait_until="domcontentloaded")

                # BOTé˜²æ­¢ãƒšãƒ¼ã‚¸å¯¾å¿œ
                if BOT_WALL_PAT.search(gp_page.content() or ""):
                    time.sleep(2)
                    try: gp_page.reload(wait_until="domcontentloaded", timeout=30000)
                    except: pass

                # ãƒãƒ¼ãƒ åãƒ»ã‚¹ã‚³ã‚¢å–å¾—
                home, away = get_home_away_names(gp_page)
                hs, aw = get_scores(gp_page)
                print(f"âš½  | {home} {hs}-{aw} {away}")

                # ğŸ”¹ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿å–å¾—
                stats_pairs = scrape_stats_pairs(gp_page)

                # å–å¾—ã‚¹ã‚¿ãƒƒãƒ„ã®ãƒ€ãƒ³ãƒ—(print [STATS] Â§{section} )
                debug_dump_stats(stats_pairs)
                print(f"ğŸ“Š çµ±è¨ˆãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({len(stats_pairs)}é …ç›®)" if stats_pairs else "âš ï¸ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãªã—")

                # ğŸ”¹ è©¦åˆãƒ¡ã‚¿æƒ…å ±ãƒ»é †ä½æƒ…å ±
                meta = get_match_meta(gp_page)
                ranks = get_match_standings(gp_page, home, away)

                # ğŸ”¹ ã‚¹ã‚¿ãƒƒãƒ„æŠ½å‡ºè£œå®Œ
                def get_stat(sec, key):
                    try:
                        return stats_pairs.get(sec, {}).get(key, ["", ""])
                    except:
                        return ["", ""]
                # ğŸ”¹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºè£œå®Œ
                def get_meta(sec):
                    try:
                        return meta.get(sec, {})
                    except:
                        return [""]
                def get_ranks(sec):
                    try:
                        return ranks.get(sec, {})
                    except:
                        return [""]

                game_category = get_meta("å›½") + ": " + get_meta("ãƒªãƒ¼ã‚°")
                live = get_meta("è©¦åˆæ™‚é–“")
                get_record = get_meta("å–å¾—æ™‚åˆ»")
                home_rank = get_ranks("home_rank")
                away_rank = get_ranks("away_rank")
                shot_exp_home, shot_exp_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤ï¼ˆxGï¼‰")
                ball_pos_home, ball_pos_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ãƒœãƒ¼ãƒ«æ”¯é…ç‡")
                shoot_home, shoot_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ã‚·ãƒ¥ãƒ¼ãƒˆæ•°")
                shot_sum_home, shot_sum_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "åˆè¨ˆã‚·ãƒ¥ãƒ¼ãƒˆ")
                shot_in_home, shot_in_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆ")
                big_chance_home, big_chance_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹")
                corner_home, corner_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯")
                yellow_home, yellow_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰")
                shot_home, shot_away = get_stat("ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„", "ã‚·ãƒ¥ãƒ¼ãƒˆ")
                shot_in_exp_home, shot_in_exp_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤ï¼ˆxGOTï¼‰")
                shot_out_home, shot_out_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ")
                shot_block_home, shot_block_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚·ãƒ¥ãƒ¼ãƒˆãƒ–ãƒ­ãƒƒã‚¯")
                shot_in_box_block_home, shot_in_box_block_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "ãƒœãƒƒã‚¯ã‚¹å†…ã‹ã‚‰ã®ã‚·ãƒ¥ãƒ¼ãƒˆ")
                shot_out_box_block_home, shot_out_box_block_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "ãƒœãƒƒã‚¯ã‚¹å¤–ã‹ã‚‰ã®ã‚·ãƒ¥ãƒ¼ãƒˆ")
                shot_post_home, shot_post_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "ã‚´ãƒ¼ãƒ«æ ã«å½“ãŸã‚‹")
                shot_head_home, shot_head_away = get_stat("ã‚·ãƒ¥ãƒ¼ãƒˆ", "ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹ã‚´ãƒ¼ãƒ«")
                oppo_in_box_touch_home, oppo_in_box_touch_away = get_stat("ã‚¢ã‚¿ãƒƒã‚¯", "ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹å†…ã§ã®ã‚¿ãƒƒãƒ")
                offside_home, offside_away = get_stat("ã‚¢ã‚¿ãƒƒã‚¯", "ã‚ªãƒ•ã‚µã‚¤ãƒ‰")
                free_kick_home, free_kick_away = get_stat("ã‚¢ã‚¿ãƒƒã‚¯", "ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯")
                pass_home, pass_away = get_stat("ãƒ‘ã‚¹", "ãƒ‘ã‚¹")
                long_pass_home, long_pass_away = get_stat("ãƒ‘ã‚¹", "ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹")
                final_third_pass_home, final_third_pass_away = get_stat("ãƒ‘ã‚¹", "ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ã§ã®ãƒ‘ã‚¹")
                cross_home, cross_away = get_stat("ãƒ‘ã‚¹", "ã‚¯ãƒ­ã‚¹")
                throw_in_home, throw_in_away = get_stat("ãƒ‘ã‚¹", "ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³")
                foul_home, foul_away = get_stat("ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹", "ãƒ•ã‚¡ã‚¦ãƒ«")
                tackle_home, tackle_away = get_stat("ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹", "ã‚¿ãƒƒã‚¯ãƒ«")
                duel_home, duel_away = get_stat("ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹", "ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°")
                clear_home, clear_away = get_stat("ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹", "ã‚¯ãƒªã‚¢ãƒªãƒ³ã‚°")
                intercept_home, intercept_away = get_stat("ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹", "ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ")
                keeper_save_home, keeper_save_away = get_stat("ã‚´ãƒ¼ãƒ«ã‚­ãƒ¼ãƒ‘ãƒ¼", "ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–")
                get_link = get_ranks("url")
                referee = get_meta("ãƒ¬ãƒ•ã‚§ãƒªãƒ¼")
                studium = get_meta("é–‹å‚¬åœ°")
                capacity = get_meta("åå®¹äººæ•°")
                audience = get_meta("å‚åŠ ")

                # ===== ã“ã“ã‹ã‚‰è¿½è¨˜ï¼ˆd.update(stats_dict) ã®ç›´å¾Œï¼‰ =====

                def first_nonempty(*vals):
                    for v in vals:
                        if str(v or "").strip():
                            return v
                    return ""

                def put(hkey, akey, hval, aval, overwrite=False):
                    # overwrite=False ã®ã¨ãã¯ã€ã™ã§ã«å€¤ãŒå…¥ã£ã¦ã„ã‚Œã°ä¸Šæ›¸ãã—ãªã„
                    if overwrite or not str(d.get(hkey, "")).strip():
                        d[hkey] = hval or ""
                        print(f"{hkey} : {hval}")
                    if overwrite or not str(d.get(akey, "")).strip():
                        d[akey] = aval or ""
                        print(f"{akey} : {aval}")

                # ğŸ”¹ HEADERæ§‹é€ ä½“ç”Ÿæˆ
                d = {col: "" for col in HEADER}
                
                mid = extract_mid(get_link) or ""
                tkey = parse_live_time_to_seconds(meta.get("è©¦åˆæ™‚é–“", ""))

                last_seq = int(SEQMAP.get(mid, 0))
                seq = last_seq + 1
                SEQMAP[mid] = seq

                # --- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ ---
                put("ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ","", home, "")
                put("ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ","", away, "")
                put("ãƒ›ãƒ¼ãƒ ã‚¹ã‚³ã‚¢","", hs, "")
                put("ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ã‚³ã‚¢","", aw, "")
                put("è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª","", game_category, "")
                put("ãƒ›ãƒ¼ãƒ é †ä½","ã‚¢ã‚¦ã‚§ãƒ¼é †ä½", home_rank, away_rank)
                put("è©¦åˆæ™‚é–“","", live, "")
                put("è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—","", get_link, "")

                put("ãƒ¬ãƒ•ã‚§ãƒªãƒ¼","", referee, "")
                put("ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ","", studium, "")
                put("åå®¹äººæ•°","", capacity, "")
                put("è¦³å®¢æ•°","", audience, "")

                # --- ä¸»ãªã‚¹ã‚¿ãƒƒãƒ„ï¼ˆå„ªå…ˆçš„ã«åæ˜ ã€‚ç©ºæ¬„ãªã‚‰å¾Œè¿°ã®ä»£æ›¿ã§è£œå®Œï¼‰ ---
                put("ãƒ›ãƒ¼ãƒ æœŸå¾…å€¤","ã‚¢ã‚¦ã‚§ãƒ¼æœŸå¾…å€¤", shot_exp_home, shot_exp_away)
                put("ãƒ›ãƒ¼ãƒ ãƒœãƒ¼ãƒ«æ”¯é…ç‡","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒ¼ãƒ«æ”¯é…ç‡", ball_pos_home, ball_pos_away)

                #   ã‚·ãƒ¥ãƒ¼ãƒˆç·æ•°ã¯ã€(ã‚·ãƒ¥ãƒ¼ãƒˆæ•°, ã‚·ãƒ¥ãƒ¼ãƒˆ, åˆè¨ˆã‚·ãƒ¥ãƒ¼ãƒˆ) ã®é †ã§å„ªå…ˆ
                shoot_home_final = first_nonempty(shoot_home, shot_home, shot_sum_home)
                shoot_away_final = first_nonempty(shoot_away, shot_away, shot_sum_away)
                put("ãƒ›ãƒ¼ãƒ ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", shoot_home_final, shoot_away_final)

                put("ãƒ›ãƒ¼ãƒ æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", shot_in_home, shot_in_away)
                put("ãƒ›ãƒ¼ãƒ æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°","ã‚¢ã‚¦ã‚§ãƒ¼æ å¤–ã‚·ãƒ¥ãƒ¼ãƒˆæ•°", shot_out_home, shot_out_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚·ãƒ¥ãƒ¼ãƒˆ", shot_block_home, shot_block_away)
                put("ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å†…ã‚·ãƒ¥ãƒ¼ãƒˆ", shot_in_box_block_home, shot_in_box_block_away)
                put("ãƒ›ãƒ¼ãƒ ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ãƒœãƒƒã‚¯ã‚¹å¤–ã‚·ãƒ¥ãƒ¼ãƒˆ", shot_out_box_block_home, shot_out_box_block_away)
                put("ãƒ›ãƒ¼ãƒ ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ã‚´ãƒ¼ãƒ«ãƒã‚¹ãƒˆ", shot_post_home, shot_post_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ˜ãƒ‡ã‚£ãƒ³ã‚°ã‚´ãƒ¼ãƒ«", shot_head_home, shot_head_away)

                put("ãƒ›ãƒ¼ãƒ ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ“ãƒƒã‚°ãƒãƒ£ãƒ³ã‚¹", big_chance_home, big_chance_away)
                put("ãƒ›ãƒ¼ãƒ ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯","ã‚¢ã‚¦ã‚§ãƒ¼ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯", corner_home, corner_away)
                put("ãƒ›ãƒ¼ãƒ ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰", yellow_home, yellow_away)

                # xGOTï¼ˆæ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤ï¼‰
                put("ãƒ›ãƒ¼ãƒ æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤","ã‚¢ã‚¦ã‚§ãƒ¼æ å†…ã‚´ãƒ¼ãƒ«æœŸå¾…å€¤", shot_in_exp_home, shot_in_exp_away)

                # ã‚¢ã‚¿ãƒƒã‚¯ç³»
                put("ãƒ›ãƒ¼ãƒ ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ","ã‚¢ã‚¦ã‚§ãƒ¼ç›¸æ‰‹ãƒœãƒƒã‚¯ã‚¹ã‚¿ãƒƒãƒ", oppo_in_box_touch_home, oppo_in_box_touch_away)
                put("ãƒ›ãƒ¼ãƒ ã‚ªãƒ•ã‚µã‚¤ãƒ‰","ã‚¢ã‚¦ã‚§ãƒ¼ã‚ªãƒ•ã‚µã‚¤ãƒ‰", offside_home, offside_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯", free_kick_home, free_kick_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‘ã‚¹", pass_home, pass_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ­ãƒ³ã‚°ãƒ‘ã‚¹", long_pass_home, long_pass_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰ãƒ‘ã‚¹", final_third_pass_home, final_third_pass_away)
                put("ãƒ›ãƒ¼ãƒ ã‚¯ãƒ­ã‚¹","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒ­ã‚¹", cross_home, cross_away)
                put("ãƒ›ãƒ¼ãƒ ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¤ãƒ³", throw_in_home, throw_in_away)

                # ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹ç³»
                put("ãƒ›ãƒ¼ãƒ ãƒ•ã‚¡ã‚¦ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ•ã‚¡ã‚¦ãƒ«", foul_home, foul_away)
                put("ãƒ›ãƒ¼ãƒ ã‚¿ãƒƒã‚¯ãƒ«","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¿ãƒƒã‚¯ãƒ«", tackle_home, tackle_away)
                put("ãƒ›ãƒ¼ãƒ ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°","ã‚¢ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ¥ã‚¨ãƒ«å‹åˆ©æ•°", duel_home, duel_away)
                put("ãƒ›ãƒ¼ãƒ ã‚¯ãƒªã‚¢","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¯ãƒªã‚¢", clear_home, clear_away)
                put("ãƒ›ãƒ¼ãƒ ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ","ã‚¢ã‚¦ã‚§ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆ", intercept_home, intercept_away)

                put("ã‚¹ã‚³ã‚¢æ™‚é–“","", get_record, "")

                # GK
                put("ãƒ›ãƒ¼ãƒ ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–","ã‚¢ã‚¦ã‚§ãƒ¼ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒ¼ãƒ–", keeper_save_home, keeper_save_away)


                put("è©¦åˆID","", mid, "")
                put("é€šç•ª","", last_seq, "")
                put("ã‚½ãƒ¼ãƒˆç”¨ç§’","", tkey, "")
            
                # ä»Šå›1è¡Œã®åŸ‹ã¾ã‚Šå…·åˆ
                debug_filled_columns(d)
                # âœ… ã“ã“ã§å³Excelã«1è¡Œè¿½è¨˜
                append_row_to_excel(d, SAVE_DIR)
            except Exception as e:
                print("âš ï¸ å–å¾—ã‚¨ãƒ©ãƒ¼:", e)
            finally:
                try: gp_page.close()
                except: pass

        browser.close()

def get_match_meta(pg):
    """è©¦åˆãƒšãƒ¼ã‚¸ã®å›½ãƒ»ãƒªãƒ¼ã‚°ãƒ»è©¦åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ»è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
    meta = {}

    # ç¾åœ¨ã®URLã‚’ä¿å­˜
    cur_url = pg.url

    # --- âœ… summary ã‚¿ãƒ–ã«ç§»å‹•ï¼ˆçµ±è¨ˆã‚¿ãƒ–ã§ã¯æƒ…å ±ãŒãªã„ï¼‰
    if "/summary" not in cur_url or "/summary/stats" in cur_url:
        summary_url = re.sub(r"/summary/.*", "/summary/", cur_url)
        if not summary_url.endswith("/summary/"):
            summary_url = re.sub(r"/(standings|stats|lineups|odds|commentary).*", "/summary/", cur_url)
        pg.goto(summary_url, timeout=30000, wait_until="domcontentloaded")
        time.sleep(1.2)

    # -------------------------
    # ğŸ”¹ ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆï¼ˆå›½ãƒ»ãƒªãƒ¼ã‚°ï¼‰
    # -------------------------
    try:
        crumbs = pg.locator("ol.wcl-breadcrumbList_lC9sI li[data-testid='wcl-breadcrumbsItem'] span[itemprop='name']")
        txts = [text_clean(c.text_content() or "") for c in crumbs.all()]
        if len(txts) >= 2:
            meta["å›½"] = txts[1]
        if len(txts) >= 3:
            meta["ãƒªãƒ¼ã‚°"] = txts[2]
    except Exception as e:
        print(f"âš ï¸ ãƒ‘ãƒ³ããšå–å¾—å¤±æ•—: {e}")

    # -------------------------
    # ğŸ”¹ è©¦åˆæƒ…å ±ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆãƒ¬ãƒ•ã‚§ãƒªãƒ¼ãƒ»é–‹å‚¬åœ°ãƒ»åå®¹äººæ•°ãªã©ï¼‰
    # -------------------------
    try:
        info_container = pg.locator("div[data-testid='wcl-summaryMatchInformation']")
        if not info_container.count():
            print("âš ï¸ è©¦åˆæƒ…å ±ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            label_elems = info_container.locator("span[data-testid='wcl-scores-overline-02']")
            print(f"â„¹ï¸ è©¦åˆæƒ…å ±é …ç›®æ•°: {label_elems.count()}")
            for i in range(label_elems.count()):
                try:
                    label = text_clean(label_elems.nth(i).text_content() or "")
                    label = label.replace(":", "").replace("ï¼š", "").strip()

                    # ãƒ©ãƒ™ãƒ«ã®è¦ª divï¼ˆinfoLabelWrapperï¼‰ã‹ã‚‰æ¬¡ã® sibling div(infoValue) ã‚’å–å¾—
                    val_div = label_elems.nth(i).locator(
                        "xpath=ancestor::div[contains(@class,'wcl-infoLabelWrapper')]/following-sibling::div[1]"
                    ).first

                    vals = val_div.locator("[data-testid='wcl-scores-simple-text-01']")
                    txts = [text_clean(v.text_content()) for v in vals.all()]
                    value = " ".join([t for t in txts if t])
                    if label and value:
                        print(f"ğŸ“‹ {label}: {value}")
                        meta[label] = value
                except Exception as e:
                    print(f"âš ï¸ é …ç›®å‡¦ç†å¤±æ•—: {e}")
                    continue
    except Exception as e:
        print(f"âš ï¸ è©¦åˆæƒ…å ±ãƒ–ãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—: {e}")

    # -------------------------
    # ğŸ”¹ è©¦åˆæ™‚é–“ï¼ˆãƒ©ã‚¤ãƒ–ãƒ»çµ‚äº†ãªã©ï¼‰
    # -------------------------
    try:
        st = pg.locator("div.detailScore__status div.eventAndAddedTime span.eventTime")
        if st.count():
            meta["è©¦åˆæ™‚é–“"] = text_clean(st.first.text_content())
        else:
            lt = pg.locator("[data-testid='wcl-time']").first
            if lt.count():
                meta["è©¦åˆæ™‚é–“"] = text_clean(lt.text_content())
    except:
        pass

    meta["å–å¾—æ™‚åˆ»"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # --- å…ƒã®ãƒšãƒ¼ã‚¸ã«æˆ»ã‚‹ï¼ˆå¿…è¦ãªã‚‰ï¼‰
    if "/summary/stats" in cur_url:
        try:
            pg.goto(cur_url, timeout=30000, wait_until="domcontentloaded")
        except:
            pass

    return meta

def extract_mid(s: str) -> Optional[str]:
    if not s:
        return None
    s = str(s).strip()
    m = re.search(r"[?&#]mid=([A-Za-z0-9]+)", s)
    if m: return m.group(1)
    m = re.search(r"/match/([A-Za-z0-9]{6,20})(?:/|$)", s)
    if m: return m.group(1)
    return None

def parse_live_time_to_seconds(tstr: str) -> int:
    """ã€Œ90+3ã€ã€Œå‰åŠ45ã€ã€Œå¾ŒåŠ12ã€ã€Œçµ‚äº†æ¸ˆã€ãªã©ã‚’ç§’ã«å¤‰æ›"""
    if not tstr:
        return 0
    t = tstr.strip()
    if "çµ‚äº†" in t or "FT" in t.upper():
        return 5400  # é€šå¸¸90åˆ†=5400ç§’
    if "å‰åŠ" in t:
        num = re.sub(r"[^0-9]", "", t)
        return int(num) * 60 if num else 0
    if "å¾ŒåŠ" in t:
        num = re.sub(r"[^0-9]", "", t)
        return 2700 + int(num) * 60 if num else 2700
    if re.match(r"^\d+\+\d+$", t):  # 90+3 ãªã©
        a, b = t.split("+")
        return (int(a) + int(b)) * 60
    if t.isdigit():
        return int(t) * 60
    return 0

def _norm_team_name(s: str) -> str:
    """ç…§åˆç”¨ã«ãƒãƒ¼ãƒ åã‚’æ­£è¦åŒ–ï¼ˆç©ºç™½ã¨ä¸­ç‚¹ã‚’é™¤å»ã€å…¨è§’åŠè§’ã¯ãã®ã¾ã¾ã§ã‚‚å¤§æŠµOKï¼‰"""
    s = text_clean(s or "")
    s = s.replace("ãƒ»", "")  # ä¾‹: æ¨ªæµœFãƒ»ãƒãƒªãƒã‚¹ â†’ æ¨ªæµœFãƒãƒªãƒã‚¹
    s = re.sub(r"\s+", "", s)
    return s

def goto_standings_overall(pg):
    """
    ç¾åœ¨ã®è©¦åˆURLã‹ã‚‰ standings/standings/overall/ ã«é·ç§»ã™ã‚‹ã€‚
    midã‚¯ã‚¨ãƒªã¯ç¶­æŒã€‚/summary/... ãªã©åˆ¥ã‚¿ãƒ–ã§ã‚‚OKã€‚
    """
    cur = pg.url
    parts = list(urlsplit(cur))
    # path ã‚’ /.../standings/standings/overall/ ã«å¼·åˆ¶
    # ä¾‹: /match/soccer/xxx/yyyy/ â†’ /match/soccer/xxx/yyyy/standings/standings/overall/
    if not parts[2].endswith("/"):
        parts[2] += "/"
    base = re.sub(r"/(summary|h2h|lineups|odds|commentary|stats|standings)(/.*)?$", "/", parts[2])
    parts[2] = base + "standings/standings/overall/"
    url = urlunsplit(parts)

    pg.goto(url, timeout=45000, wait_until="domcontentloaded")
    # ãƒ†ãƒ¼ãƒ–ãƒ«æç”»å¾…ã¡
    try:
        pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=15000)
    except:
        pass

def build_standings_url(match_url: str) -> str:
    """
    Flashscoreè©¦åˆURLã‹ã‚‰é †ä½è¡¨URLã‚’æ§‹ç¯‰ã€‚
    æ­£è¦å½¢å¼:
      /match/soccer/<teamA>/ <teamB>/ â†’ /match/soccer/<teamA>/<teamB>/standings/live-standings/?mid=...
    """
    # midãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒ
    mid_part = ""
    if "?" in match_url:
        base, mid_part = match_url.split("?", 1)
    else:
        base = match_url

    # æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’è£œã†
    if not base.endswith("/"):
        base += "/"

    # è©¦åˆéƒ¨åˆ†ã®æŠ½å‡º
    # ä¾‹: /match/soccer/kakamega-homeboyz-fc-OfdPtDuK/ulinzi-stars-KzxaGb9r/
    m = re.match(r"^(.*/match/soccer/[^/]+/[^/]+/)", base)
    if not m:
        # fallback: è©¦åˆãƒšãƒ¼ã‚¸éƒ¨åˆ†ã ã‘ã§ã‚‚å‡¦ç†
        m = re.match(r"^(.*/match/soccer/[^/]+/)", base)
    if not m:
        return match_url  # å®‰å…¨ç­–

    prefix = m.group(1)
    # æ­£ã—ã„ standings ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    standings_url = prefix + "standings/live-standings/"
    if mid_part:
        standings_url += "?" + mid_part
    return standings_url

def goto_standings_page(pg):
    """é †ä½ãƒšãƒ¼ã‚¸ã«é·ç§»ã€‚live-standingså„ªå…ˆã€å¤±æ•—ã—ãŸã‚‰overall"""
    url1 = build_standings_url(pg.url)
    url2 = url1.replace("live-standings", "standings/standings/overall")

    try:
        pg.goto(url1, timeout=45000, wait_until="domcontentloaded")
        pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=8000)
        return url1
    except:
        # fallback
        try:
            pg.goto(url2, timeout=45000, wait_until="domcontentloaded")
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=8000)
            return url2
        except:
            return None

def goto_standings_overall(pg):
    url = build_standings_overall_url(pg.url)
    pg.goto(url, timeout=45000, wait_until="domcontentloaded")
    # ãƒ†ãƒ¼ãƒ–ãƒ«æç”»å¾…ã¡
    try:
        pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=15000)
    except:
        pass

def parse_standings_table(pg):
    rows = pg.locator(".ui-table__body .ui-table__row")
    n = rows.count()
    out = []
    for i in range(n):
        r = rows.nth(i)
        rank_txt = text_clean(r.locator(".table__cell--rank .tableCellRank").first.text_content() or "")
        # æœ«å°¾ã®ã€Œ.ã€é™¤å»
        try:
            rank = int(rank_txt.strip().rstrip("."))
        except:
            rank = None
        team = text_clean(r.locator(".table__cell--participant .tableCellParticipant__name").first.text_content() or "")
        vals = [text_clean(x.text_content() or "") for x in r.locator("span.table__cell--value").all()]
        # pts ã¯é…åˆ—ã®æœ€å¾Œï¼ˆå‹ç‚¹ï¼‰
        pts = None
        if vals:
            try: pts = int(vals[-1])
            except: pass
        selected = "table__row--selected" in (r.get_attribute("class") or "")
        out.append({"rank": rank, "team": team, "pts": pts, "selected": selected})
    return out

def _norm_team_name(s: str) -> str:
    s = text_clean(s or "")
    s = s.replace("ãƒ»", "")
    s = re.sub(r"\s+", "", s)
    return s

def get_match_standings(pg, home_name: str, away_name: str):
    """
    ä¸¡ãƒãƒ¼ãƒ ã®é †ä½ãƒ»å‹ç‚¹ã‚’æŠ½å‡ºã€‚
    live-standings å„ªå…ˆã€ãªã‘ã‚Œã° overall ã‹ã‚‰å–å¾—ã€‚
    """
    url = goto_standings_page(pg)
    if not url:
        return {}

    rows = pg.locator(".ui-table__body .ui-table__row")
    if not rows.count():
        return {}

    out = {}
    for i in range(rows.count()):
        r = rows.nth(i)
        rank_txt = text_clean(r.locator(".table__cell--rank .tableCellRank").first.text_content() or "")
        team_name = text_clean(r.locator(".table__cell--participant .tableCellParticipant__name").first.text_content() or "")
        pts_txt = text_clean(r.locator(".table__cell--value").last.text_content() or "")

        try: rank = int(rank_txt.strip().rstrip("."))
        except: rank = None
        try: pts = int(pts_txt)
        except: pts = None

        out[team_name] = {"rank": rank, "pts": pts}

    h, a = text_clean(home_name), text_clean(away_name)
    home = next((v for k, v in out.items() if h in k or k in h), None)
    away = next((v for k, v in out.items() if a in k or k in a), None)

    return {
        "url": url,
        "home_rank": home["rank"] if home else None,
        "home_pts": home["pts"] if home else None,
        "away_rank": away["rank"] if away else None,
        "away_pts": away["pts"] if away else None,
    }

def get_match_teams_ranks(pg, home_name: str, away_name: str):
    """
    è¿”ã‚Šå€¤:
      {
        "home_rank": int|None, "away_rank": int|None,
        "home_pts": int|None,  "away_pts": int|None,
        "table_rows": int
      }
    """
    goto_standings_overall(pg)
    table = parse_standings_table(pg)

    # 1) é¸æŠè¡Œå„ªå…ˆï¼ˆé€šå¸¸2è¡Œ = å¯¾è±¡2ãƒãƒ¼ãƒ ï¼‰
    selected_rows = [r for r in table if r["selected"]]
    home_rank = away_rank = home_pts = away_pts = None

    if len(selected_rows) >= 2:
        # åå‰ç…§åˆã§ home/away ã‚’å‰²å½“ï¼ˆåŒç‚¹ãƒ»åˆ¥åå¯¾ç­–ã§normalizeï¼‰
        hkey, akey = _norm_team_name(home_name), _norm_team_name(away_name)
        for r in selected_rows[:2]:
            tkey = _norm_team_name(r["team"])
            if tkey == hkey and home_rank is None:
                home_rank, home_pts = r["rank"], r["pts"]
            elif tkey == akey and away_rank is None:
                away_rank, away_pts = r["rank"], r["pts"]
        # ç‰‡æ–¹ã—ã‹å‰²ã‚Šå½“ã¦ã‚‰ã‚Œãªã‹ã£ãŸå ´åˆã¯æ®‹ã‚Šã‚’ã‚‚ã†ä¸€æ–¹ã¸
        if (home_rank is None or away_rank is None):
            # é †ä¸åŒã§å……å½“
            if home_rank is None:
                home_rank, home_pts = selected_rows[0]["rank"], selected_rows[0]["pts"]
            if away_rank is None:
                away_rank, away_pts = selected_rows[1]["rank"], selected_rows[1]["pts"]

    else:
        # 2) é¸æŠè¡ŒãŒ1ã¤ or ç„¡ã„ â†’ åå‰ç…§åˆã®ã¿
        hkey, akey = _norm_team_name(home_name), _norm_team_name(away_name)
        for r in table:
            tkey = _norm_team_name(r["team"])
            if tkey == hkey and home_rank is None:
                home_rank, home_pts = r["rank"], r["pts"]
            if tkey == akey and away_rank is None:
                away_rank, away_pts = r["rank"], r["pts"]

    return {
        "home_rank": home_rank, "away_rank": away_rank,
        "home_pts": home_pts,   "away_pts": away_pts,
        "table_rows": len(table),
        "standings_url": build_standings_overall_url(pg.url)  # ãƒ‡ãƒãƒƒã‚°ç”¨
    }

def get_match_teams_ranks(pg, home_name: str, away_name: str):
    """
    ãã®è©¦åˆã®ãƒ›ãƒ¼ãƒ /ã‚¢ã‚¦ã‚§ãƒ¼ã«å¯¾å¿œã™ã‚‹é †ä½ã‚’è¿”ã™ã€‚
    è¿”ã‚Šå€¤: {"home_rank": int|None, "away_rank": int|None, "home_pts": int|None, "away_pts": int|None}
    """
    goto_standings_overall(pg)
    table = parse_standings_table(pg)

    hkey = _norm_team_name(home_name)
    akey = _norm_team_name(away_name)

    home_rank = away_rank = home_pts = away_pts = None

    for row in table:
        tkey = _norm_team_name(row["team"])
        if tkey == hkey and home_rank is None:
            home_rank, home_pts = row["rank"], row["pts"]
        if tkey == akey and away_rank is None:
            away_rank, away_pts = row["rank"], row["pts"]

    # ç‰‡æ–¹ã ã‘ãƒãƒƒãƒã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé¸æŠè¡Œã«é ¼ã‚‹ï¼‰
    if (home_rank is None or away_rank is None):
        sel = [r for r in table if r["selected"]]
        # é€šå¸¸ã€é¸æŠè¡Œã¯2è¡Œï¼ˆä¸¡ãƒãƒ¼ãƒ ï¼‰ã«ãªã‚‹ã“ã¨ãŒå¤šã„
        if len(sel) >= 1:
            # teamåã®å¼·åˆ¶ä¸€è‡´ãŒã§ããªã„ã‚±ãƒ¼ã‚¹ã®ä¿é™ºã¨ã—ã¦æ¡ç”¨
            if home_rank is None:
                home_rank = sel[0]["rank"] if len(sel) >= 1 else home_rank
                home_pts  = sel[0]["pts"]  if len(sel) >= 1 else home_pts
            if away_rank is None and len(sel) >= 2:
                away_rank = sel[1]["rank"]
                away_pts  = sel[1]["pts"]

    return {
        "home_rank": home_rank,
        "away_rank": away_rank,
        "home_pts": home_pts,
        "away_pts": away_pts,
        "table_rows": len(table)
    }

def build_standings_overall_url(match_url: str) -> str:
    """
    ä»»æ„ã®è©¦åˆURLã‹ã‚‰ /standings/standings/overall/ ã¸é£›ã°ã™URLã‚’ä½œã‚‹ã€‚
    ä¾‹:
      https://www.flashscore.co.jp/match/soccer/c-osaka-XXXX/kawasaki-YYYY/?mid=AwEGROFt
      â†’ https://www.flashscore.co.jp/match/soccer/c-osaka-XXXX/kawasaki-YYYY/standings/standings/overall/?mid=AwEGROFt
    """
    parts = list(urlsplit(match_url))
    path = parts[2]
    # /match/.../ â† è©¦åˆã®ã€Œãƒšã‚¢ã€ã¾ã§ã‚’æŠ½å‡º
    m = re.match(r"^(/match/[^/]+/[^/]+/)", path)
    base = m.group(1) if m else (path if path.endswith("/") else path + "/")
    parts[2] = base + "standings/live-standings/"
    return urlunsplit(parts)

def debug_dump_stats(stats_pairs: dict):
    """å–å¾—ã—ãŸ stats_pairsï¼ˆãƒã‚¹ãƒˆï¼‰ã‚’è¦‹ã‚„ã™ããƒ€ãƒ³ãƒ—"""
    if not stats_pairs:
        log("   [STATS] å–å¾—ã‚¼ãƒ­")
        return
    n_labels = 0
    for section, sub in stats_pairs.items():
        if isinstance(sub, dict):
            log(f"   [STATS] Â§{section}")
            for label, pair in sub.items():
                n_labels += 1
                if isinstance(pair, (list, tuple)) and len(pair) == 2:
                    log(f"      - {label}: {pair[0]} | {pair[1]}")
                else:
                    log(f"      - {label}: (unexpected format) {pair}")
        else:
            # å¹³å¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¿µã®ãŸã‚ï¼‰
            n_labels += 1
            log(f"   [STATS] {section}: {sub}")
    log(f"   [STATS] åˆè¨ˆãƒ©ãƒ™ãƒ«æ•°: {n_labels}")

def debug_filled_columns(d: dict):
    """ä»Šå›ã®1è©¦åˆåˆ† d ã§ã€éç©ºã®åˆ—ã‚’è¡¨ç¤ºï¼ˆã©ã®åˆ—ãŒåŸ‹ã¾ã£ãŸã‹ï¼‰"""
    filled = [k for k in HEADER if str(d.get(k, "")).strip() != ""]
    log(f"   [ROW] éç©ºåˆ—: {len(filled)} / {len(HEADER)}")
    log("   [ROW] éç©ºåˆ—ã®ä¾‹: " + ", ".join(filled[:12]) + (" ..." if len(filled) > 12 else ""))

# ================= çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ â†’ HEADERçµ±åˆ =================
def stats_to_header_dict(stats_pairs: dict) -> dict:
    """
    scrape_stats_pairs ã®çµæœï¼ˆãƒã‚¹ãƒˆæ§‹é€ ï¼‰ã‚’ HEADER ã‚­ãƒ¼å¯¾å¿œã«å±•é–‹ã€‚
    """
    out = {k: "" for k in HEADER}
    for section, subdict in stats_pairs.items():
        # section ãŒ dict ã®å ´åˆï¼ˆé€šå¸¸ï¼‰
        if isinstance(subdict, dict):
            for label, pair in subdict.items():
                if not isinstance(pair, (list, tuple)) or len(pair) != 2:
                    continue
                key = f"{section}:{label}"
                if key in STAT_KEY_MAP:
                    hcol, acol = STAT_KEY_MAP[key]
                    out[hcol] = pair[0]
                    out[acol] = pair[1]
        # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¹³å¦ï¼‰
        else:
            if not isinstance(subdict, (list, tuple)) or len(subdict) != 2:
                continue
            if section in STAT_KEY_MAP:
                hcol, acol = STAT_KEY_MAP[section]
                out[hcol] = subdict[0]
                out[acol] = subdict[1]
    return out

# ===== Excel é€æ¬¡æ›¸ãè¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
from pathlib import Path
try:
    import openpyxl
except ImportError:
    raise RuntimeError("openpyxl ãŒå¿…è¦ã§ã™ã€‚`pip install openpyxl` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®æœ€å¤§ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
MAX_ROWS_PER_FILE = 10
SHEET_NAME = "Sheet1"
FILE_PREFIX = "output_"
FILE_SUFFIX = ".xlsx"

def _existing_serials(output_dir: str) -> List[int]:
    p = Path(output_dir)
    nums = []
    for f in p.glob(f"{FILE_PREFIX}*{FILE_SUFFIX}"):
        m = re.match(rf"^{re.escape(FILE_PREFIX)}(\d+){re.escape(FILE_SUFFIX)}$", f.name)
        if m:
            nums.append(int(m.group(1)))
    return sorted(nums)

def _next_serial(output_dir: str) -> int:
    """æ—¢å­˜ã®æœ€å¤§é€£ç•ª+1 ã‚’è¿”ã™ï¼ˆâ€»ã€æœ€å°é€£ç•ª+1ã€ã®è¡¨è¨˜ã¯å¤šç¾©çš„ãªã®ã§æœ€å¤§+1ã‚’æ¡ç”¨ï¼‰"""
    nums = _existing_serials(output_dir)
    return (max(nums) + 1) if nums else 1

def _current_file_path(output_dir: str) -> Path:
    """ä»Šä½¿ã†ã¹ããƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚ç„¡ã‘ã‚Œã°æ–°è¦ï¼ˆé€£ç•ªï¼‰"""
    p = Path(output_dir)
    nums = _existing_serials(output_dir)
    if not nums:
        return p / f"{FILE_PREFIX}{_next_serial(output_dir)}{FILE_SUFFIX}"
    # ç›´è¿‘ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€å¤§é€£ç•ªï¼‰
    return p / f"{FILE_PREFIX}{max(nums)}{FILE_SUFFIX}"

def _data_rows_in(path: Path) -> int:
    """æ—¢å­˜Excelã®ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ˜ãƒƒãƒ€é™¤ãï¼‰ã‚’è¿”ã™ã€‚ç„¡ã‘ã‚Œã°0ã€‚"""
    if not path.exists():
        return 0
    wb = openpyxl.load_workbook(path, read_only=True, data_only=True)
    ws = wb[SHEET_NAME] if SHEET_NAME in wb.sheetnames else wb.active
    # ãƒ˜ãƒƒãƒ€1è¡Œã‚’é™¤å¤–
    total = ws.max_row or 0
    wb.close()
    return max(0, total - 1)

def _create_new_workbook(path: Path):
    """ãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãã§æ–°è¦ä½œæˆ"""
    df = pd.DataFrame(columns=HEADER)
    path.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=SHEET_NAME)

def append_row_to_excel(row_dict: dict, output_dir: str, max_rows_per_file: int = MAX_ROWS_PER_FILE):
    """
    1è¡Œã‚’ç¾åœ¨ã®é€£ç•ªãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã€‚ä¸Šé™ã«é”ã—ãŸã‚‰æ¬¡é€£ç•ªã‚’æ–°è¦ä½œæˆã€‚
    """
    # ä»Šä½¿ã†ãƒ•ã‚¡ã‚¤ãƒ«
    cur = _current_file_path(output_dir)
    # ç„¡ã‘ã‚Œã°æ–°è¦ä½œæˆ
    if not cur.exists():
        # æ—¢å­˜ã®æœ€å¤§é€£ç•ª+1ã§ä½œã‚‹
        cur = Path(output_dir) / f"{FILE_PREFIX}{_next_serial(output_dir)}{FILE_SUFFIX}"
        _create_new_workbook(cur)

    # ä¸Šé™ãƒã‚§ãƒƒã‚¯
    current_rows = _data_rows_in(cur)
    if current_rows >= max_rows_per_file:
        # æ¬¡ã®é€£ç•ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆ
        cur = Path(output_dir) / f"{FILE_PREFIX}{_next_serial(output_dir)}{FILE_SUFFIX}"
        _create_new_workbook(cur)
        current_rows = 0  # æ–°è¦ãªã®ã§0

    # è¿½è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆåˆ—é †ã¯HEADERã«åˆã‚ã›ã‚‹ï¼‰
    df = pd.DataFrame([row_dict])
    for col in HEADER:
        if col not in df.columns:
            df[col] = ""
    df = df[HEADER]

    # è¿½è¨˜
    # startrow = æ—¢å­˜ã®æœ€çµ‚è¡Œï¼ˆãƒ˜ãƒƒãƒ€1è¡Œã‚ã‚‹ã®ã§ã€startrow=æ—¢å­˜ç·è¡Œï¼‰ã«æ›¸ãã€header=False
    with pd.ExcelWriter(cur, engine="openpyxl", mode="a", if_sheet_exists="overlay") as w:
        # æ—¢å­˜ã®æœ€çµ‚è¡Œã‚’å–å¾—ï¼ˆä¸Šã§current_rows=ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ãªã®ã§ã€ãƒ˜ãƒƒãƒ€1è¡Œã‚’è¶³ã™ï¼‰
        startrow = current_rows + 1
        df.to_excel(w, index=False, header=False, sheet_name=SHEET_NAME, startrow=startrow)

    print(f"ğŸ’¾ è¿½è¨˜å®Œäº†: {cur.name} ï¼ˆãƒ‡ãƒ¼ã‚¿è¡Œ {current_rows+1} â†’ {current_rows+1} ä»¶ç›®ã‚’è¿½åŠ ï¼‰")

if __name__ == "__main__":
    main()
