# -*- coding: utf-8 -*-
"""
Flashscore „É©„Ç§„ÉñË©¶Âêà -> (stats / summary meta / standings) „ÇíÂèñÂæó„Åó„Å¶ Excel „Å´ÈÄêÊ¨°ËøΩË®ò„Åô„ÇãÂÖ®Âá¶ÁêÜÁâàÔºàÂÅúÊ≠¢ÂØæÁ≠ñ„Éª„É≠„Ç∞Âº∑ÂåñÁâàÔºâ

‚úÖ ÈáçË¶Å‰ªïÊßò
- stats „ÅÆÂÄ§„ÅØ„Äå34%Ôºà31/90Ôºâ„Äç„Å™„Å©‚Äú„Åù„ÅÆ„Åæ„Åæ‚ÄùÂèñÂæóÔºàÂä†Â∑•„Åó„Å™„ÅÑÔºâ
- FlashscoreÂÅ¥: "„Çª„ÇØ„Ç∑„Éß„É≥:„É©„Éô„É´" -> canonical„Ç≠„Éº -> STAT_KEY_MAP -> HEADERÂàó„Å∏ÊäïÂÖ•
- HEADER„Å®„Çπ„ÇØ„É¨„Ç§„Éó„ÅåÊ≠£„Åó„ÅÑÂàó„Å´ÂÖ•„Å£„Å¶„ÅÑ„Çã„ÅãÊ§úË®º„É≠„Ç∞„ÇíÂá∫„ÅôÔºàVERIFYÔºâ

‚úÖ ‰ªäÂõû„ÅÇ„Å™„Åü„ÅÆ„Ç≥„Éº„Éâ„Å´Ê∑∑ÂÖ•„Åó„Å¶„ÅÑ„ÅüËá¥ÂëΩÁöÑ„Éê„Ç∞„ÇíËß£Ê∂à
- normalize_stats_pairs „Åå‰∫åÈáçÂÆöÁæ©„Åß‰∏äÊõ∏„Åç ‚Üí 1„Å§„Å´Áµ±Âêà
- rb / mid / meta / final_category „Å™„Å©Êú™ÂÆöÁæ© ‚Üí „Åô„Åπ„Å¶ÂÆöÁæ©„Åó„Å¶ÊµÅ„Çå„ÇíÊàêÁ´ã
- scrape_stats_pairs „Çí‰∫åÈáç„Å´Âëº„Å∂ ‚Üí 1Âõû„Å´Áµ±‰∏Ä
"""

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
import time, re, os, pickle, datetime, traceback
from pathlib import Path
from urllib.parse import urlsplit, urlunsplit, urlparse, parse_qs
from typing import Optional, List, Dict, Tuple, Any
import multiprocessing as mp
import queue as pyqueue
import pandas as pd
import openpyxl


# =========================
# Ë®≠ÂÆö
# =========================
SAVE_DIR = "/Users/shiraishitoshio/bookmaker/outputs"
SEQMAP_PATH = os.path.join(SAVE_DIR, "seqmap.pkl")
_MATCH_ROOT_RE = re.compile(r"^(/match/[^/]+/[^/]+/[^/]+/)")

WORKER_TIMEOUT_SEC = 180      # Â≠ê„Éó„É≠„Çª„ÇπÂÖ®‰ΩìÔºà1Ë©¶ÂêàÔºâ
NAV_TIMEOUT_MS     = 20000    # safe_goto „ÅÆ 1st try
WAIT_TIMEOUT_MS    = 10000    # wait_for_selector

VERBOSE = True
def log(msg: str):
    if VERBOSE:
        print(msg, flush=True)

BOT_WALL_PAT = r"Just a moment|Access Denied|verify you are human|„ÉÅ„Çß„ÉÉ„ÇØ|Á¢∫Ë™ç"


# =========================
# HEADERÔºàExcelÂàóÔºâ
# =========================
HEADER = [
    "„Éõ„Éº„É†È†Ü‰Ωç","Ë©¶ÂêàÂõΩÂèä„Å≥„Ç´„ÉÜ„Ç¥„É™","Ë©¶ÂêàÊôÇÈñì","„Éõ„Éº„É†„ÉÅ„Éº„É†","„Éõ„Éº„É†„Çπ„Ç≥„Ç¢","„Ç¢„Ç¶„Çß„ÉºÈ†Ü‰Ωç","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†",
    "„Ç¢„Ç¶„Çß„Éº„Çπ„Ç≥„Ç¢","„Éõ„Éº„É†ÊúüÂæÖÂÄ§","„Ç¢„Ç¶„Çß„ÉºÊúüÂæÖÂÄ§","„Éõ„Éº„É†Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§","„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§",
    "„Éõ„Éº„É†„Éú„Éº„É´ÊîØÈÖçÁéá","„Ç¢„Ç¶„Çß„Éº„Éú„Éº„É´ÊîØÈÖçÁéá","„Éõ„Éº„É†„Ç∑„É•„Éº„ÉàÊï∞","„Ç¢„Ç¶„Çß„Éº„Ç∑„É•„Éº„ÉàÊï∞",
    "„Éõ„Éº„É†Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞","„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞","„Éõ„Éº„É†Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞","„Ç¢„Ç¶„Çß„ÉºÊû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞",
    "„Éõ„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà","„Ç¢„Ç¶„Çß„Éº„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà","„Éõ„Éº„É†„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ","„Ç¢„Ç¶„Çß„Éº„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ",
    "„Éõ„Éº„É†„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ","„Ç¢„Ç¶„Çß„Éº„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ","„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà","„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà",
    "„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà","„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà","„Éõ„Éº„É†„Ç¥„Éº„É´„Éù„Çπ„Éà","„Ç¢„Ç¶„Çß„Éº„Ç¥„Éº„É´„Éù„Çπ„Éà","„Éõ„Éº„É†„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´","„Ç¢„Ç¶„Çß„Éº„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´",
    "„Éõ„Éº„É†„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ","„Ç¢„Ç¶„Çß„Éº„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ","„Éõ„Éº„É†„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ","„Ç¢„Ç¶„Çß„Éº„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ",
    "„Éõ„Éº„É†„Ç™„Éï„Çµ„Ç§„Éâ","„Ç¢„Ç¶„Çß„Éº„Ç™„Éï„Çµ„Ç§„Éâ","„Éõ„Éº„É†„Éï„Ç°„Ç¶„É´","„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç¶„É´",
    "„Éõ„Éº„É†„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ","„Ç¢„Ç¶„Çß„Éº„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ","„Éõ„Éº„É†„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ","„Ç¢„Ç¶„Çß„Éº„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ","„Éõ„Éº„É†„Çπ„É≠„Éº„Ç§„É≥","„Ç¢„Ç¶„Çß„Éº„Çπ„É≠„Éº„Ç§„É≥",
    "„Éõ„Éº„É†Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ","„Ç¢„Ç¶„Çß„ÉºÁõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ","„Éõ„Éº„É†„Éë„Çπ","„Ç¢„Ç¶„Çß„Éº„Éë„Çπ","„Éõ„Éº„É†„É≠„É≥„Ç∞„Éë„Çπ","„Ç¢„Ç¶„Çß„Éº„É≠„É≥„Ç∞„Éë„Çπ","„Éõ„Éº„É†„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ","„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ",
    "„Éõ„Éº„É†„ÇØ„É≠„Çπ","„Ç¢„Ç¶„Çß„Éº„ÇØ„É≠„Çπ","„Éõ„Éº„É†„Çø„ÉÉ„ÇØ„É´","„Ç¢„Ç¶„Çß„Éº„Çø„ÉÉ„ÇØ„É´","„Éõ„Éº„É†„ÇØ„É™„Ç¢","„Ç¢„Ç¶„Çß„Éº„ÇØ„É™„Ç¢","„Éõ„Éº„É†„Éá„É•„Ç®„É´ÂãùÂà©Êï∞","„Ç¢„Ç¶„Çß„Éº„Éá„É•„Ç®„É´ÂãùÂà©Êï∞",
    "„Éõ„Éº„É†„Ç§„É≥„Çø„Éº„Çª„Éó„Éà","„Ç¢„Ç¶„Çß„Éº„Ç§„É≥„Çø„Éº„Çª„Éó„Éà",
    "„Çπ„Ç≥„Ç¢ÊôÇÈñì","Â§©Ê∞ó","Ê∞óÊ∏©","ÊπøÂ∫¶","ÂØ©Âà§Âêç","„Éõ„Éº„É†Áõ£Áù£Âêç","„Ç¢„Ç¶„Çß„ÉºÁõ£Áù£Âêç","„Éõ„Éº„É†„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥","„Ç¢„Ç¶„Çß„Éº„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥",
    "„Çπ„Çø„Ç∏„Ç¢„É†","ÂèéÂÆπ‰∫∫Êï∞","Ë¶≥ÂÆ¢Êï∞","Â†¥ÊâÄ","„Éõ„Éº„É†„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖ","„Éõ„Éº„É†„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖÂá∫Â†¥Áä∂Ê≥Å","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖÂá∫Â†¥Áä∂Ê≥Å",
    "„Éõ„Éº„É†„ÉÅ„Éº„É†„Éõ„Éº„É†ÂæóÁÇπ","„Éõ„Éº„É†„ÉÅ„Éº„É†„Éõ„Éº„É†Â§±ÁÇπ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Éõ„Éº„É†ÂæóÁÇπ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Éõ„Éº„É†Â§±ÁÇπ","„Éõ„Éº„É†„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂæóÁÇπ","„Éõ„Éº„É†„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂ§±ÁÇπ",
    "„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂæóÁÇπ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂ§±ÁÇπ","ÈÄöÁü•„Éï„É©„Ç∞","Ë©¶Âêà„É™„É≥„ÇØÊñáÂ≠óÂàó","„Ç¥„Éº„É´ÊôÇÈñì","ÈÅ∏ÊâãÂêç","Âà§ÂÆöÁµêÊûú","„Éõ„Éº„É†„ÉÅ„Éº„É†„Çπ„Çø„Ç§„É´","„Ç¢„Ç¶„Çß„Ç§„ÉÅ„Éº„É†„Çπ„Çø„Ç§„É´",
    "„Ç¥„Éº„É´Á¢∫Áéá","ÂæóÁÇπ‰∫àÊÉ≥ÊôÇÈñì","Ë©¶ÂêàID","ÈÄöÁï™","„ÇΩ„Éº„ÉàÁî®Áßí"
]


# =========================
# Áµ±Ë®à„Ç≠„Éº -> HEADERÂàóÂØæÂøúÔºàcanonical„Ç≠„ÉºÂâçÊèêÔºâ
# =========================
STAT_KEY_MAP = {
    "„Ç¢„Çø„ÉÉ„ÇØ:ÊúüÂæÖÂÄ§ÔºàxGÔºâ": ("„Éõ„Éº„É†ÊúüÂæÖÂÄ§", "„Ç¢„Ç¶„Çß„ÉºÊúüÂæÖÂÄ§"),
    "„Ç¢„Çø„ÉÉ„ÇØ:Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§": ("„Éõ„Éº„É†Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§", "„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§"),
    "„Éù„Çº„ÉÉ„Ç∑„Éß„É≥:„Éú„Éº„É´ÊîØÈÖçÁéá": ("„Éõ„Éº„É†„Éú„Éº„É´ÊîØÈÖçÁéá", "„Ç¢„Ç¶„Çß„Éº„Éú„Éº„É´ÊîØÈÖçÁéá"),
    "„Ç∑„É•„Éº„Éà:„Ç∑„É•„Éº„ÉàÊï∞": ("„Éõ„Éº„É†„Ç∑„É•„Éº„ÉàÊï∞", "„Ç¢„Ç¶„Çß„Éº„Ç∑„É•„Éº„ÉàÊï∞"),
    "„Ç∑„É•„Éº„Éà:Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞": ("„Éõ„Éº„É†Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞", "„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞"),
    "„Ç∑„É•„Éº„Éà:Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞": ("„Éõ„Éº„É†Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞", "„Ç¢„Ç¶„Çß„ÉºÊû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞"),
    "„Ç∑„É•„Éº„Éà:„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà": ("„Éõ„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà", "„Ç¢„Ç¶„Çß„Éº„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà"),
    "„Ç¢„Çø„ÉÉ„ÇØ:„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ": ("„Éõ„Éº„É†„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ", "„Ç¢„Ç¶„Çß„Éº„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ"),
    "„Çª„ÉÉ„Éà„Éó„É¨„Éº:„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ": ("„Éõ„Éº„É†„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ", "„Ç¢„Ç¶„Çß„Éº„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ"),
    "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà": ("„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà", "„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà"),
    "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà": ("„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà", "„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà"),
    "„Ç∑„É•„Éº„Éà:„Éù„Çπ„Éà„Éí„ÉÉ„Éà": ("„Éõ„Éº„É†„Ç¥„Éº„É´„Éù„Çπ„Éà", "„Ç¢„Ç¶„Çß„Éº„Ç¥„Éº„É´„Éù„Çπ„Éà"),
    "„Ç∑„É•„Éº„Éà:„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´": ("„Éõ„Éº„É†„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´", "„Ç¢„Ç¶„Çß„Éº„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´"),

    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ": ("„Éõ„Éº„É†„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ", "„Ç¢„Ç¶„Çß„Éº„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ": ("„Éõ„Éº„É†„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ", "„Ç¢„Ç¶„Çß„Éº„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç™„Éï„Çµ„Ç§„Éâ": ("„Éõ„Éº„É†„Ç™„Éï„Çµ„Ç§„Éâ", "„Ç¢„Ç¶„Çß„Éº„Ç™„Éï„Çµ„Ç§„Éâ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„Ç°„Ç¶„É´": ("„Éõ„Éº„É†„Éï„Ç°„Ç¶„É´", "„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç¶„É´"),
    "„Ç´„Éº„Éâ:„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ": ("„Éõ„Éº„É†„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ", "„Ç¢„Ç¶„Çß„Éº„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ"),
    "„Ç´„Éº„Éâ:„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ": ("„Éõ„Éº„É†„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ", "„Ç¢„Ç¶„Çß„Éº„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çπ„É≠„Éº„Ç§„É≥": ("„Éõ„Éº„É†„Çπ„É≠„Éº„Ç§„É≥", "„Ç¢„Ç¶„Çß„Éº„Çπ„É≠„Éº„Ç§„É≥"),

    "„Éë„Çπ:Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ": ("„Éõ„Éº„É†Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ", "„Ç¢„Ç¶„Çß„ÉºÁõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ"),
    "„Éë„Çπ:Á∑è„Éë„ÇπÊï∞": ("„Éõ„Éº„É†„Éë„Çπ", "„Ç¢„Ç¶„Çß„Éº„Éë„Çπ"),
    "„Éë„Çπ:„É≠„É≥„Ç∞„Éë„Çπ": ("„Éõ„Éº„É†„É≠„É≥„Ç∞„Éë„Çπ", "„Ç¢„Ç¶„Çß„Éº„É≠„É≥„Ç∞„Éë„Çπ"),
    "„Éë„Çπ:„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ": ("„Éõ„Éº„É†„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ", "„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ"),
    "„Éë„Çπ:„ÇØ„É≠„Çπ": ("„Éõ„Éº„É†„ÇØ„É≠„Çπ", "„Ç¢„Ç¶„Çß„Éº„ÇØ„É≠„Çπ"),

    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çø„ÉÉ„ÇØ„É´": ("„Éõ„Éº„É†„Çø„ÉÉ„ÇØ„É´", "„Ç¢„Ç¶„Çß„Éº„Çø„ÉÉ„ÇØ„É´"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„ÇØ„É™„Ç¢": ("„Éõ„Éº„É†„ÇØ„É™„Ç¢", "„Ç¢„Ç¶„Çß„Éº„ÇØ„É™„Ç¢"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éá„É•„Ç®„É´ÂãùÂà©": ("„Éõ„Éº„É†„Éá„É•„Ç®„É´ÂãùÂà©Êï∞", "„Ç¢„Ç¶„Çß„Éº„Éá„É•„Ç®„É´ÂãùÂà©Êï∞"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç§„É≥„Çø„Éº„Çª„Éó„Éà": ("„Éõ„Éº„É†„Ç§„É≥„Çø„Éº„Çª„Éó„Éà", "„Ç¢„Ç¶„Çß„Éº„Ç§„É≥„Çø„Éº„Çª„Éó„Éà"),
}


# =========================
# Flashscore„Äå„É©„Éô„É´„Äç‚Üí canonical„Ç≠„Éº
# =========================
LABEL_TO_CANON: Dict[str, str] = {
    "„Ç¥„Éº„É´ÊúüÂæÖÂÄ§ÔºàxGÔºâ": "„Ç¢„Çø„ÉÉ„ÇØ:ÊúüÂæÖÂÄ§ÔºàxGÔºâ",
    "Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§ÔºàxGOTÔºâ": "„Ç¢„Çø„ÉÉ„ÇØ:Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§",

    "„Éú„Éº„É´ÊîØÈÖçÁéá": "„Éù„Çº„ÉÉ„Ç∑„Éß„É≥:„Éú„Éº„É´ÊîØÈÖçÁéá",
    "ÂêàË®à„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Ç∑„É•„Éº„ÉàÊï∞",
    "Êû†ÂÜÖ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞",
    "Êû†Â§ñ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞",
    "„Ç∑„É•„Éº„Éà„Éñ„É≠„ÉÉ„ÇØ": "„Ç∑„É•„Éº„Éà:„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà",
    "„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Åã„Çâ„ÅÆ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà",
    "„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Åã„Çâ„ÅÆ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà",
    "„Ç¥„Éº„É´Êû†„Å´ÂΩì„Åü„Çã": "„Ç∑„É•„Éº„Éà:„Éù„Çπ„Éà„Éí„ÉÉ„Éà",
    "„Ç¥„Éº„É´Êû†„Å´ÂΩì„Åü„Çã„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Éù„Çπ„Éà„Éí„ÉÉ„Éà",

    "„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ": "„Ç¢„Çø„ÉÉ„ÇØ:„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ",
    "„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ": "„Çª„ÉÉ„Éà„Éó„É¨„Éº:„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ",

    "„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ": "„Ç´„Éº„Éâ:„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ",
    "„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ": "„Ç´„Éº„Éâ:„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ",
    "„Éï„Ç°„Ç¶„É´": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„Ç°„Ç¶„É´",
    "„Ç™„Éï„Çµ„Ç§„Éâ": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç™„Éï„Çµ„Ç§„Éâ",
    "„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ",
    "„Çπ„É≠„Éº„Ç§„É≥": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çπ„É≠„Éº„Ç§„É≥",
    "„Çø„ÉÉ„ÇØ„É´": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çø„ÉÉ„ÇØ„É´",
    "„Éá„É•„Ç®„É´ÂãùÂà©Êï∞": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éá„É•„Ç®„É´ÂãùÂà©",
    "„ÇØ„É™„Ç¢„É™„É≥„Ç∞": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„ÇØ„É™„Ç¢",
    "„ÇØ„É™„Ç¢": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„ÇØ„É™„Ç¢",
    "„Ç§„É≥„Çø„Éº„Çª„Éó„Éà": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç§„É≥„Çø„Éº„Çª„Éó„Éà",
    "„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ",

    "Áõ∏Êâã„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Åß„ÅÆ„Çø„ÉÉ„ÉÅ": "„Éë„Çπ:Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ",
    "Áõ∏Êâã„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Çø„ÉÉ„ÉÅ": "„Éë„Çπ:Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ",
    "„Éë„Çπ": "„Éë„Çπ:Á∑è„Éë„ÇπÊï∞",
    "„É≠„É≥„Ç∞„Éë„Çπ": "„Éë„Çπ:„É≠„É≥„Ç∞„Éë„Çπ",
    "„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Åß„ÅÆ„Éë„Çπ": "„Éë„Çπ:„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ",
    "„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„ÅÆ„Éë„Çπ": "„Éë„Çπ:„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ",
    "„ÇØ„É≠„Çπ": "„Éë„Çπ:„ÇØ„É≠„Çπ",
}

# Âêå‰∏Ä„É©„Éô„É´„ÅåË§áÊï∞„Çª„ÇØ„Ç∑„Éß„É≥„Å´Âá∫„ÇãÂ†¥Âêà„ÄÅÂÑ™ÂÖàÈ†Ü‰ΩçÔºà‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ„ÇíÂÑ™ÂÖàÔºâ
SECTION_PREFER = ["‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ", "„Ç∑„É•„Éº„Éà", "„Ç¢„Çø„ÉÉ„ÇØ", "„Éë„Çπ", "„Éá„Ç£„Éï„Çß„É≥„Çπ", "„Ç¥„Éº„É´„Ç≠„Éº„Éë„Éº"]

def _section_rank(section: str) -> int:
    try:
        return SECTION_PREFER.index(section)
    except ValueError:
        return 999


# =========================
# ÂØæË±°„É™„Éº„Ç∞„Éï„Ç£„É´„ÇøÔºàË¶™„Åß‰ΩøÁî®Ôºâ
# =========================
CONTAINS_LIST = [
    "„Ç±„Éã„Ç¢: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Ç≥„É≠„É≥„Éì„Ç¢: „Éó„É™„É°„Éº„É© A", "„Çø„É≥„Ç∂„Éã„Ç¢: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞",
    "„Ç§„É≥„Ç∞„É©„É≥„Éâ: EFL „ÉÅ„É£„É≥„Éî„Ç™„É≥„Ç∑„ÉÉ„Éó", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: EFL „É™„Éº„Ç∞ 1", "„Ç®„ÉÅ„Ç™„Éî„Ç¢: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Ç≥„Çπ„Çø„É™„Ç´: „É™„Éº„Ç¨ FPD",
    "„Ç∏„É£„Éû„Ç§„Ç´: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Çπ„Éö„Ç§„É≥: „É©„Éª„É™„Éº„Ç¨", "„Éñ„É©„Ç∏„É´: „Çª„É™„Ç® A „Éô„Çø„Éº„Éé", "„Éñ„É©„Ç∏„É´: „Çª„É™„Ç® B", "„Éâ„Ç§„ÉÑ: „Éñ„É≥„Éá„Çπ„É™„Éº„Ç¨",
    "ÈüìÂõΩ: K „É™„Éº„Ç∞ 1", "‰∏≠ÂõΩ: ‰∏≠ÂõΩ„Çπ„Éº„Éë„Éº„É™„Éº„Ç∞", "Êó•Êú¨: J1 „É™„Éº„Ç∞", "Êó•Êú¨: J2 „É™„Éº„Ç∞", "Êó•Êú¨: J3 „É™„Éº„Ç∞", "„Ç§„É≥„Éâ„Éç„Ç∑„Ç¢: „Çπ„Éº„Éë„Éº„É™„Éº„Ç∞",
    "„Ç™„Éº„Çπ„Éà„É©„É™„Ç¢: A „É™„Éº„Ç∞„Éª„É°„É≥", "„ÉÅ„É•„Éã„Ç∏„Ç¢: „ÉÅ„É•„Éã„Ç∏„Ç¢ÔΩ•„Éó„É≠„É™„Éº„Ç∞", "„Ç¶„Ç¨„É≥„ÉÄ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„É°„Ç≠„Ç∑„Ç≥: „É™„Éº„Ç¨ MX",
    "„Éï„É©„É≥„Çπ: „É™„Éº„Ç∞„Éª„Ç¢„É≥", "„Çπ„Ç≥„ÉÉ„Éà„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„Ç∑„ÉÉ„Éó", "„Ç™„É©„É≥„ÉÄ: „Ç®„Éº„É´„Éá„Ç£„Éì„Ç∏", "„Ç¢„É´„Çº„É≥„ÉÅ„É≥: „Éà„É´„Éç„Ç™„Éª„Éô„Çø„Éº„Éé",
    "„Ç§„Çø„É™„Ç¢: „Çª„É™„Ç® A", "„Ç§„Çø„É™„Ç¢: „Çª„É™„Ç® B", "„Éù„É´„Éà„Ç¨„É´: „É™„Éº„Ç¨„Éª„Éù„É´„Éà„Ç¨„É´", "„Éà„É´„Ç≥: „Çπ„É•„Éö„É´„Éª„É™„Ç∞", "„Çª„É´„Éì„Ç¢: „Çπ„Éº„Éö„É´„É™„Éº„Ç¨",
    "Êó•Êú¨: WE„É™„Éº„Ç∞", "„Éú„É™„Éì„Ç¢: LFPB", "„Éñ„É´„Ç¨„É™„Ç¢: „Éë„É´„É¥„Ç°„Éª„É™„Éº„Ç¨", "„Ç´„É°„É´„Éº„É≥: „Ç®„É™„Éº„Éà 1", "„Éö„É´„Éº: „É™„Éº„Ç¨ 1",
    "„Ç®„Çπ„Éà„Éã„Ç¢: „É°„Çπ„Çø„É™„É™„Éº„Ç¨", "„Ç¶„ÇØ„É©„Ç§„Éä: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Éô„É´„ÇÆ„Éº: „Ç∏„É•„Éî„É©„ÉºÔΩ•„Éó„É≠„É™„Éº„Ç∞", "„Ç®„ÇØ„Ç¢„Éâ„É´: „É™„Éº„Ç¨„Éª„Éó„É≠",
    "Êó•Êú¨: YBC „É´„É¥„Ç°„É≥„Ç´„ÉÉ„Éó", "Êó•Êú¨: Â§©ÁöáÊùØ"
]
UNDER_LIST  = ["U17", "U18", "U19", "U20", "U21", "U22", "U23", "U24", "U25"]
GENDER_LIST = ["Â•≥Â≠ê"]
EXP_LIST    = ["„Éù„É´„Éà„Ç¨„É´: „É™„Éº„Ç¨„Éª„Éù„É´„Éà„Ç¨„É´ 2", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞ 2", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞ U18"]


# =========================
# SEQMAP
# =========================
SEQMAP: Dict[str, int] = {}

def load_seqmap():
    global SEQMAP
    if os.path.exists(SEQMAP_PATH):
        try:
            with open(SEQMAP_PATH, "rb") as f:
                SEQMAP = pickle.load(f) or {}
            log(f"üîÅ [SEQ] Ë™≠„ÅøËæº„Åø: {SEQMAP_PATH}Ôºà‰ª∂Êï∞={len(SEQMAP)}Ôºâ")
        except Exception as e:
            log(f"‚ö†Ô∏è [SEQ] loadÂ§±Êïó: {e}")
            SEQMAP = {}
    else:
        SEQMAP = {}
        log(f"üÜï [SEQ] Êñ∞Ë¶èÔºà{SEQMAP_PATH} „ÅåÂ≠òÂú®„Åó„Åæ„Åõ„ÇìÔºâ")

def save_seqmap():
    try:
        os.makedirs(os.path.dirname(SEQMAP_PATH), exist_ok=True)
        with open(SEQMAP_PATH, "wb") as f:
            pickle.dump(SEQMAP, f)
        log(f"üíæ [SEQ] ‰øùÂ≠ò: {SEQMAP_PATH}Ôºà‰ª∂Êï∞={len(SEQMAP)}Ôºâ")
    except Exception as e:
        log(f"‚ö†Ô∏è [SEQ] saveÂ§±Êïó: {e}")


# =========================
# util
# =========================
def text_clean(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def parse_live_time_to_seconds(tstr: str) -> int:
    if not tstr:
        return 0
    t = tstr.strip()
    if "ÁµÇ‰∫Ü" in t or "FT" in t.upper():
        return 5400
    if "ÂâçÂçä" in t:
        num = re.sub(r"[^0-9]", "", t)
        return int(num) * 60 if num else 0
    if "ÂæåÂçä" in t:
        num = re.sub(r"[^0-9]", "", t)
        return 2700 + int(num) * 60 if num else 2700
    if re.match(r"^\d+\+\d+$", t):
        a, b = t.split("+")
        return (int(a) + int(b)) * 60
    if t.isdigit():
        return int(t) * 60
    return 0


# =========================
# VERIFYÔºà„Éû„ÉÉ„Éî„É≥„Ç∞„Å®Âüã„Åæ„Çä„ÉÅ„Çß„ÉÉ„ÇØÔºâ
# =========================
def verify_header_and_stat_map():
    bad = []
    for k, (hcol, acol) in STAT_KEY_MAP.items():
        if hcol not in HEADER:
            bad.append(("HOME", k, hcol))
        if acol not in HEADER:
            bad.append(("AWAY", k, acol))
    if bad:
        log("‚ùå [VERIFY] STAT_KEY_MAP „ÅåÂèÇÁÖß„Åó„Å¶„ÅÑ„ÇãÂàó„Åå HEADER „Å´ÁÑ°„ÅÑ")
        for side, k, col in bad:
            log(f"   - {side} key='{k}' col='{col}'")
        raise RuntimeError("STAT_KEY_MAP column mismatch with HEADER")
    log("‚úÖ [VERIFY] STAT_KEY_MAP „ÅÆÂàóÂêç„ÅØ HEADER „Å®Êï¥Âêà„Åó„Å¶„ÅÑ„Åæ„Åô")

def verify_stats_mapping(keys_title: str, stats_pairs: Dict[str, Tuple[str, str]]):
    scraped_keys = set(stats_pairs.keys())
    mapped_keys  = set(STAT_KEY_MAP.keys())

    direct_hit   = sorted(scraped_keys & mapped_keys)
    miss_scraped = sorted(scraped_keys - mapped_keys)

    log(f"‚úÖ [VERIFY:{keys_title}] Áõ¥Êé•‰∏ÄËá¥„Ç≠„ÉºÊï∞: {len(direct_hit)}")
    for k in direct_hit[:40]:
        hv, av = stats_pairs.get(k, ("",""))
        log(f"   ‚úì {k} = ({hv}, {av})")

    log(f"‚ö†Ô∏è [VERIFY:{keys_title}] „Éû„ÉÉ„Éó„Å´ÁÑ°„ÅÑ„Çπ„ÇØ„É¨„Ç§„Éó„Ç≠„ÉºÊï∞: {len(miss_scraped)}")
    for k in miss_scraped[:60]:
        hv, av = stats_pairs.get(k, ("",""))
        log(f"   ? {k} = ({hv}, {av})")

def verify_row_filled(rb: "RowBuilder", limit: int = 120):
    filled = []
    for col in HEADER:
        v = rb.d.get(col, "")
        if v not in ("", None):
            filled.append((col, v))
    log(f"‚úÖ [VERIFY] row „Å´ÂÄ§„ÅåÂÖ•„Å£„ÅüÂàóÊï∞: {len(filled)}")
    for col, v in filled[:limit]:
        log(f"   ‚Ä¢ {col} = {v}")


# =========================
# URL builders
# =========================
def extract_mid(any_url: str) -> Optional[str]:
    if not any_url:
        return None
    qs = parse_qs(urlparse(any_url).query)
    return qs.get("mid", [None])[0]

def _match_root_from_any_url(any_url: str) -> Tuple[Tuple[str, str, str], str]:
    parts = urlsplit(any_url)
    path = parts.path or ""
    m = _MATCH_ROOT_RE.search(path)
    if not m:
        raise ValueError(f"match root not found: {any_url}")
    match_root_path = m.group(1)
    mid = extract_mid(any_url)
    return (parts.scheme, parts.netloc, match_root_path), (mid or "")

def build_stats_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "summary/stats/0/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_summary_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "summary/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_live_standings_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "standings/live-standings/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_overall_standings_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "standings/standings/overall/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))


# =========================
# STOPÂØæÁ≠ñÔºösafe_gotoÔºà‚òÖÊúÄÈáçË¶ÅÔºâ
# =========================
def safe_goto(pg, url: str, timeout_ms: int = NAV_TIMEOUT_MS, tag: str = "") -> bool:
    ttag = f" {tag}" if tag else ""
    log(f"üß≠ [GOTO]{ttag} try1 commit: {url}")
    try:
        pg.goto(url, timeout=timeout_ms, wait_until="commit")
        return True
    except PWTimeout as e:
        log(f"‚è±Ô∏è [GOTO]{ttag} try1 timeout: {e}")
    except Exception as e:
        log(f"‚ö†Ô∏è [GOTO]{ttag} try1 error: {type(e).__name__}: {e}")

    try:
        log(f"üõë [GOTO]{ttag} window.stop()")
        pg.evaluate("() => window.stop()")
    except Exception as e:
        log(f"‚ö†Ô∏è [GOTO]{ttag} window.stop error: {type(e).__name__}: {e}")

    log(f"üß≠ [GOTO]{ttag} try2 domcontentloaded: {url}")
    try:
        pg.goto(url, timeout=8000, wait_until="domcontentloaded")
        return True
    except PWTimeout as e:
        log(f"‚è±Ô∏è [GOTO]{ttag} try2 timeout: {e}")
    except Exception as e:
        log(f"‚ö†Ô∏è [GOTO]{ttag} try2 error: {type(e).__name__}: {e}")

    try:
        log(f"üßº [GOTO]{ttag} about:blank")
        pg.goto("about:blank", timeout=3000, wait_until="commit")
    except Exception:
        pass

    log(f"üß≠ [GOTO]{ttag} try3 domcontentloaded(after blank): {url}")
    try:
        pg.goto(url, timeout=8000, wait_until="domcontentloaded")
        return True
    except Exception as e:
        log(f"‚ùå [GOTO]{ttag} try3 failed: {type(e).__name__}: {e}")
        return False


# =========================
# consent / cmp
# =========================
def kill_onetrust(page):
    try:
        btn = page.locator("#onetrust-accept-btn-handler")
        if btn.count():
            btn.click(timeout=1200, force=True)
    except:
        pass
    try:
        page.evaluate("""
        () => {
          const ids = ["onetrust-consent-sdk", "onetrust-banner-sdk"];
          ids.forEach(id => document.getElementById(id)?.remove());
          document.querySelectorAll(".ot-sdk-container, .ot-sdk-row, .otOverlay, .ot-pc-footer, .ot-pc-header")
            .forEach(el => el.remove());
        }""", timeout=1200)
    except:
        pass

def kill_consent_banners(page):
    kill_onetrust(page)
    candidates = [
        "button:has-text('„Åô„Åπ„Å¶ÊãíÂê¶„Åô„Çã')",
        "button:has-text('ÂÖ®„Å¶ÊãíÂê¶„Åô„Çã')",
        "button:has-text('ÊãíÂê¶„Åô„Çã')",
        "button:has-text('ÂêåÊÑè„Åó„Åæ„Åô')",
        "button:has-text('ÂêåÊÑè„Åô„Çã')",
        "button:has-text('Reject all')",
        "button:has-text('Accept all')",
        "[role='button']:has-text('„Åô„Åπ„Å¶ÊãíÂê¶„Åô„Çã')",
        "[role='button']:has-text('ÂêåÊÑè„Åó„Åæ„Åô')",
    ]
    for sel in candidates:
        try:
            b = page.locator(sel).first
            if b.count() and b.is_visible(timeout=500):
                b.click(timeout=1200, force=True)
                break
        except:
            pass

    try:
        page.evaluate("""
        () => {
          const kill = [
            "#qc-cmp2-container",
            "#didomi-host",
            "#sp_message_container_",
            ".fc-consent-root",
            ".message-component",
            ".pmConsentWall"
          ];
          kill.forEach(s => document.querySelectorAll(s).forEach(el => el.remove()));
        }""", timeout=1200)
    except:
        pass

def is_bot_wall(pg) -> bool:
    try:
        return pg.locator(f"text=/{BOT_WALL_PAT}/i").first.is_visible(timeout=900)
    except:
        return False


# =========================
# route blockingÔºàËªΩÈáèÂåñÔºâ
# =========================
def setup_route_blocking(ctx):
    def _route(route):
        try:
            rtype = route.request.resource_type
            url = (route.request.url or "").lower()
            if rtype in ("image", "media", "font"):
                return route.abort()
            if any(x in url for x in ("doubleclick", "googlesyndication", "adservice", "adsystem", "taboola", "outbrain")):
                return route.abort()
        except:
            pass
        return route.continue_()
    ctx.route("**/*", _route)


# =========================
# Ë©¶Âêà„Éö„Éº„Ç∏Ôºö„ÉÅ„Éº„É†Âêç„Éª„Çπ„Ç≥„Ç¢„ÉªÊôÇÈñì
# =========================
def get_home_away_names(pg) -> Tuple[str, str]:
    try:
        cont = pg.locator("div.duelParticipant__container").first
        if not cont.count():
            cont = pg
        h = cont.locator(".duelParticipant__home a.participant__participantName").first
        a = cont.locator(".duelParticipant__away a.participant__participantName").first
        home = text_clean(h.text_content()) if h.count() else ""
        away = text_clean(a.text_content()) if a.count() else ""
        if not home:
            img = cont.locator(".duelParticipant__home img.participant__image").first
            home = text_clean(img.get_attribute("alt") or "")
        if not away:
            img = cont.locator(".duelParticipant__away img.participant__image").first
            away = text_clean(img.get_attribute("alt") or "")
        return home, away
    except:
        return "", ""

def get_scores(pg) -> Tuple[str, str]:
    def _clean(s: str) -> str:
        return re.sub(r"\s+", "", (s or "").replace("\u00A0", " ")).strip()

    def _from_container(el):
        try:
            spans = el.locator("span")
            vals = []
            for i in range(spans.count()):
                sp = spans.nth(i)
                cls = (sp.get_attribute("class") or "")
                if "divider" in cls:
                    continue
                txt = _clean(sp.text_content() or "")
                if re.fullmatch(r"\d+", txt):
                    vals.append(txt)
            if len(vals) >= 2:
                return vals[0], vals[-1]
        except:
            pass

        try:
            t = _clean(el.inner_text() or "")
            m = re.search(r"(\d+)\s*[\-\u2212\u2012\u2013\u2014\u2015]\s*(\d+)", t)
            if m:
                return m.group(1), m.group(2)
        except:
            pass
        return "", ""

    try:
        fx = pg.locator(".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedScore").first
        if fx.count():
            h, a = _from_container(fx)
            if h and a:
                return h, a
    except:
        pass

    try:
        live = pg.locator("div.detailScore__wrapper.detailScore__live").first
        if live.count():
            h, a = _from_container(live)
            if h and a:
                return h, a
    except:
        pass

    try:
        wrap = pg.locator("div.detailScore__wrapper").first
        if wrap.count():
            h, a = _from_container(wrap)
            if h and a:
                return h, a
    except:
        pass

    return "", ""

def get_match_time_text(pg) -> str:
    sels = [
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedHeaderDuel__detailStatus",
        "div.detailScore__status .fixedHeaderDuel__detailStatus",
        "div.detailScore__status .eventAndAddedTime .eventTime",
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .eventAndAddedTime .eventTime",
        "div.detailScore__status",
        "[data-testid='wcl-time']",
    ]
    for s in sels:
        try:
            el = pg.locator(f"{s} >> visible=true").first
            if el.count():
                txt = (el.text_content() or "").strip().replace("\u00A0", " ")
                if txt:
                    return txt
        except:
            pass
    return ""


# =========================
# RowBuilder
# =========================
class RowBuilder:
    def __init__(self, header: List[str], match_tag: str = ""):
        self.header = header
        self.d = {col: "" for col in header}
        self.match_tag = match_tag

    def put(self, key: str, value: Any):
        if key not in self.d:
            return
        self.d[key] = "" if value is None else value

    def put_pair(self, home_key: str, away_key: str, home_val: Any, away_val: Any):
        self.put(home_key, home_val)
        self.put(away_key, away_val)

def apply_stats_to_row(rb: RowBuilder, canon_pairs: Dict[str, Tuple[str, str]]):
    for stat_key, (hcol, acol) in STAT_KEY_MAP.items():
        if stat_key in canon_pairs:
            hv, av = canon_pairs[stat_key]
            rb.put_pair(hcol, acol, hv, av)


# =========================
# statsÔºörawÂèñÂæóÔºàÂä†Â∑•„Å™„ÅóÔºâ
# =========================
STAT_ROW_SELECTOR = "[data-testid='wcl-statistics']"

def goto_statistics_page(pg) -> bool:
    stats_url = build_stats_url(pg.url)

    kill_consent_banners(pg)

    if "/summary/stats/0/" not in pg.url:
        log(f"‚û°Ô∏è [STATS] goto: {stats_url}")
        ok = safe_goto(pg, stats_url, timeout_ms=NAV_TIMEOUT_MS, tag="STATS")
        if not ok:
            log("‚ö†Ô∏è [STATS] gotoÂ§±ÊïóÔºàsafe_goto„Åß„ÇÇÔºâ")
            return False
        kill_consent_banners(pg)

    if is_bot_wall(pg):
        log("üß± [STATS] BOTÂ£Å„Å£„ÅΩ„ÅÑ ‚Üí stats„Çπ„Ç≠„ÉÉ„Éó")
        return False

    log("‚è≥ [STATS] wait_for_selector wcl-statistics ...")
    for attempt in range(2):
        try:
            pg.wait_for_selector(STAT_ROW_SELECTOR, timeout=WAIT_TIMEOUT_MS)
            log("‚úÖ [STATS] statistics selector appeared")
            return True
        except Exception as e:
            log(f"‚ö†Ô∏è [STATS] wait_for_selector failed({attempt+1}/2): {type(e).__name__}: {e}")
            kill_consent_banners(pg)

    log("‚ö†Ô∏è [STATS] Áµ±Ë®àÂá∫„Å™„ÅÑÔºàÂêåÊÑèÈô§ÂéªÂæå„ÇÇÔºâ")
    return False

def scrape_stats_raw(pg) -> Dict[str, List[str]]:
    """
    out: {"‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ:„Éë„Çπ": ["34%Ôºà31/90Ôºâ","51%Ôºà24/47Ôºâ"], ...}
    ‚Äª ÂÄ§„ÅØÂä†Â∑•„Åó„Å™„ÅÑ
    """
    if not goto_statistics_page(pg):
        return {}

    log("üìä [STATS] JS extraction start")
    try:
        pg.wait_for_selector('[data-testid="wcl-statistics"]', timeout=WAIT_TIMEOUT_MS)
    except Exception:
        log("‚ö†Ô∏è [STATS] statistics selector not found")
        return {}

    stats = pg.evaluate("""
    () => {
      const out = {};
      const wrapper = document.querySelector('.sectionsWrapper');
      if (!wrapper) return out;

      const clean = (s) =>
        (s || '')
          .replace(/\\u00A0/g, ' ')
          .replace(/\\s+/g, ' ')
          .trim();

      const pickValue = (cell) => {
        if (!cell) return '';
        const spans = cell.querySelectorAll("span[data-testid='wcl-scores-simple-text-01']");
        if (!spans || spans.length === 0) return '';
        const parts = [];
        for (const sp of spans) {
          const t = clean(sp.textContent);
          if (t) parts.push(t);
        }
        // ‚òÖÈ†ÜÁï™„Åù„ÅÆ„Åæ„ÅæÈÄ£ÁµêÔºà‰æã: "34%Ôºà31/90Ôºâ"Ôºâ
        return parts.join(' ');
      };

      const sections = wrapper.querySelectorAll('.section');
      for (const sec of sections) {
        const sectionTitle = clean(sec.querySelector('.sectionHeader')?.textContent);
        if (!sectionTitle) continue;

        const rows = sec.querySelectorAll('[data-testid="wcl-statistics"]');
        for (const row of rows) {
          const label = clean(
            row.querySelector('[data-testid="wcl-statistics-category"] span')?.textContent
          );
          if (!label) continue;

          const values = row.querySelectorAll('[data-testid="wcl-statistics-value"]');
          if (!values || values.length < 2) continue;

          const home = pickValue(values[0]);
          const away = pickValue(values[values.length - 1]);

          out[`${sectionTitle}:${label}`] = [home, away];
        }
      }
      return out;
    }
    """) or {}

    log(f"üìä [STATS] extracted items = {len(stats)}")
    return stats


# =========================
# statsÔºöraw -> canonicalÔºàlabel‚Üícanon, „Çª„ÇØ„Ç∑„Éß„É≥ÂÑ™ÂÖàÔºâ
# =========================
def normalize_stats_raw_to_canon(stats_raw: Dict[str, Any]) -> Dict[str, Tuple[str, str]]:
    """
    stats_raw: {"‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ:ÂêàË®à„Ç∑„É•„Éº„Éà":[home,away], ...}
    -> {"„Ç∑„É•„Éº„Éà:„Ç∑„É•„Éº„ÉàÊï∞":(home,away), ...}
    """
    if not stats_raw:
        return {}

    best_by_label: Dict[str, Tuple[int, Tuple[str, str]]] = {}

    for k, v in stats_raw.items():
        if not isinstance(v, (list, tuple)) or len(v) < 2:
            continue
        if ":" not in k:
            continue

        section, label = k.split(":", 1)
        section = (section or "").strip()
        label = (label or "").strip()

        canon = LABEL_TO_CANON.get(label)
        if not canon:
            continue

        hv, av = str(v[0]), str(v[1])
        rank = _section_rank(section)

        if (label not in best_by_label) or (rank < best_by_label[label][0]):
            best_by_label[label] = (rank, (hv, av))

    out: Dict[str, Tuple[str, str]] = {}
    for label, (_, pair) in best_by_label.items():
        canon = LABEL_TO_CANON.get(label)
        if canon:
            out[canon] = pair
    return out

def dump_unmapped_stats(stats_raw: Dict[str, Any]):
    labels = []
    for k in (stats_raw or {}).keys():
        if ":" in k:
            labels.append(k.split(":", 1)[1].strip())
    unmapped = sorted({lb for lb in labels if lb and lb not in LABEL_TO_CANON})
    if unmapped:
        log(f"üß™ [STATS] unmapped labels ({len(unmapped)}): {unmapped}")


# =========================
# metaÔºàsummaryÔºâ
# =========================
def get_match_meta(pg) -> Dict[str, str]:
    meta: Dict[str, str] = {}
    summary_url = build_summary_url(pg.url)

    if "/summary/" not in pg.url or "/summary/stats" in pg.url:
        log(f"‚û°Ô∏è [META] goto: {summary_url}")
        ok = safe_goto(pg, summary_url, timeout_ms=NAV_TIMEOUT_MS, tag="META")
        if not ok:
            meta["Ë©¶ÂêàÊôÇÈñì"] = get_match_time_text(pg)
            meta["ÂèñÂæóÊôÇÂàª"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            return meta
        kill_consent_banners(pg)

    if is_bot_wall(pg):
        meta["Ë©¶ÂêàÊôÇÈñì"] = get_match_time_text(pg)
        meta["ÂèñÂæóÊôÇÂàª"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return meta

    try:
        pg.wait_for_selector(
            "ol li span[itemprop='name'], [data-testid*='breadcrumbs'], .tournamentHeader, .duelParticipant",
            timeout=WAIT_TIMEOUT_MS
        )
    except:
        pass

    country = ""
    league = ""

    try:
        crumb_txts = []
        cand = pg.locator("ol li span[itemprop='name'], [data-testid*='breadcrumbs'] span[itemprop='name']")
        for i in range(cand.count()):
            t = text_clean(cand.nth(i).text_content() or "")
            if t:
                crumb_txts.append(t)
        if len(crumb_txts) >= 2:
            country = crumb_txts[1]
        if len(crumb_txts) >= 3:
            league = crumb_txts[2]
    except:
        pass

    if not country or not league:
        try:
            c = pg.locator(".tournamentHeader__country, .tournamentHeader__category, [class*='tournamentHeader__category']").first
            t = text_clean(c.text_content() or "")
            if t:
                if ":" in t and (not country or not league):
                    a, b = [x.strip() for x in t.split(":", 1)]
                    if not country:
                        country = a
                    if not league:
                        league = b
                elif not country:
                    country = t
        except:
            pass

        try:
            l = pg.locator(".tournamentHeader__name, [class*='tournamentHeader__name']").first
            t = text_clean(l.text_content() or "")
            if t and not league:
                league = t
        except:
            pass

    meta["ÂõΩ"] = country
    meta["„É™„Éº„Ç∞"] = league

    label_aliases = {
        "„É¨„Éï„Çß„É™„Éº": "„É¨„Éï„Çß„É™„Éº",
        "ÂØ©Âà§": "„É¨„Éï„Çß„É™„Éº",
        "‰∏ªÂØ©": "„É¨„Éï„Çß„É™„Éº",
        "ÈñãÂÇ¨Âú∞": "ÈñãÂÇ¨Âú∞",
        "„Çπ„Çø„Ç∏„Ç¢„É†": "ÈñãÂÇ¨Âú∞",
        "‰ºöÂ†¥": "ÈñãÂÇ¨Âú∞",
        "ÂèéÂÆπ‰∫∫Êï∞": "ÂèéÂÆπ‰∫∫Êï∞",
        "„Ç≠„É£„Éë„Ç∑„ÉÜ„Ç£": "ÂèéÂÆπ‰∫∫Êï∞",
        "Ë¶≥ÂÆ¢": "Ë¶≥ÂÆ¢",
        "Ë¶≥ÂÆ¢Êï∞": "Ë¶≥ÂÆ¢",
        "ÂèÇÂä†": "Ë¶≥ÂÆ¢",
    }
    want = set(label_aliases.keys())

    def put_meta(label: str, value: str):
        label = text_clean(label).replace(":", "").replace("Ôºö", "").strip()
        value = text_clean(value)
        if not label or not value:
            return
        norm = label_aliases.get(label, label)
        if norm not in meta:
            meta[norm] = value

    try:
        dts = pg.locator("dl dt")
        n = dts.count()
        for i in range(n):
            dt = dts.nth(i)
            lab = text_clean(dt.text_content() or "")
            if lab not in want:
                continue
            dd = dt.locator("xpath=following-sibling::dd[1]").first
            val = text_clean(dd.text_content() or "") if dd.count() else ""
            put_meta(lab, val)
    except:
        pass

    meta["Ë©¶ÂêàÊôÇÈñì"] = get_match_time_text(pg)
    meta["ÂèñÂæóÊôÇÂàª"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return meta


# =========================
# standingsÔºàÈ†Ü‰ΩçÔºâ
# =========================
def goto_standings_page(pg) -> Optional[str]:
    url1 = build_live_standings_url(pg.url)
    url2 = build_overall_standings_url(pg.url)

    log(f"‚û°Ô∏è [RANK] goto(live): {url1}")
    ok = safe_goto(pg, url1, timeout_ms=NAV_TIMEOUT_MS, tag="RANK-LIVE")
    if ok:
        kill_consent_banners(pg)
        try:
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=WAIT_TIMEOUT_MS)
            return url1
        except Exception as e:
            log(f"‚ö†Ô∏è [RANK] live wait failed: {type(e).__name__}: {e}")

    log(f"‚û°Ô∏è [RANK] goto(overall): {url2}")
    ok = safe_goto(pg, url2, timeout_ms=NAV_TIMEOUT_MS, tag="RANK-OVERALL")
    if ok:
        kill_consent_banners(pg)
        try:
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=WAIT_TIMEOUT_MS)
            return url2
        except Exception as e:
            log(f"‚ö†Ô∏è [RANK] overall wait failed: {type(e).__name__}: {e}")

    return None

def get_match_standings(pg, home_name: str, away_name: str) -> Dict[str, Any]:
    url = goto_standings_page(pg)
    if not url:
        return {}

    rows = pg.locator(".ui-table__body .ui-table__row")
    try:
        n = rows.count()
    except:
        n = 0
    if n == 0:
        return {}

    table = {}
    for i in range(n):
        r = rows.nth(i)
        rank_txt = text_clean(r.locator(".table__cell--rank .tableCellRank").first.text_content() or "")
        team_name = text_clean(r.locator(".table__cell--participant .tableCellParticipant__name").first.text_content() or "")
        pts_txt = text_clean(r.locator(".table__cell--value").last.text_content() or "")

        try:
            rank = int(rank_txt.strip().rstrip("."))
        except:
            rank = None
        try:
            pts = int(pts_txt)
        except:
            pts = None

        if team_name:
            table[team_name] = {"rank": rank, "pts": pts}

    h = text_clean(home_name)
    a = text_clean(away_name)

    home = next((v for k, v in table.items() if (h and (h in k or k in h))), None)
    away = next((v for k, v in table.items() if (a and (a in k or k in a))), None)

    return {
        "url": url,
        "home_rank": home["rank"] if home else None,
        "home_pts": home["pts"] if home else None,
        "away_rank": away["rank"] if away else None,
        "away_pts": away["pts"] if away else None,
    }


# =========================
# ExcelÔºöÈÄêÊ¨°ËøΩË®ò
# =========================
MAX_ROWS_PER_FILE = 10
SHEET_NAME = "Sheet1"
FILE_PREFIX = "output_"
FILE_SUFFIX = ".xlsx"

def _existing_serials(output_dir: str) -> List[int]:
    p = Path(output_dir)
    nums = []
    for f in p.glob(f"{FILE_PREFIX}*{FILE_SUFFIX}"):
        m = re.match(rf"^{re.escape(FILE_PREFIX)}(\d+){re.escape(FILE_SUFFIX)}$", f.name)
        if m:
            nums.append(int(m.group(1)))
    return sorted(nums)

def _next_serial(output_dir: str) -> int:
    nums = _existing_serials(output_dir)
    return (max(nums) + 1) if nums else 1

def _current_file_path(output_dir: str) -> Path:
    p = Path(output_dir)
    nums = _existing_serials(output_dir)
    if not nums:
        return p / f"{FILE_PREFIX}{_next_serial(output_dir)}{FILE_SUFFIX}"
    return p / f"{FILE_PREFIX}{max(nums)}{FILE_SUFFIX}"

def _data_rows_in(path: Path) -> int:
    if not path.exists():
        return 0
    wb = openpyxl.load_workbook(path, read_only=True, data_only=True)
    ws = wb[SHEET_NAME] if SHEET_NAME in wb.sheetnames else wb.active
    total = ws.max_row or 0
    wb.close()
    return max(0, total - 1)

def _create_new_workbook(path: Path):
    df = pd.DataFrame(columns=HEADER)
    path.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=SHEET_NAME)
    log(f"üÜï [EXCEL] Êñ∞Ë¶è‰ΩúÊàê: {path.name}")

def append_row_to_excel(row_dict: dict, output_dir: str, max_rows_per_file: int = MAX_ROWS_PER_FILE):
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    cur = _current_file_path(output_dir)
    if not cur.exists():
        _create_new_workbook(cur)

    current_rows = _data_rows_in(cur)
    if current_rows >= max_rows_per_file:
        cur = output_dir / f"{FILE_PREFIX}{_next_serial(output_dir)}{FILE_SUFFIX}"
        _create_new_workbook(cur)
        current_rows = 0

    df = pd.DataFrame([row_dict])
    for col in HEADER:
        if col not in df.columns:
            df[col] = ""
    df = df[HEADER]

    with pd.ExcelWriter(cur, engine="openpyxl", mode="a", if_sheet_exists="overlay") as w:
        startrow = current_rows + 1
        df.to_excel(w, index=False, header=False, sheet_name=SHEET_NAME, startrow=startrow)

    log(f"üíæ [EXCEL] ËøΩË®òÂÆå‰∫Ü: {cur.name} Ôºà{current_rows} ‚Üí {current_rows+1}Ôºâ")


# =========================
# Ë¶™Ôºö„É©„Ç§„ÉñURLÂàóÊåô
# =========================
def collect_live_links_filtered() -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, slow_mo=70)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"),
            locale="ja-JP",
            timezone_id="Asia/Tokyo"
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        page = ctx.new_page()

        log("üåê Flashscore„Éà„ÉÉ„Éó„ÇíÈñã„Åç„Åæ„Åô...")
        ok = safe_goto(page, "https://www.flashscore.co.jp/", timeout_ms=45000, tag="TOP")
        if not ok:
            log("‚ùå „Éà„ÉÉ„Éó„Éö„Éº„Ç∏„ÅåÈñã„Åë„Åæ„Åõ„Çì„Åß„Åó„Åü")
            try:
                browser.close()
            except:
                pass
            return []

        kill_consent_banners(page)

        try:
            live_sel = (
                "div.filters__tab:has(div.filters__text--short:has-text('„É©„Ç§„Éñ')),"
                " div.filters__tab:has(div.filters__text--long:has-text('ÈñãÂÇ¨‰∏≠„ÅÆË©¶Âêà'))"
            )
            page.locator(live_sel).first.click(timeout=4000)
            log("‚úÖ [TOP] „É©„Ç§„Éñ„Çø„Éñ click")
        except Exception as e:
            log(f"‚ö†Ô∏è [TOP] „É©„Ç§„Éñ„Çø„Éñ click Â§±Êïó: {type(e).__name__}: {e}")

        try:
            page.wait_for_selector("div.event__match.event__match--live", timeout=20000)
        except:
            pass

        # „Ç¢„Ç≥„Éº„Éá„Ç£„Ç™„É≥Â±ïÈñãÔºà„ÅÇ„Çå„Å∞Ôºâ
        try:
            buttons = page.locator("button[data-testid='wcl-accordionButton']")
            n_btn = buttons.count()
            opened = 0
            for i in range(n_btn):
                btn = buttons.nth(i)
                aria = btn.get_attribute("aria-label") or ""
                if "ÈùûË°®Á§∫" in aria:
                    continue
                try:
                    btn.scroll_into_view_if_needed(timeout=1000)
                    btn.click(timeout=1500)
                    opened += 1
                    time.sleep(0.12)
                except:
                    pass
            if opened:
                time.sleep(0.2)
        except:
            pass

        items = page.evaluate("""
        () => {
          const results = [];
          const headers = Array.from(document.querySelectorAll("[data-testid='wcl-headerLeague']"));
          for (const h of headers) {
            const league =
              (h.querySelector("[data-testid='wcl-scores-simple-text-01']")?.textContent || "").trim() ||
              (h.querySelector("a.headerLeague__title")?.getAttribute("title") || "").trim();

            const country =
              (h.querySelector(".headerLeague__category-text")?.textContent || "").trim() ||
              (h.querySelector(".headerLeague__flag")?.getAttribute("title") || "").trim();

            const category = [country, league].filter(Boolean).join(": ").trim();

            const wrapper = h.closest(".headerLeague__wrapper");
            if (!wrapper) continue;

            let cur = wrapper.nextElementSibling;
            while (cur) {
              if (cur.querySelector?.("[data-testid='wcl-headerLeague']")) break;

              const links = cur.querySelectorAll(
                "div.event__match.event__match--live a.eventRowLink[href*='/match/'][href*='?mid=']"
              );
              for (const a of links) {
                results.push({ href: a.href, category });
              }
              cur = cur.nextElementSibling;
            }
          }
          return results;
        }
        """) or []

        log(f"üß± headerLeague„Éñ„É≠„ÉÉ„ÇØÂèñÂæó: {len(items)} ‰ª∂")

        seen_mid = set()
        for it in items:
            href = it.get("href", "") or ""
            cat  = it.get("category", "") or ""
            mid  = extract_mid(href)

            if not mid or mid in seen_mid:
                continue
            seen_mid.add(mid)

            if not cat:
                continue

            if not any(c in cat for c in CONTAINS_LIST):
                continue
            if any(x in cat for x in UNDER_LIST) or any(x in cat for x in GENDER_LIST) or any(x in cat for x in EXP_LIST):
                continue

            out.append({"url": href, "category": cat})

        log(f"üéØ „Éï„Ç£„É´„ÇøÂæåURL: {len(out)} ‰ª∂")

        try:
            browser.close()
        except:
            pass

    return out


# =========================
# WorkerÔºö1Ë©¶ÂêàÂá¶ÁêÜÔºàÂÆåÂÖ®ÁâàÔºâ
# =========================
def process_one_match_in_worker(url: str, top_category: str = "") -> Dict[str, Any]:
    result: Dict[str, Any] = {"ok": False, "url": url, "top_category": top_category}
    mid = extract_mid(url) or ""

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"),
            locale="ja-JP",
            timezone_id="Asia/Tokyo"
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        pg = ctx.new_page()

        try:
            rb = RowBuilder(HEADER, match_tag=mid)

            # Âü∫Êú¨ÊÉÖÂ†±Ôºàrow „Å´ÂÖà„Å´ÂÖ•„Çå„ÇãÔºâ
            rb.put("Ë©¶ÂêàID", mid)
            rb.put("Ë©¶Âêà„É™„É≥„ÇØÊñáÂ≠óÂàó", url)  # „Åù„ÅÆ„Åæ„Åæ URL „ÇíÂÖ•„Çå„Çã

            log("üß© [WORKER] open match page")
            ok = safe_goto(pg, url, timeout_ms=NAV_TIMEOUT_MS, tag="MATCH")
            if not ok:
                return {"ok": False, "url": url, "top_category": top_category, "error": "match_goto_failed"}

            kill_consent_banners(pg)
            if is_bot_wall(pg):
                return {"ok": False, "url": url, "top_category": top_category, "error": "bot_wall"}

            log("üîé [WORKER] read teams/scores/time")
            home, away = get_home_away_names(pg)
            hs, aw = get_scores(pg)
            ttxt = get_match_time_text(pg)

            rb.put("„Éõ„Éº„É†„ÉÅ„Éº„É†", home)
            rb.put("„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†", away)
            rb.put("„Éõ„Éº„É†„Çπ„Ç≥„Ç¢", hs)
            rb.put("„Ç¢„Ç¶„Çß„Éº„Çπ„Ç≥„Ç¢", aw)
            rb.put("Ë©¶ÂêàÊôÇÈñì", ttxt)

            # metaÔºàÂõΩ/„É™„Éº„Ç∞/ÈñãÂÇ¨Âú∞/Ë¶≥ÂÆ¢„Å™„Å©Ôºâ
            meta = get_match_meta(pg)
            meta_category = ""
            if meta.get("ÂõΩ") and meta.get("„É™„Éº„Ç∞"):
                meta_category = f"{meta['ÂõΩ']}: {meta['„É™„Éº„Ç∞']}".strip()
            final_category = meta_category or (top_category or "")

            rb.put("Ë©¶ÂêàÂõΩÂèä„Å≥„Ç´„ÉÜ„Ç¥„É™", final_category)
            rb.put("„Çπ„Ç≥„Ç¢ÊôÇÈñì", meta.get("ÂèñÂæóÊôÇÂàª", ""))  # ‰æã: 2025-12-28 18:57:49

            # ‰ª£Ë°®ÁöÑ„Å™ meta „Çí HEADER „Å∏
            # ‚Äª „ÅÇ„Å™„Åü„ÅÆ HEADER „ÅØ„Äå„Çπ„Çø„Ç∏„Ç¢„É†„Äç„ÄåÂèéÂÆπ‰∫∫Êï∞„Äç„ÄåË¶≥ÂÆ¢Êï∞„Äç„ÄåÂ†¥ÊâÄ„Äç„Å™„Å©„Åå„ÅÇ„Çã
            # meta ÂÅ¥„ÅÆ„Ç≠„Éº„ÅØ„ÄåÈñãÂÇ¨Âú∞„Äç„ÄåÂèéÂÆπ‰∫∫Êï∞„Äç„ÄåË¶≥ÂÆ¢„Äç
            if meta.get("ÈñãÂÇ¨Âú∞"):
                rb.put("„Çπ„Çø„Ç∏„Ç¢„É†", meta.get("ÈñãÂÇ¨Âú∞"))
            if meta.get("ÂèéÂÆπ‰∫∫Êï∞"):
                rb.put("ÂèéÂÆπ‰∫∫Êï∞", meta.get("ÂèéÂÆπ‰∫∫Êï∞"))
            if meta.get("Ë¶≥ÂÆ¢"):
                rb.put("Ë¶≥ÂÆ¢Êï∞", meta.get("Ë¶≥ÂÆ¢"))
            if meta.get("„É¨„Éï„Çß„É™„Éº"):
                rb.put("ÂØ©Âà§Âêç", meta.get("„É¨„Éï„Çß„É™„Éº"))

            # standingsÔºàÈ†Ü‰ΩçÔºâ
            log("üìå [WORKER] standings")
            st = get_match_standings(pg, home, away)
            if st:
                if st.get("home_rank") is not None:
                    rb.put("„Éõ„Éº„É†È†Ü‰Ωç", st.get("home_rank"))
                if st.get("away_rank") is not None:
                    rb.put("„Ç¢„Ç¶„Çß„ÉºÈ†Ü‰Ωç", st.get("away_rank"))

            # statsÔºàraw -> canonical -> rowÔºâ
            log("üìà [WORKER] scrape stats")
            stats_raw = scrape_stats_raw(pg)
            # dump_unmapped_stats(stats_raw)  # ÂøÖË¶Å„Å™„Çâ„Ç™„É≥

            canon_pairs = normalize_stats_raw_to_canon(stats_raw)

            # VERIFY
            verify_stats_mapping("CANON", canon_pairs)

            apply_stats_to_row(rb, canon_pairs)

            # ‰ªï‰∏ä„Åí
            rb.put("ÈÄöÁï™", "")  # Ë¶™„ÅßÁ¢∫ÂÆö
            rb.put("„ÇΩ„Éº„ÉàÁî®Áßí", parse_live_time_to_seconds(rb.d.get("Ë©¶ÂêàÊôÇÈñì", "")))

            verify_row_filled(rb)

            result.update({
                "ok": True,
                "mid": mid,
                "meta_category": meta_category,
                "final_category": final_category,
                "row": rb.d,
            })
            return result

        except Exception as e:
            result["error"] = f"{type(e).__name__}: {e}"
            result["trace"] = traceback.format_exc(limit=7)
            log(f"üí• [WORKER] exception: {result['error']}")
            log(result["trace"])
            return result

        finally:
            try:
                pg.close()
            except:
                pass
            try:
                browser.close()
            except:
                pass


def _worker_entry(url: str, top_category: str, q: "mp.Queue"):
    res = process_one_match_in_worker(url, top_category=top_category)
    try:
        q.put(res)
    except:
        pass

def run_match_with_timeout(url: str, top_category: str, timeout_sec: int = WORKER_TIMEOUT_SEC) -> Dict[str, Any]:
    q: mp.Queue = mp.Queue(maxsize=1)
    p = mp.Process(target=_worker_entry, args=(url, top_category, q), daemon=True)
    p.start()
    p.join(timeout=timeout_sec)

    if p.is_alive():
        log(f"üß® [TIMEOUT] Â≠ê„Éó„É≠„Çª„ÇπÂº∑Âà∂ÁµÇ‰∫Ü: {url} ({timeout_sec}s)")
        try:
            p.terminate()
        except:
            pass
        p.join(3)
        return {"ok": False, "url": url, "top_category": top_category, "error": f"timeout({timeout_sec}s)"}

    try:
        return q.get(timeout=2)
    except pyqueue.Empty:
        return {"ok": False, "url": url, "top_category": top_category, "error": "no_result_from_worker"}


# =========================
# main
# =========================
def main():
    mp.set_start_method("spawn", force=True)

    verify_header_and_stat_map()
    load_seqmap()
    os.makedirs(SAVE_DIR, exist_ok=True)

    items = collect_live_links_filtered()
    log(f"üéØ „É©„Ç§„ÉñË©¶Âêà: {len(items)} ‰ª∂")

    for i, it in enumerate(items, 1):
        url = it.get("url", "") or ""
        top_category = it.get("category", "") or ""

        log(f"\n==============================")
        log(f"[{i}/{len(items)}] {url}")
        log(f"TOP category = {top_category}")
        log(f"==============================")

        res = run_match_with_timeout(url, top_category=top_category, timeout_sec=WORKER_TIMEOUT_SEC)
        if not res.get("ok"):
            log(f"‚ö†Ô∏è [WORKER] Â§±Êïó: {res.get('error','')} url={url}")
            continue

        mid = res.get("mid", "") or ""
        row = res.get("row", {}) or {}
        final_category = (res.get("final_category", "") or "").strip()

        # Ë¶™„Åß„ÇÇÂÜç„Éï„Ç£„É´„ÇøÔºàÊúÄÁµÇ„Ç´„ÉÜ„Ç¥„É™„ÅßÂà§ÂÆöÔºâ
        if not any(c in final_category for c in CONTAINS_LIST):
            log(f"‚è≠Ô∏è „Çπ„Ç≠„ÉÉ„ÉóÔºà„É™„Çπ„ÉàÂ§ñÔºâ: {final_category}")
            continue
        if any(x in final_category for x in UNDER_LIST) or any(x in final_category for x in GENDER_LIST) or any(x in final_category for x in EXP_LIST):
            log(f"üö´ Èô§Â§ñ: {final_category}")
            continue

        # ÈÄöÁï™Á¢∫ÂÆö
        last_seq = int(SEQMAP.get(mid, 0)) if mid else 0
        seq = last_seq + 1
        if mid:
            SEQMAP[mid] = seq

        row["ÈÄöÁï™"] = seq
        row["Ë©¶ÂêàID"] = mid

        append_row_to_excel(row, SAVE_DIR)

    save_seqmap()

if __name__ == "__main__":
    main()
