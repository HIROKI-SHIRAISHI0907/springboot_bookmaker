# -*- coding: utf-8 -*-
from playwright.sync_api import sync_playwright
import time
import re, os
import datetime
from typing import List, Dict, Optional
import pandas as pd
from pathlib import Path
import glob
import sys
try:
    import openpyxl
except ImportError:
    raise RuntimeError("openpyxl ãŒå¿…è¦ã§ã™ã€‚`pip install openpyxl` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


# ============== å–å¾—æ¡ä»¶ ====================

HEADER_SCHEDULED = [
    "è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª", "è©¦åˆäºˆå®šæ™‚é–“", "ãƒ›ãƒ¼ãƒ é †ä½", "ã‚¢ã‚¦ã‚§ãƒ¼é †ä½", "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹", "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹", "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹",
    "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹", "è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—", "ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚é–“"
]

# ===== Excel å‡ºåŠ›é–¢é€£ =====
# bmData.py ã¨åŒæ§˜ã® outputs ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æƒ³å®š
SAVE_DIR_SCHEDULED = "/Users/shiraishitoshio/bookmaker/future"

# ===== Excel é€æ¬¡æ›¸ãè¾¼ã¿ï¼ˆé–‹å‚¬äºˆå®šï¼‰ =====
# 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®æœ€å¤§ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ˜ãƒƒãƒ€é™¤ãï¼‰
MAX_ROWS_PER_FILE_SCHEDULED = 10      # bmData.py ã¨åŒã˜å€¤ã€‚å¿…è¦ãªã‚‰å¾Œã§å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚
SHEET_NAME_SCHEDULED = "Sheet1"
FILE_PREFIX_SCHEDULED = "future_"
FILE_SUFFIX_SCHEDULED = ".xlsx"

def _existing_serials_scheduled(output_dir: str) -> List[int]:
    p = Path(output_dir)
    nums = []
    for f in p.glob(f"{FILE_PREFIX_SCHEDULED}*{FILE_SUFFIX_SCHEDULED}"):
        m = re.match(rf"^{re.escape(FILE_PREFIX_SCHEDULED)}(\d+){re.escape(FILE_SUFFIX_SCHEDULED)}$", f.name)
        if m:
            nums.append(int(m.group(1)))
    return sorted(nums)

def _next_serial_scheduled(output_dir: str) -> int:
    """æ—¢å­˜ã®æœ€å¤§é€£ç•ª+1 ã‚’è¿”ã™"""
    nums = _existing_serials_scheduled(output_dir)
    return (max(nums) + 1) if nums else 1

def _current_file_path_scheduled(output_dir: str) -> Path:
    """ä»Šä½¿ã†ã¹ã future_*.xlsx ã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚ç„¡ã‘ã‚Œã°æ–°è¦ï¼ˆé€£ç•ªï¼‰ã€‚"""
    p = Path(output_dir)
    nums = _existing_serials_scheduled(output_dir)
    if not nums:
        return p / f"{FILE_PREFIX_SCHEDULED}{_next_serial_scheduled(output_dir)}{FILE_SUFFIX_SCHEDULED}"
    # ç›´è¿‘ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€å¤§é€£ç•ªï¼‰
    return p / f"{FILE_PREFIX_SCHEDULED}{max(nums)}{FILE_SUFFIX_SCHEDULED}"

def _data_rows_in_scheduled(path: Path) -> int:
    """æ—¢å­˜Excelã®ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ˜ãƒƒãƒ€é™¤ãï¼‰ã‚’è¿”ã™ã€‚ç„¡ã‘ã‚Œã°0ã€‚"""
    if not path.exists():
        return 0
    wb = openpyxl.load_workbook(path, read_only=True, data_only=True)
    ws = wb[SHEET_NAME_SCHEDULED] if SHEET_NAME_SCHEDULED in wb.sheetnames else wb.active
    total = ws.max_row or 0
    wb.close()
    return max(0, total - 1)  # ãƒ˜ãƒƒãƒ€1è¡Œã‚’é™¤å¤–

def _create_new_workbook_scheduled(path: Path):
    """HEADER_SCHEDULED ä»˜ãã§æ–°è¦ future_*.xlsx ã‚’ä½œæˆ"""
    df = pd.DataFrame(columns=HEADER_SCHEDULED)
    path.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=SHEET_NAME_SCHEDULED)

def append_scheduled_row_to_excel(
    row_dict: Dict[str, str],
    output_dir: str = SAVE_DIR_SCHEDULED,
    max_rows_per_file: int = MAX_ROWS_PER_FILE_SCHEDULED
):
    """
    1 è©¦åˆåˆ†ã®è¾æ›¸ row_dict ã‚’
    - future_N.xlsx ã®æœ«å°¾ã«è¿½è¨˜
    - N ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ï¼ˆmax_rows_per_file è¡Œã”ã¨ï¼‰ã«æ–°ã—ã„ future_(N+1).xlsx ã‚’ä½œã‚‹
    ã¨ã„ã†å½¢ã§ä¿å­˜ã™ã‚‹ã€‚
    """
    output_dir_path = Path(output_dir)
    output_dir_path.mkdir(parents=True, exist_ok=True)

    cur = _current_file_path_scheduled(output_dir_path)
    if not cur.exists():
        _create_new_workbook_scheduled(cur)

    # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆãƒ˜ãƒƒãƒ€é™¤ãï¼‰
    current_rows = _data_rows_in_scheduled(cur)

    # ä¸Šé™ã‚’è¶…ãˆãŸã‚‰æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    if current_rows >= max_rows_per_file:
        cur = output_dir_path / f"{FILE_PREFIX_SCHEDULED}{_next_serial_scheduled(output_dir)}{FILE_SUFFIX_SCHEDULED}"
        _create_new_workbook_scheduled(cur)
        current_rows = 0

    # è¿½è¨˜ç”¨ DF ã‚’ HEADER_SCHEDULED é †ã«æ•´å½¢
    df = pd.DataFrame([row_dict])
    for col in HEADER_SCHEDULED:
        if col not in df.columns:
            df[col] = ""
    df = df[HEADER_SCHEDULED]

    with pd.ExcelWriter(cur, engine="openpyxl", mode="a", if_sheet_exists="overlay") as w:
        startrow = current_rows + 1  # ãƒ˜ãƒƒãƒ€1è¡Œã‚ã‚Š
        df.to_excel(w, index=False, header=False, sheet_name=SHEET_NAME_SCHEDULED, startrow=startrow)

    print(f"ğŸ’¾ [FUTURE] è¿½è¨˜å®Œäº†: {cur.name} ï¼ˆãƒ‡ãƒ¼ã‚¿è¡Œ {current_rows} â†’ {current_rows+1} ä»¶ç›®ã‚’è¿½åŠ ï¼‰")

def save_scheduled_to_excel(match_results: List[Dict[str, str]], output_dir: str = SAVE_DIR_SCHEDULED):
    """
    é–‹å‚¬äºˆå®šãƒ‡ãƒ¼ã‚¿ï¼ˆmatch_resultsï¼‰ã‚’ Excel ã«ä¿å­˜ã™ã‚‹ã€‚
    bmData.py ã® append_row_to_excel ã¨åŒã˜è€ƒãˆæ–¹ã§ã€
    future_N.xlsx ã«é€æ¬¡è¿½è¨˜ã—ã€ä¸€å®šä»¶æ•°ã§æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã€‚
    """
    if not match_results:
        log("âœ‹ Excel ã«æ›¸ãè¾¼ã‚€é–‹å‚¬äºˆå®šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆmatch_results ãŒç©ºï¼‰")
        return

    os.makedirs(output_dir, exist_ok=True)

    # åˆ—ã”ã¨ã®éç©ºä»¶æ•°ã‚’ã–ã£ãã‚Šãƒ­ã‚°ï¼ˆã¾ã¨ã‚ã¦ï¼‰
    try:
        df_tmp = pd.DataFrame(match_results)
        for col in HEADER_SCHEDULED:
            if col not in df_tmp.columns:
                df_tmp[col] = ""
        df_tmp = df_tmp[HEADER_SCHEDULED]

        non_empty_counts = df_tmp.apply(lambda s: s.astype(str).str.strip().ne("").sum())
        log("ğŸ“„ [EXCEL-SCHEDULED] åˆ—ã”ã¨ã®éç©ºä»¶æ•°ï¼ˆä¸Šä½10åˆ—ï¼‰:")
        top10 = non_empty_counts.sort_values(ascending=False).head(10)
        for col, cnt in top10.items():
            log(f"   - {col}: {cnt}")
        log(f"ğŸ“„ [EXCEL-SCHEDULED] ç·è¡Œæ•°(ä»Šå›è¿½åŠ åˆ†): {len(df_tmp)} / ç·åˆ—æ•°: {len(df_tmp.columns)}")
    except Exception as e:
        log(f"âš ï¸ [EXCEL-SCHEDULED] éç©ºä»¶æ•°è¨ˆç®—ã§ä¾‹å¤–: {e}")

# ==========================================
# âœ… å¯¾è±¡ãƒªãƒ¼ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ãƒªã‚¹ãƒˆ
# ==========================================
CONTAINS_LIST = [
    "ã‚±ãƒ‹ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚³ãƒ­ãƒ³ãƒ“ã‚¢: ãƒ—ãƒªãƒ¡ãƒ¼ãƒ© A", "ã‚¿ãƒ³ã‚¶ãƒ‹ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°",
    "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: EFL ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚·ãƒƒãƒ—", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: EFL ãƒªãƒ¼ã‚° 1", "ã‚¨ãƒã‚ªãƒ”ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚³ã‚¹ã‚¿ãƒªã‚«: ãƒªãƒ¼ã‚¬ FPD",
    "ã‚¸ãƒ£ãƒã‚¤ã‚«: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚¹ãƒšã‚¤ãƒ³: ãƒ©ãƒ»ãƒªãƒ¼ã‚¬", "ãƒ–ãƒ©ã‚¸ãƒ«: ã‚»ãƒªã‚¨ A ãƒ™ã‚¿ãƒ¼ãƒ", "ãƒ–ãƒ©ã‚¸ãƒ«: ã‚»ãƒªã‚¨ B", "ãƒ‰ã‚¤ãƒ„: ãƒ–ãƒ³ãƒ‡ã‚¹ãƒªãƒ¼ã‚¬",
    "éŸ“å›½: K ãƒªãƒ¼ã‚° 1", "ä¸­å›½: ä¸­å›½ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J1 ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J2 ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J3 ãƒªãƒ¼ã‚°", "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢: ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ¼ã‚°",
    "ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢: A ãƒªãƒ¼ã‚°ãƒ»ãƒ¡ãƒ³", "ãƒãƒ¥ãƒ‹ã‚¸ã‚¢: ãƒãƒ¥ãƒ‹ã‚¸ã‚¢ï½¥ãƒ—ãƒ­ãƒªãƒ¼ã‚°", "ã‚¦ã‚¬ãƒ³ãƒ€: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ãƒ¡ã‚­ã‚·ã‚³: ãƒªãƒ¼ã‚¬ MX",
    "ãƒ•ãƒ©ãƒ³ã‚¹: ãƒªãƒ¼ã‚°ãƒ»ã‚¢ãƒ³", "ã‚¹ã‚³ãƒƒãƒˆãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ã‚·ãƒƒãƒ—", "ã‚ªãƒ©ãƒ³ãƒ€: ã‚¨ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ“ã‚¸", "ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³: ãƒˆãƒ«ãƒã‚ªãƒ»ãƒ™ã‚¿ãƒ¼ãƒ",
    "ã‚¤ã‚¿ãƒªã‚¢: ã‚»ãƒªã‚¨ A", "ã‚¤ã‚¿ãƒªã‚¢: ã‚»ãƒªã‚¨ B", "ãƒãƒ«ãƒˆã‚¬ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ«", "ãƒˆãƒ«ã‚³: ã‚¹ãƒ¥ãƒšãƒ«ãƒ»ãƒªã‚°", "ã‚»ãƒ«ãƒ“ã‚¢: ã‚¹ãƒ¼ãƒšãƒ«ãƒªãƒ¼ã‚¬",
    "æ—¥æœ¬: WEãƒªãƒ¼ã‚°", "ãƒœãƒªãƒ“ã‚¢: LFPB", "ãƒ–ãƒ«ã‚¬ãƒªã‚¢: ãƒ‘ãƒ«ãƒ´ã‚¡ãƒ»ãƒªãƒ¼ã‚¬", "ã‚«ãƒ¡ãƒ«ãƒ¼ãƒ³: ã‚¨ãƒªãƒ¼ãƒˆ 1", "ãƒšãƒ«ãƒ¼: ãƒªãƒ¼ã‚¬ 1",
    "ã‚¨ã‚¹ãƒˆãƒ‹ã‚¢: ãƒ¡ã‚¹ã‚¿ãƒªãƒªãƒ¼ã‚¬", "ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠ: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ãƒ™ãƒ«ã‚®ãƒ¼: ã‚¸ãƒ¥ãƒ”ãƒ©ãƒ¼ï½¥ãƒ—ãƒ­ãƒªãƒ¼ã‚°", "ã‚¨ã‚¯ã‚¢ãƒ‰ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒ—ãƒ­",
    "æ—¥æœ¬: YBC ãƒ«ãƒ´ã‚¡ãƒ³ã‚«ãƒƒãƒ—", "æ—¥æœ¬: å¤©çš‡æ¯"
]
UNDER_LIST  = ["U17", "U18", "U19", "U20", "U21", "U22", "U23", "U24", "U25"]
GENDER_LIST = ["å¥³å­"]
EXP_LIST    = ["ãƒãƒ«ãƒˆã‚¬ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ« 2", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° 2", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° U18"]

# ============== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==============

VERBOSE = True

def log(msg: str):
    if VERBOSE:
        print(msg)

def text_clean(s: str) -> str:
    import re
    return re.sub(r"\s+", " ", (s or "")).strip()

def extract_mid(s: str) -> Optional[str]:
    """URLã‹ã‚‰ mid ã‚’æŠ½å‡º"""
    if not s:
        return None
    s = str(s).strip()
    m = re.search(r"[?&#]mid=([A-Za-z0-9]+)", s)
    if m:
        return m.group(1)
    m = re.search(r"/match/([A-Za-z0-9]{6,20})(?:/|$)", s)
    if m:
        return m.group(1)
    return None

# ============== Flashscoreã€Œé–‹å‚¬äºˆå®šã€ãƒŠãƒ“ ==============

def goto_football_top(page):
    """ã‚µãƒƒã‚«ãƒ¼ â†’ é–‹å‚¬äºˆå®šã‚¿ãƒ–ã¸é·ç§»"""
    log("ğŸŒ Flashscore ãƒˆãƒƒãƒ—ã¸ã‚¢ã‚¯ã‚»ã‚¹...")
    page.goto("https://www.flashscore.co.jp/", timeout=45000, wait_until="domcontentloaded")

    # CookieãƒãƒŠãƒ¼ãªã©
    try:
        page.locator("#onetrust-accept-btn-handler").click(timeout=2000)
        log("âœ… CookieãƒãƒŠãƒ¼ã‚’é–‰ã˜ã¾ã—ãŸ")
    except:
        pass

    # ã‚µãƒƒã‚«ãƒ¼ãŒé¸ã°ã‚Œã¦ã„ãªã„å ´åˆã«å‚™ãˆã¦ã€Œã‚µãƒƒã‚«ãƒ¼ã€ã‚¯ãƒªãƒƒã‚¯ï¼ˆä¿é™ºï¼‰
    try:
        soccer_btn = page.locator("a,button").filter(has_text="ã‚µãƒƒã‚«ãƒ¼").first
        if soccer_btn and soccer_btn.count():
            soccer_btn.click(timeout=4000)
            time.sleep(0.8)
    except:
        pass

    # ã€Œé–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
    try:
        tab = page.locator("div.filters__tab[data-analytics-alias='scheduled']").first
        if tab and tab.count():
            tab.click(timeout=4000)
        else:
            tab = page.locator("div.filters__tab").filter(
                has_text=re.compile(r"(é–‹å‚¬äºˆå®š)")
            ).first
            tab.click(timeout=4000)
        log("âœ… ã€é–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
    except Exception as e:
        log(f"âš ï¸ é–‹å‚¬äºˆå®šã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆå¤±æ•—: {e}")

    # è©¦åˆè¡ŒãŒæç”»ã•ã‚Œã‚‹ã¾ã§å°‘ã—å¾…ã¤
    try:
        page.wait_for_timeout(1000)
        page.wait_for_load_state("networkidle", timeout=8000)
    except:
        pass

def expand_all_collapsed_leagues(page):
    """
    Flashscoreã€é–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã§ã€
    æŠ˜ã‚ŠãŸãŸã¾ã‚Œã¦ã„ã‚‹ãƒªãƒ¼ã‚°ï¼ˆï¼ãƒªãƒ¼ã‚°å…¨è©¦åˆãŒéè¡¨ç¤ºï¼‰ã‚’ã™ã¹ã¦å±•é–‹ã™ã‚‹ã€‚
    """
    print("ğŸ“‚ æŠ˜ã‚ŠãŸãŸã¿ãƒªãƒ¼ã‚°ï¼ˆéè¡¨ç¤ºï¼‰ã‚’å±•é–‹ã—ã¾ã™...")

    btn_selector = (
        "button[data-testid='wcl-accordionButton']"
        "[aria-label='ãƒªãƒ¼ã‚°å…¨è©¦åˆ è¡¨ç¤º']"
    )

    max_loops = 200  # å¿µã®ãŸã‚å®‰å…¨å¼

    for _ in range(max_loops):
        btns = page.locator(btn_selector)
        count = btns.count()

        if count == 0:
            print("   âœ… ã™ã¹ã¦ã®æŠ˜ã‚ŠãŸãŸã¿ãƒªãƒ¼ã‚°ã‚’å±•é–‹ã—ã¾ã—ãŸ")
            return

        print(f"   æ®‹ã‚Šã€è¡¨ç¤ºã€ãƒœã‚¿ãƒ³æ•°: {count}")

        btn = btns.first

        try:
            btn.scroll_into_view_if_needed()
        except Exception:
            pass

        try:
            btn.click(timeout=2000)
        except Exception as e:
            print(f"   âš ï¸ ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
            break

        page.wait_for_timeout(200)

    print("   âš ï¸ ãƒ«ãƒ¼ãƒ—ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã¾ã éè¡¨ç¤ºãƒªãƒ¼ã‚°ãŒæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

# ============== è©¦åˆè¡Œã®åŸºæœ¬æƒ…å ±å–å¾— ==============

def _get_match_row_teams_and_time(row):
    """
    1ã¤ã®è©¦åˆè¡Œã‹ã‚‰
      - è©¦åˆäºˆå®šæ™‚é–“
      - ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ å
      - ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ å
    ã‚’æŠ½å‡ºï¼ˆæ—§UI / æ–°UI ä¸¡å¯¾å¿œï¼‰
    """
    # ===== æ™‚é–“ =====
    ktime = ""

    # æ—§UI: .event__time
    try:
        ktime = text_clean(row.locator(".event__time").first.text_content() or "")
    except Exception:
        pass

    # æ–°UI: data-testid ãƒ™ãƒ¼ã‚¹
    if not ktime:
        try:
            ktime = text_clean(
                row.locator(
                    "[data-testid='wcl-time'], "
                    "[data-testid='wcl-start-time'], "
                    "[data-testid='wcl-time-status']"
                ).first.text_content() or ""
            )
        except Exception:
            pass

    # ===== ãƒãƒ¼ãƒ å =====
    home = ""
    away = ""

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘ : æ—§UI (.event__participant--home / --away)
    try:
        h = row.locator(".event__participant--home .event__participant--name").first
        a = row.locator(".event__participant--away .event__participant--name").first
        if h.count():
            home = text_clean(h.text_content() or "")
        if a.count():
            away = text_clean(a.text_content() or "")
    except Exception:
        pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘¡: æ—§UI ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (.event__participant)
    if not home or not away:
        try:
            ps = row.locator(".event__participant .event__participant--name")
            if ps.count() >= 2:
                if not home:
                    home = text_clean(ps.first.text_content() or "")
                if not away:
                    away = text_clean(ps.last.text_content() or "")
        except Exception:
            pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘¢: æ–°UI æ˜ç¤ºçš„ã‚»ãƒ¬ã‚¯ã‚¿
    if not home or not away:
        try:
            h = row.locator(
                ".event__homeParticipant span.wcl-name_jjfMf, "
                ".event__homeParticipant [data-testid='wcl-scores-simple-text-01']"
            ).first
            a = row.locator(
                ".event__awayParticipant span.wcl-name_jjfMf, "
                ".event__awayParticipant [data-testid='wcl-scores-simple-text-01']"
            ).first

            if h and h.count():
                if not home:
                    home = text_clean(h.text_content() or "")
            if a and a.count():
                if not away:
                    away = text_clean(a.text_content() or "")
        except Exception:
            pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘£: æ–°UI ã‚†ã‚‹ã‚
    if not home or not away:
        try:
            ps = row.locator(
                "[data-testid='wcl-matchRow-participant'] span.wcl-name_jjfMf, "
                "[data-testid='wcl-matchRow-participant'] [data-testid='wcl-scores-simple-text-01']"
            )
            n = ps.count()
            if n >= 2:
                if not home:
                    home = text_clean(ps.nth(0).text_content() or "")
                if not away:
                    away = text_clean(ps.nth(n - 1).text_content() or "")
        except Exception:
            pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘¤: <img alt="ãƒãƒ¼ãƒ å">
    if not home or not away:
        try:
            imgs = row.locator("[data-testid='wcl-matchRow-participant'] img[data-testid='wcl-participantLogo']")
            n_img = imgs.count()
            if n_img >= 2:
                if not home:
                    alt0 = imgs.nth(0).get_attribute("alt") or ""
                    home = text_clean(alt0)
                if not away:
                    alt1 = imgs.nth(n_img - 1).get_attribute("alt") or ""
                    away = text_clean(alt1)
        except Exception:
            pass

    if not home or not away:
        try:
            snippet = (row.inner_text() or "").strip().replace("\n", " ")[:200]
        except Exception:
            snippet = "<inner_text å–å¾—å¤±æ•—>"
        print(f"âš ï¸ ãƒãƒ¼ãƒ åå–å¾—å¤±æ•—: time={ktime}, snippet={snippet}")

    return ktime, home, away

def _get_match_row_link(row) -> str:
    """è©¦åˆè¡Œã‹ã‚‰ matchURL ã‚’æŠ½å‡º"""
    try:
        a = row.locator("a.eventRowLink[href*='/match/'][href*='?mid=']").first
        if a and a.count():
            href = a.get_attribute("href") or ""
            if href.startswith("http"):
                return href
            return "https://www.flashscore.co.jp" + href
    except:
        pass
    return ""

# ============== æ—¥ä»˜ ==============

def get_current_match_date(page) -> Optional[datetime.date]:
    """
    Flashscore ãƒˆãƒƒãƒ—ã®æ—¥ä»˜ãƒœã‚¿ãƒ³ï¼ˆä¾‹: '05/12 é‡‘'ï¼‰ã‹ã‚‰
    datetime.date ã‚’ä½œã£ã¦è¿”ã™ã€‚
    ãƒœã‚¿ãƒ³ãŒå–ã‚Œãªã‹ã£ãŸã‚Šãƒ‘ãƒ¼ã‚¹ã§ããªã‘ã‚Œã° None ã‚’è¿”ã™ã€‚
    """
    try:
        btn = page.locator("button[data-testid='wcl-dayPickerButton']").first
        if not btn or not btn.count():
            return None

        txt = text_clean(btn.inner_text() or "")
        # ä¾‹: "05/12 é‡‘" â†’ day=05, month=12
        m = re.search(r"(\d{2})/(\d{2})", txt)
        if not m:
            return None

        day = int(m.group(1))
        month = int(m.group(2))

        # å¹´ã¯ã¨ã‚Šã‚ãˆãšä»Šå¹´ã‚’æ¡ç”¨ï¼ˆå¹´ã¾ãŸãã¯ç°¡æ˜“å¯¾å¿œï¼‰
        year = datetime.datetime.now().year
        return datetime.date(year, month, day)

    except Exception:
        return None

# ============== é–‹å‚¬äºˆå®šã‚¿ãƒ–ã‹ã‚‰åŸºæœ¬æƒ…å ±ã ã‘é›†ã‚ã‚‹ ==============

def collect_scheduled_matches_on_current_day(page) -> List[Dict[str, str]]:
    """
    ç¾åœ¨è¡¨ç¤ºä¸­ã®æ—¥ä»˜ï¼ˆé–‹å‚¬äºˆå®šã‚¿ãƒ–ï¼‰ã‹ã‚‰è©¦åˆæƒ…å ±ã‚’åé›†ã€‚
    ã“ã“ã§ã¯ã€Œæ™‚é–“ãƒ»ãƒ›ãƒ¼ãƒ ãƒ»ã‚¢ã‚¦ã‚§ãƒ¼ãƒ»ãƒªãƒ³ã‚¯ã€ã ã‘ã‚’é›†ã‚ã€
    å›½ãƒªãƒ¼ã‚°ï¼†é †ä½ã¯å¾Œã§è©¦åˆãƒšãƒ¼ã‚¸ã‹ã‚‰åŸ‹ã‚ã‚‹ã€‚
    """
    try:
        page.wait_for_selector("div.event__match", timeout=12000)
    except:
        log("âš ï¸ event__match ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¾ã¾ç¶šè¡Œ")

    expand_all_collapsed_leagues(page)

    # æ—¥ä»˜å–å¾—
    match_date = get_current_match_date(page)

    rows = page.locator("div.event__match.event__match--scheduled")
    if rows.count() == 0:
        rows = page.locator("div.event__match")
    n = rows.count()
    log(f"ğŸ¯ é–‹å‚¬äºˆå®šè©¦åˆ è¡Œæ•°: {n}")

    results: List[Dict[str, str]] = []
    seen_mids = set()

    now_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for i in range(n):
        row = rows.nth(i)
        try:
            ktime, home, away = _get_match_row_teams_and_time(row)
            link = _get_match_row_link(row)

            # ğŸ”¹ æ—¥ä»˜ï¼‹æ™‚é–“ã®æ–‡å­—åˆ—ã‚’ä½œæˆ
            if match_date and ktime:
                # ä¾‹: "2025-12-05 04:00"
                match_dt_str = f"{match_date.strftime('%Y-%m-%d')} {ktime}"
            else:
                match_dt_str = ktime  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

            log(f"row:  {match_dt_str}, {home}, {away}, {link}")

            mid = extract_mid(link)
            if mid and mid in seen_mids:
                log(f"   â­ï¸ é‡è¤‡è©¦åˆ(mid={mid})ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                continue
            if mid:
                seen_mids.add(mid)

            d = {k: "" for k in HEADER_SCHEDULED}
            d["è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª"] = ""  # â† å¾Œã§åŸ‹ã‚ã‚‹
            d["è©¦åˆäºˆå®šæ™‚é–“"]       = match_dt_str
            d["ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ "]       = home
            d["ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ "]     = away
            d["è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—"]   = link
            d["ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚é–“"]     = now_str

            results.append(d)

            log(f"   [{i+1}/{n}] | {ktime} | {home} vs {away} | mid={mid}")
        except Exception as e:
            log(f"   âš ï¸ è¡Œ{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    log(f"âœ… å½“æ—¥åˆ† å–å¾—ä»¶æ•°: {len(results)}")
    return results

def click_next_day(page) -> bool:
    """
    ã€Œç¿Œæ—¥ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æ—¥ä»˜ã‚’1æ—¥é€²ã‚ã‚‹ã€‚
    æˆåŠŸã—ãŸã‚‰ True, è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Falseã€‚
    """
    try:
        btn = page.locator("button.wcl-arrow_YpdN4[data-day-picker-arrow='next']").first
        if not btn or not btn.count():
            log("âš ï¸ ç¿Œæ—¥ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return False
        btn.click(timeout=3000)
        log("â¡ï¸ ã€ç¿Œæ—¥ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã—ãŸ")
        time.sleep(1.0)
        try:
            page.wait_for_load_state("networkidle", timeout=8000)
        except:
            pass
        return True
    except Exception as e:
        log(f"âš ï¸ ç¿Œæ—¥ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
        return False

# ============== é †ä½è¡¨ & å›½ãƒªãƒ¼ã‚°å–å¾— ==============

def build_standings_url_from_match_url(match_url: str) -> str:
    """
    è©¦åˆãƒšãƒ¼ã‚¸URLã‚’é †ä½è¡¨ã‚¿ãƒ–ã®URLã«å¤‰æ›ã™ã‚‹ã€‚
    """
    if not match_url:
        return ""
    if "/standings/" in match_url:
        return match_url
    return re.sub(r"/(\?mid=)", r"/standings/\1", match_url, count=1)

def get_team_ranks_from_standings_table(page, home_name: str, away_name: str):
    """
    ã™ã§ã«ã€Œé †ä½è¡¨ã€ã‚¿ãƒ–ï¼ˆã‚ªãƒ¼ãƒãƒ¼ã‚ªãƒ¼ãƒ«ï¼‰ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ page ã‹ã‚‰ã€
    ãƒ›ãƒ¼ãƒ ï¼†ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã®é †ä½ï¼ˆrankï¼‰ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    home_name_norm = text_clean(home_name)
    away_name_norm = text_clean(away_name)

    home_rank = ""
    away_rank = ""

    rows = page.locator("div.ui-table__body > div.ui-table__row")
    n_rows = rows.count()

    for i in range(n_rows):
        row = rows.nth(i)
        try:
            team_elem = row.locator(".tableCellParticipant__name").first
            if not team_elem.count():
                continue
            team_name = text_clean(team_elem.text_content() or "")

            rank_elem = row.locator(".tableCellRank").first
            if not rank_elem.count():
                continue
            rank_text = text_clean(rank_elem.text_content() or "")
            rank_text = rank_text.rstrip(".").strip()

            if not home_rank and team_name == home_name_norm:
                home_rank = rank_text
            if not away_rank and team_name == away_name_norm:
                away_rank = rank_text

            if home_rank and away_rank:
                break
        except:
            continue

    return home_rank, away_rank

def get_country_and_league_from_match_page(page):
    """
    è©¦åˆãƒšãƒ¼ã‚¸ï¼ˆã‚µãƒãƒªãƒ¼ / é †ä½è¡¨ã‚¿ãƒ–ï¼‰ã®ãƒ‘ãƒ³ããšã‹ã‚‰
    - å›½å
    - ãƒªãƒ¼ã‚°åï¼ˆï¼‹ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
    ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    country = ""
    league = ""

    try:
        try:
            page.wait_for_selector(
                "nav[data-testid='wcl-breadcrumbs'] span[data-testid='wcl-scores-overline-03']",
                timeout=3000
            )
        except Exception:
            pass

        spans = page.locator(
            "nav[data-testid='wcl-breadcrumbs'] span[data-testid='wcl-scores-overline-03']"
        )
        count = spans.count()

        if count == 0:
            return "", ""

        texts = []
        for i in range(count):
            txt = text_clean(spans.nth(i).text_content() or "")
            if txt:
                texts.append(txt)

        # æœŸå¾…ãƒ‘ã‚¿ãƒ¼ãƒ³:
        #   0: ã‚µãƒƒã‚«ãƒ¼
        #   1: ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰ï¼ˆå›½ï¼‰
        #   2: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° - ãƒ©ã‚¦ãƒ³ãƒ‰ 14ï¼ˆãƒªãƒ¼ã‚°ï¼‰
        start_idx = 0
        if texts and texts[0] == "ã‚µãƒƒã‚«ãƒ¼":
            start_idx = 1

        if len(texts) > start_idx:
            country = texts[start_idx]
        if len(texts) > start_idx + 1:
            league = texts[start_idx + 1]

    except Exception:
        pass

    return country, league

def fetch_ranks_for_match(page, match_url: str, home_name: str, away_name: str):
    """
    è©¦åˆURLã‹ã‚‰é †ä½è¡¨ã‚¿ãƒ–ã«é£›ã³ã€
      - ãƒ›ãƒ¼ãƒ é †ä½
      - ã‚¢ã‚¦ã‚§ãƒ¼é †ä½
      - å›½å
      - ãƒªãƒ¼ã‚°åï¼ˆãƒ©ã‚¦ãƒ³ãƒ‰ä»˜ãï¼‰
    ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    if not match_url:
        return "", "", "", ""

    standings_url = build_standings_url_from_match_url(match_url)

    try:
        log(f"   ğŸ“Š é †ä½è¡¨å–å¾—: {standings_url}")
        page.goto(standings_url, timeout=25000, wait_until="domcontentloaded")

        try:
            standings_tab = page.locator(
                "a[data-analytics-alias='stats-detail'] button, "
                "a[href*='/standings/'] button"
            ).first
            if standings_tab and standings_tab.count():
                selected = standings_tab.get_attribute("data-selected")
                if selected != "true":
                    standings_tab.click(timeout=3000)
                    page.wait_for_timeout(500)
        except:
            pass

        country, league = get_country_and_league_from_match_page(page)

        page.wait_for_selector("div.ui-table__body div.ui-table__row", timeout=12000)

        home_rank, away_rank = get_team_ranks_from_standings_table(page, home_name, away_name)
        log(f"      â†’ rank: home={home_rank}, away={away_rank}, country={country}, league={league}")
        return home_rank, away_rank, country, league

    except Exception as e:
        log(f"   âš ï¸ é †ä½è¡¨å–å¾—å¤±æ•—: {e}")
        return "", "", "", ""

# ============== å…¨è©¦åˆã«å¯¾ã—ã¦ å›½ãƒªãƒ¼ã‚° & é †ä½ & ãƒ•ã‚£ãƒ«ã‚¿ ==============

def fill_ranks_for_matches(ctx, matches: List[Dict[str, str]]):
    """
    ã€Œé–‹å‚¬äºˆå®šã€ã§åé›†ã—ãŸè©¦åˆãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ã€
    å„è©¦åˆãƒšãƒ¼ã‚¸ã®é †ä½è¡¨ã‹ã‚‰
      - ãƒ›ãƒ¼ãƒ é †ä½
      - ã‚¢ã‚¦ã‚§ãƒ¼é †ä½
      - è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒªï¼ˆå›½: ãƒªãƒ¼ã‚°ï¼‰
    ã‚’åŸ‹ã‚ã€ã‹ã¤å¯¾è±¡ãƒªãƒ¼ã‚°ã ã‘æ®‹ã™ã€‚
    """
    if not matches:
        return

    page = ctx.new_page()

    filtered: List[Dict[str, str]] = []

    for idx, m in enumerate(matches):
        url  = m.get("è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—") or ""
        home = m.get("ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ") or ""
        away = m.get("ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ") or ""

        if not url or not home or not away:
            log(f"â­ï¸ URL/ãƒãƒ¼ãƒ åä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {home} vs {away}")
            continue

        log(f"=== é †ä½å–å¾— {idx+1}/{len(matches)} ===")
        home_rank, away_rank, country, league = fetch_ranks_for_match(page, url, home, away)

        # å›½ï¼‹ãƒªãƒ¼ã‚°ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæ–‡å­—åˆ—ã‚’çµ„ã¿ç«‹ã¦
        game_category = ""
        if country and league:
            game_category = f"{country}: {league}"
        elif country or league:
            game_category = country or league

        if not game_category:
            log("â­ï¸ ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡: ï¼ˆã‚«ãƒ†ã‚´ãƒªå–å¾—å¤±æ•—ï¼‰")
            continue

        # 1) CONTAINS_LIST ã«å«ã¾ã‚Œã‚‹ãƒªãƒ¼ã‚°ã ã‘å¯¾è±¡
        if not any(c in game_category for c in CONTAINS_LIST):
            log(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡: {game_category}ï¼ˆãƒªã‚¹ãƒˆå¤–ï¼‰")
            continue

        # 2) å¹´ä»£ï¼ˆUxxï¼‰ãƒ»å¥³å­ãƒ»ä¾‹å¤–ãƒªãƒ¼ã‚°ã‚’å«ã‚€å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if (any(x in game_category for x in UNDER_LIST) or
            any(x in game_category for x in GENDER_LIST) or
            any(x in game_category for x in EXP_LIST)):
            log(f"ğŸš« é™¤å¤–å¯¾è±¡: {game_category}")
            continue

        # ã“ã“ã¾ã§æ¥ãŸè©¦åˆã ã‘ã‚’æ¡ç”¨
        if home_rank:
            m["ãƒ›ãƒ¼ãƒ é †ä½"] = home_rank
        if away_rank:
            m["ã‚¢ã‚¦ã‚§ãƒ¼é †ä½"] = away_rank
        m["è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª"] = game_category

        # ğŸ”¹ ã“ã“ã§å³ Excel ã«1è¡Œè¿½è¨˜
        append_scheduled_row_to_excel(m, output_dir=SAVE_DIR_SCHEDULED)

        filtered.append(m)
        log(f"âœ… å¯¾è±¡è©¦åˆ: {game_category} | {home} vs {away}")

    page.close()

    # å…ƒã®ãƒªã‚¹ãƒˆã‚’æ›¸ãæ›ãˆ
    matches[:] = filtered


# ============== ãƒ¡ã‚¤ãƒ³å…¥å£ ==============

def fetch_scheduled_matches(days) -> List[Dict[str, str]]:
    """
    Flashscore é–‹å‚¬äºˆå®šã‚¿ãƒ–ã‹ã‚‰ã€
      - ä»Šæ—¥ï¼ˆè¡¨ç¤ºä¸­ã®æ—¥ä»˜ï¼‰
      - ç¿Œæ—¥ä»¥é™ï¼ˆdays-1å›ã€ç¿Œæ—¥ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰
    ã®è©¦åˆæƒ…å ±ã‚’å–å¾—ã—ã¦ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚

    days=2 â†’ ä»Šæ—¥ï¼‹ç¿Œæ—¥
    """
    all_results: List[Dict[str, str]] = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, slow_mo=70)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"),
            locale="ja-JP",
            timezone_id="Asia/Tokyo",
        )
        page = ctx.new_page()

        goto_football_top(page)

        # â‘  é–‹å‚¬äºˆå®šã‚¿ãƒ–ã‹ã‚‰å…¨è©¦åˆã‚’é›†ã‚ã‚‹ï¼ˆã‚«ãƒ†ã‚´ãƒªã¯ã¾ã ç©ºï¼‰
        for day_idx in range(days):
            if day_idx > 0:
                ok = click_next_day(page)
                if not ok:
                    log("â­ï¸ ç¿Œæ—¥ã¸ã®é·ç§»ãŒã§ããªã‹ã£ãŸãŸã‚ã€ä»¥é™ã®å–å¾—ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    break
            log(f"==================== æ—¥ä»˜ã‚ªãƒ•ã‚»ãƒƒãƒˆ {day_idx} æ—¥ç›® ====================")
            day_results = collect_scheduled_matches_on_current_day(page)
            all_results.extend(day_results)

        page.close()

        # â‘¡ å„è©¦åˆãƒšãƒ¼ã‚¸ã®ã€Œé †ä½è¡¨ã€ã‹ã‚‰
        #    ãƒ›ãƒ¼ãƒ é †ä½ / ã‚¢ã‚¦ã‚§ãƒ¼é †ä½ / å›½ï¼†ãƒªãƒ¼ã‚°ã‚’åŸ‹ã‚ã¤ã¤ã€
        #    å¯¾è±¡ãƒªãƒ¼ã‚°ã ã‘ã«ãƒ•ã‚£ãƒ«ã‚¿
        fill_ranks_for_matches(ctx, all_results)

        browser.close()

    log(f"ğŸ‰ ç·å–å¾—ä»¶æ•°: {len(all_results)}")
    return all_results

# ==========================================
# âœ… future_*.xlsx â†’ future_*.csv å¤‰æ›
# ==========================================

def get_existing_future_xlsx_seqs(base_dir: str = SAVE_DIR_SCHEDULED) -> List[int]:
    """
    future_*.xlsx ã®é€šç•ªä¸€è¦§ã‚’æ˜‡é †ã§è¿”ã™ã€‚
    ä¾‹: future_1.xlsx, future_3.xlsx â†’ [1, 3]
    """
    pattern = os.path.join(base_dir, "future_*.xlsx")
    xlsx_files = glob.glob(pattern)
    seqs: List[int] = []

    for path in xlsx_files:
        basename = os.path.basename(path)
        if not basename.startswith("future_") or not basename.endswith(".xlsx"):
            continue
        try:
            num = int(basename.replace("future_", "").replace(".xlsx", ""))
            seqs.append(num)
        except ValueError:
            continue

    if not seqs:
        print("å¤‰æ›å¯¾è±¡ã® future_*.xlsx ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []

    print("å¯¾è±¡Excelãƒ•ã‚¡ã‚¤ãƒ«æ•°:", len(seqs))
    return sorted(seqs)


def excel_to_csv(excel_file: str, csv_file: str):
    """
    1ã¤ã® Excel (future_N.xlsx) ã‚’ CSV (future_N.csv) ã«å¤‰æ›ã—ã€
    å¤‰æ›ã«æˆåŠŸã—ãŸã‚‰å…ƒã® Excel ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """
    try:
        df = pd.read_excel(excel_file)
        df.to_csv(csv_file, index=False)
        print(f"âœ… Excel -> CSV å¤‰æ›å®Œäº†: {os.path.basename(excel_file)} â†’ {os.path.basename(csv_file)}")
        os.remove(excel_file)
        print(f"ğŸ—‘ å…ƒExcelå‰Šé™¤: {os.path.basename(excel_file)}")
    except Exception as e:
        print(f"âš ï¸ Excel -> CSVå¤‰æ›å¤±æ•—: {excel_file} ({e})")


def convert_all_future_excels_to_csv(base_dir: str = SAVE_DIR_SCHEDULED):
    """
    base_dir é…ä¸‹ã® future_*.xlsx ã‚’ã™ã¹ã¦ future_*.csv ã«å¤‰æ›ã™ã‚‹ã€‚
    ã™ã§ã«åŒåã® future_N.csv ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚
    """
    seq_list_all = get_existing_future_xlsx_seqs(base_dir)
    if not seq_list_all:
        return

    for seq in seq_list_all:
        xlsx_name = f"future_{seq}.xlsx"
        csv_name  = f"future_{seq}.csv"
        xlsx_path = os.path.join(base_dir, xlsx_name)
        csv_path  = os.path.join(base_dir, csv_name)

        if not os.path.exists(xlsx_path):
            continue

        # æ—¢ã«åŒã˜ç•ªå·ã®CSVãŒã‚ã‚‹å ´åˆã¯å¿µã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(csv_path):
            print(f"â­ï¸ æ—¢ã«CSVãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {csv_name}")
            continue

        excel_to_csv(xlsx_path, csv_path)

if __name__ == "__main__":
    matches = fetch_scheduled_matches(days=7)
    print(f"ç·ä»¶æ•°: {len(matches)}")

    # ğŸ”¹ Excel ã«ä¿å­˜
    save_scheduled_to_excel(matches, output_dir=SAVE_DIR_SCHEDULED)

    # ğŸ”¹ ä¿å­˜ã•ã‚ŒãŸ future_*.xlsx ã‚’ CSV ã«å¤‰æ›
    convert_all_future_excels_to_csv(base_dir=SAVE_DIR_SCHEDULED)

    if matches:
        from pprint import pprint
        pprint(matches[0])
