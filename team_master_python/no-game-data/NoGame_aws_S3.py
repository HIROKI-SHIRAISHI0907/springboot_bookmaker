# -*- coding: utf-8 -*-
"""
Flashscoreã€Œé–‹å‚¬äºˆå®šã€ã‹ã‚‰ç¿Œæ—¥ã®å¯¾è±¡ãƒªãƒ¼ã‚°è©¦åˆã ã‘æŠ½å‡ºã—ã€
ã€ŒECSåœæ­¢ã—ã¦ã‚ˆã„æ™‚é–“å¸¯ã€ã‚’ç®—å‡ºã—ã¦ S3 ã« JSON ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

æƒ³å®š:
- ECS(Fargate) ã§å®Ÿè¡Œ
- Playwright(Chromium) ã‚’ã‚³ãƒ³ãƒ†ãƒŠå†…ã§å‹•ã‹ã™
- å‡ºåŠ›ã¯ S3 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: aws-s3-no-ecs-task-time-csv) ã« put_object

å¿…è¦ãªAWSæ¨©é™(ã‚¿ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«):
- s3:PutObject (ä¿å­˜å…ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é…ä¸‹)
"""

from playwright.sync_api import sync_playwright
import time
import re
import datetime
import json
import os
from typing import List, Dict, Optional, Tuple
from pathlib import Path

import boto3
from botocore.exceptions import ClientError


# =========================
# Environment / Settings
# =========================

# Playwright / Flashscore
FLASHCORE_URL = "https://www.flashscore.co.jp/"
TIMEZONE_ID = "Asia/Tokyo"
LOCALE = "ja-JP"
USER_AGENT ="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"

HEADLESS = True
SLOW_MO_MS = 0.0

# Slot calculation
# ã©ã‚Œãã‚‰ã„ã®æ™‚é–“ECSç¨¼åƒãŒåœæ­¢ã§ããŸã‚‰åœæ­¢æ‰±ã„ã«ã™ã‚‹ã‹(ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆåœæ­¢ã¯èµ·å‹•å¤±æ•—ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ãŸã‚)
MIN_GAP_MINUTES = int(os.environ.get("MIN_GAP_MINUTES", "0"))
POST_MATCH_BUFFER_MINUTES = int(os.environ.get("POST_MATCH_BUFFER_MINUTES", "180"))  # è©¦åˆå¾Œ3hã¯ç¨¼åƒ
PRE_MATCH_BUFFER_MINUTES = int(os.environ.get("PRE_MATCH_BUFFER_MINUTES", "30"))     # æ¬¡è©¦åˆ30åˆ†å‰ã«ç¨¼åƒå†é–‹

# Local temp dir (ECS/Lambda friendly)
LOCAL_OUT_DIR = "/tmp/no_game"

# S3 output
S3_BUCKET_NO_ECS = "aws-s3-no-ecs-task-time-csv"
S3_KEY_PREFIX = ""  # ç©ºã§ã‚‚OK
S3_REGION = "ap-northeast-1"  # boto3ã¯è‡ªå‹•ã§ã‚‚OK

# Logging
VERBOSE = "1"


def log(msg: str):
    if VERBOSE:
        print(msg)


# =========================
# League Filters (same as your futureData style)
# =========================

CONTAINS_LIST = [
    "ã‚±ãƒ‹ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚³ãƒ­ãƒ³ãƒ“ã‚¢: ãƒ—ãƒªãƒ¡ãƒ¼ãƒ© A", "ã‚¿ãƒ³ã‚¶ãƒ‹ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°",
    "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: EFL ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚·ãƒƒãƒ—", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: EFL ãƒªãƒ¼ã‚° 1", "ã‚¨ãƒã‚ªãƒ”ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚³ã‚¹ã‚¿ãƒªã‚«: ãƒªãƒ¼ã‚¬ FPD",
    "ã‚¸ãƒ£ãƒã‚¤ã‚«: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚¹ãƒšã‚¤ãƒ³: ãƒ©ãƒ»ãƒªãƒ¼ã‚¬", "ãƒ–ãƒ©ã‚¸ãƒ«: ã‚»ãƒªã‚¨ A ãƒ™ã‚¿ãƒ¼ãƒ", "ãƒ–ãƒ©ã‚¸ãƒ«: ã‚»ãƒªã‚¨ B", "ãƒ‰ã‚¤ãƒ„: ãƒ–ãƒ³ãƒ‡ã‚¹ãƒªãƒ¼ã‚¬",
    "éŸ“å›½: K ãƒªãƒ¼ã‚° 1", "ä¸­å›½: ä¸­å›½ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J1 ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J2 ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J3 ãƒªãƒ¼ã‚°", "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢: ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ¼ã‚°",
    "ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢: A ãƒªãƒ¼ã‚°ãƒ»ãƒ¡ãƒ³", "ãƒãƒ¥ãƒ‹ã‚¸ã‚¢: ãƒãƒ¥ãƒ‹ã‚¸ã‚¢ï½¥ãƒ—ãƒ­ãƒªãƒ¼ã‚°", "ã‚¦ã‚¬ãƒ³ãƒ€: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ãƒ¡ã‚­ã‚·ã‚³: ãƒªãƒ¼ã‚¬ MX",
    "ãƒ•ãƒ©ãƒ³ã‚¹: ãƒªãƒ¼ã‚°ãƒ»ã‚¢ãƒ³", "ã‚¹ã‚³ãƒƒãƒˆãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ã‚·ãƒƒãƒ—", "ã‚ªãƒ©ãƒ³ãƒ€: ã‚¨ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ“ã‚¸", "ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³: ãƒˆãƒ«ãƒã‚ªãƒ»ãƒ™ã‚¿ãƒ¼ãƒ",
    "ã‚¤ã‚¿ãƒªã‚¢: ã‚»ãƒªã‚¨ A", "ã‚¤ã‚¿ãƒªã‚¢: ã‚»ãƒªã‚¨ B", "ãƒãƒ«ãƒˆã‚¬ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ«", "ãƒˆãƒ«ã‚³: ã‚¹ãƒ¥ãƒšãƒ«ãƒ»ãƒªã‚°", "ã‚»ãƒ«ãƒ“ã‚¢: ã‚¹ãƒ¼ãƒšãƒ«ãƒªãƒ¼ã‚¬",
    "æ—¥æœ¬: WEãƒªãƒ¼ã‚°", "ãƒœãƒªãƒ“ã‚¢: LFPB", "ãƒ–ãƒ«ã‚¬ãƒªã‚¢: ãƒ‘ãƒ«ãƒ´ã‚¡ãƒ»ãƒªãƒ¼ã‚¬", "ã‚«ãƒ¡ãƒ«ãƒ¼ãƒ³: ã‚¨ãƒªãƒ¼ãƒˆ 1", "ãƒšãƒ«ãƒ¼: ãƒªãƒ¼ã‚¬ 1",
    "ã‚¨ã‚¹ãƒˆãƒ‹ã‚¢: ãƒ¡ã‚¹ã‚¿ãƒªãƒªãƒ¼ã‚¬", "ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠ: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ãƒ™ãƒ«ã‚®ãƒ¼: ã‚¸ãƒ¥ãƒ”ãƒ©ãƒ¼ï½¥ãƒ—ãƒ­ãƒªãƒ¼ã‚°", "ã‚¨ã‚¯ã‚¢ãƒ‰ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒ—ãƒ­",
    "æ—¥æœ¬: YBC ãƒ«ãƒ´ã‚¡ãƒ³ã‚«ãƒƒãƒ—", "æ—¥æœ¬: å¤©çš‡æ¯"
]
UNDER_LIST  = ["U17", "U18", "U19", "U20", "U21", "U22", "U23", "U24", "U25"]
GENDER_LIST = ["å¥³å­"]
EXP_LIST    = ["ãƒãƒ«ãƒˆã‚¬ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ« 2", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° 2", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° U18"]


# =========================
# Helpers
# =========================

def text_clean(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def extract_mid(s: str) -> Optional[str]:
    if not s:
        return None
    s = str(s).strip()
    m = re.search(r"[?&#]mid=([A-Za-z0-9]+)", s)
    if m:
        return m.group(1)
    m = re.search(r"/match/([A-Za-z0-9]{6,20})(?:/|$)", s)
    if m:
        return m.group(1)
    return None


# =========================
# Flashscore navigation
# =========================

def goto_football_top(page):
    log("ğŸŒ Flashscore ãƒˆãƒƒãƒ—ã¸ã‚¢ã‚¯ã‚»ã‚¹...")
    page.goto(FLASHCORE_URL, timeout=45000, wait_until="domcontentloaded")

    # Cookie banner
    try:
        page.locator("#onetrust-accept-btn-handler").click(timeout=2000)
        log("âœ… CookieãƒãƒŠãƒ¼ã‚’é–‰ã˜ã¾ã—ãŸ")
    except:
        pass

    # Click "ã‚µãƒƒã‚«ãƒ¼" just in case
    try:
        soccer_btn = page.locator("a,button").filter(has_text="ã‚µãƒƒã‚«ãƒ¼").first
        if soccer_btn and soccer_btn.count():
            soccer_btn.click(timeout=4000)
            time.sleep(0.8)
    except:
        pass

    # Switch to "é–‹å‚¬äºˆå®š" tab
    try:
        tab = page.locator("div.filters__tab[data-analytics-alias='scheduled']").first
        if tab and tab.count():
            tab.click(timeout=4000)
        else:
            tab = page.locator("div.filters__tab").filter(has_text=re.compile(r"(é–‹å‚¬äºˆå®š)")).first
            tab.click(timeout=4000)
        log("âœ… ã€é–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
    except Exception as e:
        log(f"âš ï¸ é–‹å‚¬äºˆå®šã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆå¤±æ•—: {e}")

    try:
        page.wait_for_timeout(1000)
        page.wait_for_load_state("networkidle", timeout=8000)
    except:
        pass


def expand_all_collapsed_leagues(page):
    log("ğŸ“‚ æŠ˜ã‚ŠãŸãŸã¿ãƒªãƒ¼ã‚°ï¼ˆéè¡¨ç¤ºï¼‰ã‚’å±•é–‹ã—ã¾ã™...")
    btn_selector = (
        "button[data-testid='wcl-accordionButton']"
        "[aria-label='ãƒªãƒ¼ã‚°å…¨è©¦åˆ è¡¨ç¤º']"
    )
    max_loops = 200
    for _ in range(max_loops):
        btns = page.locator(btn_selector)
        count = btns.count()
        if count == 0:
            log("   âœ… ã™ã¹ã¦ã®æŠ˜ã‚ŠãŸãŸã¿ãƒªãƒ¼ã‚°ã‚’å±•é–‹ã—ã¾ã—ãŸ")
            return
        log(f"   æ®‹ã‚Šã€è¡¨ç¤ºã€ãƒœã‚¿ãƒ³æ•°: {count}")
        btn = btns.first
        try:
            btn.scroll_into_view_if_needed()
        except:
            pass
        try:
            btn.click(timeout=2000)
        except Exception as e:
            log(f"   âš ï¸ ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
            break
        page.wait_for_timeout(200)
    log("   âš ï¸ ãƒ«ãƒ¼ãƒ—ä¸Šé™ã€‚ã¾ã éè¡¨ç¤ºãŒæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚ã‚Šã€‚")


def click_next_day(page) -> bool:
    try:
        btn = page.locator("button.wcl-arrow_YpdN4[data-day-picker-arrow='next']").first
        if not btn or not btn.count():
            log("âš ï¸ ç¿Œæ—¥ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return False
        btn.click(timeout=3000)
        log("â¡ï¸ ã€ç¿Œæ—¥ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã—ãŸ")
        time.sleep(1.0)
        try:
            page.wait_for_load_state("networkidle", timeout=8000)
        except:
            pass
        return True
    except Exception as e:
        log(f"âš ï¸ ç¿Œæ—¥ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
        return False


# =========================
# Match row parsing
# =========================

def _get_match_row_teams_and_time(row):
    ktime = ""
    try:
        ktime = text_clean(row.locator(".event__time").first.text_content() or "")
    except:
        pass
    if not ktime:
        try:
            ktime = text_clean(
                row.locator(
                    "[data-testid='wcl-time'], "
                    "[data-testid='wcl-start-time'], "
                    "[data-testid='wcl-time-status']"
                ).first.text_content() or ""
            )
        except:
            pass

    home = ""
    away = ""

    # Old UI
    try:
        h = row.locator(".event__participant--home .event__participant--name").first
        a = row.locator(".event__participant--away .event__participant--name").first
        if h.count():
            home = text_clean(h.text_content() or "")
        if a.count():
            away = text_clean(a.text_content() or "")
    except:
        pass

    # Other patterns
    if not home or not away:
        try:
            ps = row.locator(".event__participant .event__participant--name")
            if ps.count() >= 2:
                if not home:
                    home = text_clean(ps.first.text_content() or "")
                if not away:
                    away = text_clean(ps.last.text_content() or "")
        except:
            pass

    if not home or not away:
        try:
            h = row.locator(
                ".event__homeParticipant span.wcl-name_jjfMf, "
                ".event__homeParticipant [data-testid='wcl-scores-simple-text-01']"
            ).first
            a = row.locator(
                ".event__awayParticipant span.wcl-name_jjfMf, "
                ".event__awayParticipant [data-testid='wcl-scores-simple-text-01']"
            ).first
            if h and h.count() and not home:
                home = text_clean(h.text_content() or "")
            if a and a.count() and not away:
                away = text_clean(a.text_content() or "")
        except:
            pass

    if not home or not away:
        try:
            ps = row.locator(
                "[data-testid='wcl-matchRow-participant'] span.wcl-name_jjfMf, "
                "[data-testid='wcl-matchRow-participant'] [data-testid='wcl-scores-simple-text-01']"
            )
            n = ps.count()
            if n >= 2:
                if not home:
                    home = text_clean(ps.nth(0).text_content() or "")
                if not away:
                    away = text_clean(ps.nth(n - 1).text_content() or "")
        except:
            pass

    if not home or not away:
        try:
            imgs = row.locator("[data-testid='wcl-matchRow-participant'] img[data-testid='wcl-participantLogo']")
            n_img = imgs.count()
            if n_img >= 2:
                if not home:
                    home = text_clean(imgs.nth(0).get_attribute("alt") or "")
                if not away:
                    away = text_clean(imgs.nth(n_img - 1).get_attribute("alt") or "")
        except:
            pass

    if not home or not away:
        try:
            snippet = (row.inner_text() or "").strip().replace("\n", " ")[:200]
        except:
            snippet = "<inner_textå–å¾—å¤±æ•—>"
        log(f"âš ï¸ ãƒãƒ¼ãƒ åå–å¾—å¤±æ•—: time={ktime}, snippet={snippet}")

    return ktime, home, away


def _get_match_row_link(row) -> str:
    try:
        a = row.locator("a.eventRowLink[href*='/match/'][href*='?mid=']").first
        if a and a.count():
            href = a.get_attribute("href") or ""
            if href.startswith("http"):
                return href
            return "https://www.flashscore.co.jp" + href
    except:
        pass
    return ""


def get_current_match_date(page) -> Optional[datetime.date]:
    try:
        btn = page.locator("button[data-testid='wcl-dayPickerButton']").first
        if not btn or not btn.count():
            return None
        txt = text_clean(btn.inner_text() or "")
        m = re.search(r"(\d{2})/(\d{2})", txt)
        if not m:
            return None
        day = int(m.group(1))
        month = int(m.group(2))
        year = datetime.datetime.now().year
        return datetime.date(year, month, day)
    except:
        return None


def collect_scheduled_matches_on_current_day(page) -> List[Dict[str, str]]:
    try:
        page.wait_for_selector("div.event__match", timeout=12000)
    except:
        log("âš ï¸ event__match ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¾ã¾ç¶šè¡Œ")

    expand_all_collapsed_leagues(page)
    match_date = get_current_match_date(page)

    rows = page.locator("div.event__match.event__match--scheduled")
    if rows.count() == 0:
        rows = page.locator("div.event__match")
    n = rows.count()
    log(f"ğŸ¯ é–‹å‚¬äºˆå®šè©¦åˆ è¡Œæ•°: {n}")

    results: List[Dict[str, str]] = []
    seen_mids = set()
    now_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for i in range(n):
        row = rows.nth(i)
        try:
            ktime, home, away = _get_match_row_teams_and_time(row)
            link = _get_match_row_link(row)

            if match_date and ktime:
                match_dt_str = f"{match_date.strftime('%Y-%m-%d')} {ktime}"
            else:
                match_dt_str = ktime

            mid = extract_mid(link)
            if mid and mid in seen_mids:
                log(f"   â­ï¸ é‡è¤‡è©¦åˆ(mid={mid})ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                continue
            if mid:
                seen_mids.add(mid)

            d = {
                "datetime_str": match_dt_str,
                "home": home,
                "away": away,
                "url": link,
                "fetched_at": now_str,
            }
            results.append(d)
            log(f"   [{i+1}/{n}] | {match_dt_str} | {home} vs {away}")
        except Exception as e:
            log(f"   âš ï¸ è¡Œ{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    log(f"âœ… å½“æ—¥åˆ† å–å¾—ä»¶æ•°: {len(results)}")
    return results


# =========================
# Match page -> Country/League + filter
# =========================

def get_country_and_league_from_match_page(page) -> Tuple[str, str]:
    country = ""
    league = ""
    try:
        try:
            page.wait_for_selector(
                "nav[data-testid='wcl-breadcrumbs'] span[data-testid='wcl-scores-overline-03']",
                timeout=3000
            )
        except:
            pass

        spans = page.locator("nav[data-testid='wcl-breadcrumbs'] span[data-testid='wcl-scores-overline-03']")
        count = spans.count()
        if count == 0:
            return "", ""

        texts = []
        for i in range(count):
            txt = text_clean(spans.nth(i).text_content() or "")
            if txt:
                texts.append(txt)

        start_idx = 0
        if texts and texts[0] == "ã‚µãƒƒã‚«ãƒ¼":
            start_idx = 1
        if len(texts) > start_idx:
            country = texts[start_idx]
        if len(texts) > start_idx + 1:
            league = texts[start_idx + 1]
    except:
        pass
    return country, league


def enrich_and_filter_by_league(ctx, matches: List[Dict[str, str]]) -> None:
    if not matches:
        return

    page = ctx.new_page()
    filtered: List[Dict[str, str]] = []

    for idx, m in enumerate(matches):
        url = m.get("url") or ""
        if not url:
            log("â­ï¸ URLãªã—è©¦åˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
            continue

        log(f"=== ãƒªãƒ¼ã‚°å–å¾— {idx+1}/{len(matches)} ===")
        try:
            page.goto(url, timeout=25000, wait_until="domcontentloaded")
        except Exception as e:
            log(f"   âš ï¸ è©¦åˆãƒšãƒ¼ã‚¸é·ç§»å¤±æ•—: {e}")
            continue

        country, league = get_country_and_league_from_match_page(page)
        category = f"{country}: {league}" if (country and league) else (country or league or "")

        if not category:
            log("â­ï¸ ã‚«ãƒ†ã‚´ãƒªå–å¾—å¤±æ•— â†’ é™¤å¤–")
            continue

        if not any(c in category for c in CONTAINS_LIST):
            log(f"â­ï¸ å¯¾è±¡å¤–ãƒªãƒ¼ã‚°: {category}")
            continue

        if (any(x in category for x in UNDER_LIST) or
            any(x in category for x in GENDER_LIST) or
            any(x in category for x in EXP_LIST)):
            log(f"ğŸš« é™¤å¤–ã‚«ãƒ†ã‚´ãƒª: {category}")
            continue

        m["category"] = category
        filtered.append(m)
        log(f"âœ… æ¡ç”¨: {category} | {m.get('home')} vs {m.get('away')}")

    page.close()
    matches[:] = filtered


# =========================
# Slot calculation
# =========================

def _parse_match_datetime(dt_str: str) -> Optional[datetime.datetime]:
    if not dt_str:
        return None
    dt_str = str(dt_str).strip()
    for fmt in ("%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M"):
        try:
            return datetime.datetime.strptime(dt_str, fmt)
        except ValueError:
            continue
    try:
        t = datetime.datetime.strptime(dt_str, "%H:%M").time()
        today = datetime.date.today()
        return datetime.datetime.combine(today, t)
    except ValueError:
        return None


def _calc_free_slots_for_date(
    start_of_day: datetime.datetime,
    end_of_day: datetime.datetime,
    match_datetimes: List[datetime.datetime],
    min_gap_minutes: int = 0
) -> List[Tuple[datetime.datetime, datetime.datetime]]:
    times = sorted(set(match_datetimes))
    free_slots: List[Tuple[datetime.datetime, datetime.datetime]] = []
    current = start_of_day

    for dt in times:
        if dt > current:
            gap_minutes = (dt - current).total_seconds() / 60
            if gap_minutes >= min_gap_minutes:
                free_slots.append((current, dt))
        if dt > current:
            current = dt

    if current < end_of_day:
        gap_minutes = (end_of_day - current).total_seconds() / 60
        if gap_minutes >= min_gap_minutes:
            free_slots.append((current, end_of_day))

    return free_slots


def get_free_slots_for_matches(matches: List[Dict[str, str]], min_gap_minutes: int = 0):
    dts: List[datetime.datetime] = []
    for m in matches:
        dt = _parse_match_datetime(m.get("datetime_str", ""))
        if dt:
            dts.append(dt)

    if not dts:
        log("âš ï¸ æœ‰åŠ¹ãªè©¦åˆé–‹å§‹æ™‚åˆ»ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return []

    target_date = dts[0].date()
    start_of_day = datetime.datetime.combine(target_date, datetime.time(0, 0))
    end_of_day = start_of_day + datetime.timedelta(days=1)

    log(f"\nğŸ¯ å¯¾è±¡æ—¥: {target_date}\n")
    log("ğŸ“ å¯¾è±¡æ—¥ã®è©¦åˆé–‹å§‹æ™‚åˆ»ï¼ˆå¯¾è±¡ãƒªãƒ¼ã‚°ã®ã¿ï¼‰:")
    for dt in sorted(dts):
        log(" ãƒ» " + dt.strftime("%Y-%m-%d %H:%M"))
    log("")

    return _calc_free_slots_for_date(start_of_day, end_of_day, dts, min_gap_minutes)


def convert_free_slots_to_ecs_slots(
    free_slots: List[Tuple[datetime.datetime, datetime.datetime]],
    post_match_buffer_minutes: int = 180,
    pre_match_buffer_minutes: int = 30
) -> List[Tuple[datetime.datetime, datetime.datetime]]:
    ecs_slots: List[Tuple[datetime.datetime, datetime.datetime]] = []
    delta_post = datetime.timedelta(minutes=post_match_buffer_minutes)
    delta_pre = datetime.timedelta(minutes=pre_match_buffer_minutes)

    for free_start, free_end in free_slots:
        ecs_start = free_start + delta_post
        ecs_end = free_end - delta_pre
        if ecs_start < ecs_end:
            ecs_slots.append((ecs_start, ecs_end))
    return ecs_slots


# =========================
# S3 output
# =========================

def upload_json_to_s3(bucket: str, key: str, json_obj) -> bool:
    s3 = boto3.client("s3", region_name=S3_REGION or None)
    body = json.dumps(json_obj, ensure_ascii=False, indent=2).encode("utf-8")
    try:
        s3.put_object(
            Bucket=bucket,
            Key=key,
            Body=body,
            ContentType="application/json; charset=utf-8",
        )
        log(f"âœ… S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: s3://{bucket}/{key}")
        return True
    except ClientError as e:
        log(f"âŒ S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return False


# =========================
# Main flow
# =========================

def fetch_nextday_matches_and_free_slots(min_gap_minutes: int = 0):
    matches: List[Dict[str, str]] = []
    scraped_ok = False

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(user_agent=USER_AGENT, locale=LOCALE, timezone_id=TIMEZONE_ID)
        page = ctx.new_page()

        goto_football_top(page)

        ok = click_next_day(page)
        if not ok:
            log("âŒ ç¿Œæ—¥ã«é€²ã‚ãªã‹ã£ãŸãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
            try: page.close()
            except: pass
            browser.close()
            return [], [], False

        log("==================== ç¿Œæ—¥ã®è©¦åˆã‚’å–å¾— ====================")
        day_results = collect_scheduled_matches_on_current_day(page)
        matches.extend(day_results)
        scraped_ok = True

        try: page.close()
        except: pass

        enrich_and_filter_by_league(ctx, matches)
        browser.close()

    free_slots = get_free_slots_for_matches(matches, min_gap_minutes=min_gap_minutes)
    return matches, free_slots, scraped_ok

def fetch_nextday_matches_and_free_slots(min_gap_minutes: int = 0):
    matches: List[Dict[str, str]] = []
    scraped_ok = False

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(user_agent=USER_AGENT, locale=LOCALE, timezone_id=TIMEZONE_ID)
        page = ctx.new_page()

        goto_football_top(page)

        ok = click_next_day(page)
        if not ok:
            log("âŒ ç¿Œæ—¥ã«é€²ã‚ãªã‹ã£ãŸãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
            try: page.close()
            except: pass
            browser.close()
            return [], [], False  # â† å¤±æ•—

        log("==================== ç¿Œæ—¥ã®è©¦åˆã‚’å–å¾— ====================")
        day_results = collect_scheduled_matches_on_current_day(page)
        matches.extend(day_results)
        scraped_ok = True  # â† ä¸€è¦§å–å¾—ã¯ã§ããŸã€ã¨ã¿ãªã™

        try: page.close()
        except: pass

        enrich_and_filter_by_league(ctx, matches)
        browser.close()

    free_slots = get_free_slots_for_matches(matches, min_gap_minutes=min_gap_minutes)
    return matches, free_slots, scraped_ok

def main():
    matches, free_slots, scraped_ok = fetch_nextday_matches_and_free_slots(
        min_gap_minutes=MIN_GAP_MINUTES
    )

    log("==== ğŸ•’ ç¿Œæ—¥ã®ã€è©¦åˆãŒãªã„æ™‚é–“å¸¯ï¼ˆraw free slotsï¼‰ã€ ====")
    if not free_slots:
        log("ï¼ˆã‚¹ã‚­ãƒæ™‚é–“ãªã— / è©¦åˆãªã— / å–å¾—å¤±æ•—ï¼‰")
    else:
        for start, end in free_slots:
            log(f"  {start.strftime('%Y-%m-%d %H:%M')} ã€œ {end.strftime('%H:%M')}")

    ecs_slots = convert_free_slots_to_ecs_slots(
        free_slots,
        post_match_buffer_minutes=POST_MATCH_BUFFER_MINUTES,
        pre_match_buffer_minutes=PRE_MATCH_BUFFER_MINUTES
    )

    # âœ… è©¦åˆã‚¼ãƒ­ä»¶ã§ã‚‚ã€Œå–å¾—æˆåŠŸãŒç¢ºèªã§ããŸã¨ãã ã‘ã€ä¸¸1æ—¥åœæ­¢
    if scraped_ok and not matches:
        log("âš ï¸ å¯¾è±¡è©¦åˆãŒ0ä»¶ï¼ˆå–å¾—æˆåŠŸï¼‰â†’ ç¿Œæ—¥ã¯24æ™‚é–“ ECSåœæ­¢ã¨ã—ã¾ã™ã€‚")
        target_date = datetime.date.today() + datetime.timedelta(days=1)
        start_of_day = datetime.datetime.combine(target_date, datetime.time(0, 0))
        end_of_day   = start_of_day + datetime.timedelta(days=1)
        ecs_slots = [(start_of_day, end_of_day)]
    elif (not scraped_ok) and (not matches):
        log("âš ï¸ å–å¾—å¤±æ•—ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€24æ™‚é–“åœæ­¢ã¯è¡Œã„ã¾ã›ã‚“ï¼ˆå®‰å…¨å´ï¼‰ã€‚")

    log("\n==== ğŸ“´ ECS åœæ­¢ã—ã¦ã„ã¦ã‚ˆã„æ™‚é–“å¸¯ï¼ˆderived from free slotsï¼‰ ====")
    if not ecs_slots:
        log("ï¼ˆåœæ­¢å¯èƒ½ãªæ™‚é–“å¸¯ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")
    else:
        for start, end in ecs_slots:
            log(f"  {start.strftime('%Y-%m-%d %H:%M')} ã€œ {end.strftime('%H:%M')}")

    json_slots = [{"start": s.isoformat(), "end": e.isoformat()} for s, e in ecs_slots]
    json_payload = {
        "generated_at": datetime.datetime.now().isoformat(),
        "timezone": TIMEZONE_ID,
        "min_gap_minutes": MIN_GAP_MINUTES,
        "post_match_buffer_minutes": POST_MATCH_BUFFER_MINUTES,
        "pre_match_buffer_minutes": PRE_MATCH_BUFFER_MINUTES,
        "ecs_stop_intervals": json_slots,
        "matched_games_count": len(matches),
    }

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ—¥ä»˜ä»˜ãï¼‰
    target_date_str = None
    if matches:
        target_date_str = (matches[0].get("datetime_str") or "")[:10].replace("/", "-")
    if not target_date_str:
        target_date_str = (datetime.date.today() + datetime.timedelta(days=1)).strftime("%Y-%m-%d")

    out_dir = Path(LOCAL_OUT_DIR)
    out_dir.mkdir(parents=True, exist_ok=True)
    out_file = f"ecs_slots_{target_date_str}.json"
    out_path = out_dir / out_file

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(json_payload, f, ensure_ascii=False, indent=2)

    log(f"\nâœ… ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜: {out_path}")

    # S3 upload
    s3_key = out_file  # âœ… ãƒã‚±ãƒƒãƒˆç›´ä¸‹
    ok = upload_json_to_s3(S3_BUCKET_NO_ECS, s3_key, json_payload)

    # ECSã®ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã¯ã€å¤±æ•—ã‚’ exit code ã«åæ˜ ã—ã¦ãŠãã¨é‹ç”¨ã—ã‚„ã™ã„
    if not ok:
        raise SystemExit(2)

    log("\nğŸ‰ å®Œäº†")


if __name__ == "__main__":
    main()
