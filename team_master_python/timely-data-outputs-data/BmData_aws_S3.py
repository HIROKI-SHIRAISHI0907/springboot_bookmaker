# -*- coding: utf-8 -*-
"""
Flashscore „É©„Ç§„ÉñË©¶Âêà -> (stats / summary meta / standings) „ÇíÂèñÂæó„Åó„ÄÅ
1Ë©¶Âêà=1Ë°å„ÅÆCSV„Çí S3 „Å´ÈÄêÊ¨°‰øùÂ≠ò„Åô„ÇãÂÖ®Âá¶ÁêÜÁâàÔºàÊñπÂºèAÔºâ

‚úÖ ÊñπÂºèAÔºàÊé®Â•®Ôºâ
- S3„ÅØËøΩË®ò„Åß„Åç„Å™„ÅÑ„ÅÆ„Åß„Äå1Ë°å=1„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Äç„Åß‰øùÂ≠ò
- Â§±Êïó„Åó„Å¶„ÇÇ‰ªñË°å„Å´ÂΩ±Èüø„Å™„Åó
- SEQMAPÔºàË©¶ÂêàID„Åî„Å®„ÅÆÈÄöÁï™Ôºâ„ÇÇS3„Å´‰øùÂ≠ò„Åó„Å¶Ê∞∏Á∂öÂåñÔºàECS„Åß„ÇÇÈÄ£Áï™„ÅåÁ∂ö„ÅèÔºâ

‚úÖ ÈáçË¶Å‰ªïÊßò
- stats „ÅÆÂÄ§„ÅØ„Äå34%Ôºà31/90Ôºâ„Äç„Å™„Å©‚Äú„Åù„ÅÆ„Åæ„Åæ‚ÄùÂèñÂæóÔºàÂä†Â∑•„Åó„Å™„ÅÑÔºâ
- "„Çª„ÇØ„Ç∑„Éß„É≥:„É©„Éô„É´" -> canonical„Ç≠„Éº -> STAT_KEY_MAP -> HEADERÂàó„Å∏ÊäïÂÖ•
- VERIFY„É≠„Ç∞Ôºà„Éû„ÉÉ„Éî„É≥„Ç∞‰∏ÄËá¥/Êú™‰∏ÄËá¥„ÄÅrowÂüã„Åæ„ÇäÁä∂Ê≥ÅÔºâ„ÅÇ„Çä

ÊÉ≥ÂÆö
- ECS(Fargate) „ÅßÂÆüË°å
- Playwright(Chromium) „ÅØ„Ç≥„É≥„ÉÜ„ÉäÂÜÖ
- „Çø„Çπ„ÇØ„É≠„Éº„É´„Åß S3 PutObject/GetObject Ê®©Èôê
"""

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
import time
import re
import os
import pickle
import datetime
import traceback
from urllib.parse import urlsplit, urlunsplit, urlparse, parse_qs
from typing import Optional, List, Dict, Tuple, Any
import multiprocessing as mp
import queue as pyqueue
import io
import csv
import json

import boto3
from botocore.exceptions import ClientError


# =========================
# Env helpers (ECS env„ÅØÂÖ®ÈÉ®ÊñáÂ≠óÂàó)
# =========================
def env_bool(name: str, default: bool = True) -> bool:
    v = os.environ.get(name)
    if v is None:
        return default
    return v.strip().lower() in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int = 0) -> int:
    v = os.environ.get(name)
    if v is None or v.strip() == "":
        return default
    try:
        return int(float(v.strip()))
    except ValueError:
        return default

def env_float(name: str, default: float = 0.0) -> float:
    v = os.environ.get(name)
    if v is None or v.strip() == "":
        return default
    try:
        return float(v.strip())
    except ValueError:
        return default

def env_str(name: str, default: str = "") -> str:
    v = os.environ.get(name)
    return default if v is None else v


# =========================
# Settings
# =========================
FLASHCORE_URL = "https://www.flashscore.co.jp/"
TIMEZONE_ID   = "Asia/Tokyo"
LOCALE        = "ja-JP"
USER_AGENT    = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"

HEADLESS      = True
SLOW_MO_MS    = 0.0

WORKER_TIMEOUT_SEC = env_int("WORKER_TIMEOUT_SEC", 180)   # Â≠ê„Éó„É≠„Çª„ÇπÂÖ®‰ΩìÔºà1Ë©¶ÂêàÔºâ
NAV_TIMEOUT_MS     = env_int("NAV_TIMEOUT_MS", 20000)     # safe_goto 1st try
WAIT_TIMEOUT_MS    = env_int("WAIT_TIMEOUT_MS", 10000)    # wait_for_selector

VERBOSE = True

BOT_WALL_PAT = r"Just a moment|Access Denied|verify you are human|„ÉÅ„Çß„ÉÉ„ÇØ|Á¢∫Ë™ç"

# S3 settingsÔºàÊñπÂºèAÔºâ
S3_BUCKET_OUTPUTS = "aws-s3-outputs-csv"
S3_REGION         = "ap-northeast-1"
S3_PREFIX         = "" # ‰æã: "outputs/"„ÄÇ„Éê„Ç±„ÉÉ„ÉàÁõ¥‰∏ã„Å™„ÇâÁ©∫„ÄÇ

# SEQMAPÔºàS3„Å´pickle‰øùÂ≠òÔºâ
SEQMAP_S3_KEY     = "seqmap/seqmap.pkl"

# 1Ë°åCSV„Å´„Éò„ÉÉ„ÉÄ„Éº„ÇíÂê´„ÇÅ„Çã„ÅãÔºà1„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà=1Ë°åÈÅãÁî®„Å™„Çâ True „Åå‰æøÂà©Ôºâ
INCLUDE_HEADER_IN_EACH_ROW = env_bool("INCLUDE_HEADER_IN_EACH_ROW", True)

# mp start methodÔºàLinux„Åßspawn„Å´„Åó„Åü„ÅÑÂ†¥Âêà„ÅÆ„ÅøÔºâ
FORCE_SPAWN = env_bool("FORCE_SPAWN", True)


def log(msg: str):
    if VERBOSE:
        print(msg, flush=True)


# =========================
# HEADERÔºàCSVÂàóÔºâ
# =========================
HEADER = [
    "„Éõ„Éº„É†È†Ü‰Ωç","Ë©¶ÂêàÂõΩÂèä„Å≥„Ç´„ÉÜ„Ç¥„É™","Ë©¶ÂêàÊôÇÈñì","„Éõ„Éº„É†„ÉÅ„Éº„É†","„Éõ„Éº„É†„Çπ„Ç≥„Ç¢","„Ç¢„Ç¶„Çß„ÉºÈ†Ü‰Ωç","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†",
    "„Ç¢„Ç¶„Çß„Éº„Çπ„Ç≥„Ç¢","„Éõ„Éº„É†ÊúüÂæÖÂÄ§","„Ç¢„Ç¶„Çß„ÉºÊúüÂæÖÂÄ§","„Éõ„Éº„É†Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§","„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§",
    "„Éõ„Éº„É†„Éú„Éº„É´ÊîØÈÖçÁéá","„Ç¢„Ç¶„Çß„Éº„Éú„Éº„É´ÊîØÈÖçÁéá","„Éõ„Éº„É†„Ç∑„É•„Éº„ÉàÊï∞","„Ç¢„Ç¶„Çß„Éº„Ç∑„É•„Éº„ÉàÊï∞",
    "„Éõ„Éº„É†Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞","„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞","„Éõ„Éº„É†Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞","„Ç¢„Ç¶„Çß„ÉºÊû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞",
    "„Éõ„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà","„Ç¢„Ç¶„Çß„Éº„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà","„Éõ„Éº„É†„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ","„Ç¢„Ç¶„Çß„Éº„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ",
    "„Éõ„Éº„É†„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ","„Ç¢„Ç¶„Çß„Éº„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ","„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà","„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà",
    "„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà","„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà","„Éõ„Éº„É†„Ç¥„Éº„É´„Éù„Çπ„Éà","„Ç¢„Ç¶„Çß„Éº„Ç¥„Éº„É´„Éù„Çπ„Éà","„Éõ„Éº„É†„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´","„Ç¢„Ç¶„Çß„Éº„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´",
    "„Éõ„Éº„É†„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ","„Ç¢„Ç¶„Çß„Éº„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ","„Éõ„Éº„É†„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ","„Ç¢„Ç¶„Çß„Éº„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ",
    "„Éõ„Éº„É†„Ç™„Éï„Çµ„Ç§„Éâ","„Ç¢„Ç¶„Çß„Éº„Ç™„Éï„Çµ„Ç§„Éâ","„Éõ„Éº„É†„Éï„Ç°„Ç¶„É´","„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç¶„É´",
    "„Éõ„Éº„É†„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ","„Ç¢„Ç¶„Çß„Éº„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ","„Éõ„Éº„É†„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ","„Ç¢„Ç¶„Çß„Éº„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ","„Éõ„Éº„É†„Çπ„É≠„Éº„Ç§„É≥","„Ç¢„Ç¶„Çß„Éº„Çπ„É≠„Éº„Ç§„É≥",
    "„Éõ„Éº„É†Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ","„Ç¢„Ç¶„Çß„ÉºÁõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ","„Éõ„Éº„É†„Éë„Çπ","„Ç¢„Ç¶„Çß„Éº„Éë„Çπ","„Éõ„Éº„É†„É≠„É≥„Ç∞„Éë„Çπ","„Ç¢„Ç¶„Çß„Éº„É≠„É≥„Ç∞„Éë„Çπ","„Éõ„Éº„É†„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ","„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ",
    "„Éõ„Éº„É†„ÇØ„É≠„Çπ","„Ç¢„Ç¶„Çß„Éº„ÇØ„É≠„Çπ","„Éõ„Éº„É†„Çø„ÉÉ„ÇØ„É´","„Ç¢„Ç¶„Çß„Éº„Çø„ÉÉ„ÇØ„É´","„Éõ„Éº„É†„ÇØ„É™„Ç¢","„Ç¢„Ç¶„Çß„Éº„ÇØ„É™„Ç¢","„Éõ„Éº„É†„Éá„É•„Ç®„É´ÂãùÂà©Êï∞","„Ç¢„Ç¶„Çß„Éº„Éá„É•„Ç®„É´ÂãùÂà©Êï∞",
    "„Éõ„Éº„É†„Ç§„É≥„Çø„Éº„Çª„Éó„Éà","„Ç¢„Ç¶„Çß„Éº„Ç§„É≥„Çø„Éº„Çª„Éó„Éà",
    "„Çπ„Ç≥„Ç¢ÊôÇÈñì","Â§©Ê∞ó","Ê∞óÊ∏©","ÊπøÂ∫¶","ÂØ©Âà§Âêç","„Éõ„Éº„É†Áõ£Áù£Âêç","„Ç¢„Ç¶„Çß„ÉºÁõ£Áù£Âêç","„Éõ„Éº„É†„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥","„Ç¢„Ç¶„Çß„Éº„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥",
    "„Çπ„Çø„Ç∏„Ç¢„É†","ÂèéÂÆπ‰∫∫Êï∞","Ë¶≥ÂÆ¢Êï∞","Â†¥ÊâÄ","„Éõ„Éº„É†„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖ","„Éõ„Éº„É†„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖÂá∫Â†¥Áä∂Ê≥Å","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†ÊúÄÂ§ßÂæóÁÇπÂèñÂæóËÄÖÂá∫Â†¥Áä∂Ê≥Å",
    "„Éõ„Éº„É†„ÉÅ„Éº„É†„Éõ„Éº„É†ÂæóÁÇπ","„Éõ„Éº„É†„ÉÅ„Éº„É†„Éõ„Éº„É†Â§±ÁÇπ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Éõ„Éº„É†ÂæóÁÇπ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Éõ„Éº„É†Â§±ÁÇπ","„Éõ„Éº„É†„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂæóÁÇπ","„Éõ„Éº„É†„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂ§±ÁÇπ",
    "„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂæóÁÇπ","„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†„Ç¢„Ç¶„Çß„ÉºÂ§±ÁÇπ","ÈÄöÁü•„Éï„É©„Ç∞","Ë©¶Âêà„É™„É≥„ÇØÊñáÂ≠óÂàó","„Ç¥„Éº„É´ÊôÇÈñì","ÈÅ∏ÊâãÂêç","Âà§ÂÆöÁµêÊûú","„Éõ„Éº„É†„ÉÅ„Éº„É†„Çπ„Çø„Ç§„É´","„Ç¢„Ç¶„Çß„Ç§„ÉÅ„Éº„É†„Çπ„Çø„Ç§„É´",
    "„Ç¥„Éº„É´Á¢∫Áéá","ÂæóÁÇπ‰∫àÊÉ≥ÊôÇÈñì","Ë©¶ÂêàID","ÈÄöÁï™","„ÇΩ„Éº„ÉàÁî®Áßí"
]


# =========================
# Áµ±Ë®à„Ç≠„Éº -> HEADERÂàóÂØæÂøúÔºàcanonical„Ç≠„ÉºÂâçÊèêÔºâ
# =========================
STAT_KEY_MAP = {
    "„Ç¢„Çø„ÉÉ„ÇØ:ÊúüÂæÖÂÄ§ÔºàxGÔºâ": ("„Éõ„Éº„É†ÊúüÂæÖÂÄ§", "„Ç¢„Ç¶„Çß„ÉºÊúüÂæÖÂÄ§"),
    "„Ç¢„Çø„ÉÉ„ÇØ:Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§": ("„Éõ„Éº„É†Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§", "„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§"),
    "„Éù„Çº„ÉÉ„Ç∑„Éß„É≥:„Éú„Éº„É´ÊîØÈÖçÁéá": ("„Éõ„Éº„É†„Éú„Éº„É´ÊîØÈÖçÁéá", "„Ç¢„Ç¶„Çß„Éº„Éú„Éº„É´ÊîØÈÖçÁéá"),
    "„Ç∑„É•„Éº„Éà:„Ç∑„É•„Éº„ÉàÊï∞": ("„Éõ„Éº„É†„Ç∑„É•„Éº„ÉàÊï∞", "„Ç¢„Ç¶„Çß„Éº„Ç∑„É•„Éº„ÉàÊï∞"),
    "„Ç∑„É•„Éº„Éà:Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞": ("„Éõ„Éº„É†Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞", "„Ç¢„Ç¶„Çß„ÉºÊû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞"),
    "„Ç∑„É•„Éº„Éà:Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞": ("„Éõ„Éº„É†Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞", "„Ç¢„Ç¶„Çß„ÉºÊû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞"),
    "„Ç∑„É•„Éº„Éà:„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà": ("„Éõ„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà", "„Ç¢„Ç¶„Çß„Éº„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà"),
    "„Ç¢„Çø„ÉÉ„ÇØ:„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ": ("„Éõ„Éº„É†„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ", "„Ç¢„Ç¶„Çß„Éº„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ"),
    "„Çª„ÉÉ„Éà„Éó„É¨„Éº:„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ": ("„Éõ„Éº„É†„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ", "„Ç¢„Ç¶„Çß„Éº„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ"),
    "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà": ("„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà", "„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà"),
    "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà": ("„Éõ„Éº„É†„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà", "„Ç¢„Ç¶„Çß„Éº„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà"),
    "„Ç∑„É•„Éº„Éà:„Éù„Çπ„Éà„Éí„ÉÉ„Éà": ("„Éõ„Éº„É†„Ç¥„Éº„É´„Éù„Çπ„Éà", "„Ç¢„Ç¶„Çß„Éº„Ç¥„Éº„É´„Éù„Çπ„Éà"),
    "„Ç∑„É•„Éº„Éà:„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´": ("„Éõ„Éº„É†„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´", "„Ç¢„Ç¶„Çß„Éº„Éò„Éá„Ç£„É≥„Ç∞„Ç¥„Éº„É´"),

    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ": ("„Éõ„Éº„É†„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ", "„Ç¢„Ç¶„Çß„Éº„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ": ("„Éõ„Éº„É†„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ", "„Ç¢„Ç¶„Çß„Éº„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç™„Éï„Çµ„Ç§„Éâ": ("„Éõ„Éº„É†„Ç™„Éï„Çµ„Ç§„Éâ", "„Ç¢„Ç¶„Çß„Éº„Ç™„Éï„Çµ„Ç§„Éâ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„Ç°„Ç¶„É´": ("„Éõ„Éº„É†„Éï„Ç°„Ç¶„É´", "„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç¶„É´"),
    "„Ç´„Éº„Éâ:„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ": ("„Éõ„Éº„É†„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ", "„Ç¢„Ç¶„Çß„Éº„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ"),
    "„Ç´„Éº„Éâ:„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ": ("„Éõ„Éº„É†„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ", "„Ç¢„Ç¶„Çß„Éº„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çπ„É≠„Éº„Ç§„É≥": ("„Éõ„Éº„É†„Çπ„É≠„Éº„Ç§„É≥", "„Ç¢„Ç¶„Çß„Éº„Çπ„É≠„Éº„Ç§„É≥"),

    "„Éë„Çπ:Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ": ("„Éõ„Éº„É†Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ", "„Ç¢„Ç¶„Çß„ÉºÁõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ"),
    "„Éë„Çπ:Á∑è„Éë„ÇπÊï∞": ("„Éõ„Éº„É†„Éë„Çπ", "„Ç¢„Ç¶„Çß„Éº„Éë„Çπ"),
    "„Éë„Çπ:„É≠„É≥„Ç∞„Éë„Çπ": ("„Éõ„Éº„É†„É≠„É≥„Ç∞„Éë„Çπ", "„Ç¢„Ç¶„Çß„Éº„É≠„É≥„Ç∞„Éë„Çπ"),
    "„Éë„Çπ:„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ": ("„Éõ„Éº„É†„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ", "„Ç¢„Ç¶„Çß„Éº„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ"),
    "„Éë„Çπ:„ÇØ„É≠„Çπ": ("„Éõ„Éº„É†„ÇØ„É≠„Çπ", "„Ç¢„Ç¶„Çß„Éº„ÇØ„É≠„Çπ"),

    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çø„ÉÉ„ÇØ„É´": ("„Éõ„Éº„É†„Çø„ÉÉ„ÇØ„É´", "„Ç¢„Ç¶„Çß„Éº„Çø„ÉÉ„ÇØ„É´"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„ÇØ„É™„Ç¢": ("„Éõ„Éº„É†„ÇØ„É™„Ç¢", "„Ç¢„Ç¶„Çß„Éº„ÇØ„É™„Ç¢"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éá„É•„Ç®„É´ÂãùÂà©": ("„Éõ„Éº„É†„Éá„É•„Ç®„É´ÂãùÂà©Êï∞", "„Ç¢„Ç¶„Çß„Éº„Éá„É•„Ç®„É´ÂãùÂà©Êï∞"),
    "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç§„É≥„Çø„Éº„Çª„Éó„Éà": ("„Éõ„Éº„É†„Ç§„É≥„Çø„Éº„Çª„Éó„Éà", "„Ç¢„Ç¶„Çß„Éº„Ç§„É≥„Çø„Éº„Çª„Éó„Éà"),
}


# =========================
# Flashscore„Äå„É©„Éô„É´„Äç‚Üí canonical„Ç≠„Éº
# =========================
LABEL_TO_CANON: Dict[str, str] = {
    "„Ç¥„Éº„É´ÊúüÂæÖÂÄ§ÔºàxGÔºâ": "„Ç¢„Çø„ÉÉ„ÇØ:ÊúüÂæÖÂÄ§ÔºàxGÔºâ",
    "Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§ÔºàxGOTÔºâ": "„Ç¢„Çø„ÉÉ„ÇØ:Êû†ÂÜÖ„Ç¥„Éº„É´ÊúüÂæÖÂÄ§",

    "„Éú„Éº„É´ÊîØÈÖçÁéá": "„Éù„Çº„ÉÉ„Ç∑„Éß„É≥:„Éú„Éº„É´ÊîØÈÖçÁéá",
    "ÂêàË®à„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Ç∑„É•„Éº„ÉàÊï∞",
    "Êû†ÂÜÖ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:Êû†ÂÜÖ„Ç∑„É•„Éº„ÉàÊï∞",
    "Êû†Â§ñ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:Êû†Â§ñ„Ç∑„É•„Éº„ÉàÊï∞",
    "„Ç∑„É•„Éº„Éà„Éñ„É≠„ÉÉ„ÇØ": "„Ç∑„É•„Éº„Éà:„Éñ„É≠„ÉÉ„ÇØ„Ç∑„É•„Éº„Éà",
    "„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Åã„Çâ„ÅÆ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Ç∑„É•„Éº„Éà",
    "„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Åã„Çâ„ÅÆ„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Éú„ÉÉ„ÇØ„ÇπÂ§ñ„Ç∑„É•„Éº„Éà",
    "„Ç¥„Éº„É´Êû†„Å´ÂΩì„Åü„Çã": "„Ç∑„É•„Éº„Éà:„Éù„Çπ„Éà„Éí„ÉÉ„Éà",
    "„Ç¥„Éº„É´Êû†„Å´ÂΩì„Åü„Çã„Ç∑„É•„Éº„Éà": "„Ç∑„É•„Éº„Éà:„Éù„Çπ„Éà„Éí„ÉÉ„Éà",

    "„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ": "„Ç¢„Çø„ÉÉ„ÇØ:„Éì„ÉÉ„Ç∞„ÉÅ„É£„É≥„Çπ",
    "„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ": "„Çª„ÉÉ„Éà„Éó„É¨„Éº:„Ç≥„Éº„Éä„Éº„Ç≠„ÉÉ„ÇØ",

    "„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ": "„Ç´„Éº„Éâ:„Ç§„Ç®„É≠„Éº„Ç´„Éº„Éâ",
    "„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ": "„Ç´„Éº„Éâ:„É¨„ÉÉ„Éâ„Ç´„Éº„Éâ",
    "„Éï„Ç°„Ç¶„É´": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„Ç°„Ç¶„É´",
    "„Ç™„Éï„Çµ„Ç§„Éâ": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç™„Éï„Çµ„Ç§„Éâ",
    "„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éï„É™„Éº„Ç≠„ÉÉ„ÇØ",
    "„Çπ„É≠„Éº„Ç§„É≥": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çπ„É≠„Éº„Ç§„É≥",
    "„Çø„ÉÉ„ÇØ„É´": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Çø„ÉÉ„ÇØ„É´",
    "„Éá„É•„Ç®„É´ÂãùÂà©Êï∞": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Éá„É•„Ç®„É´ÂãùÂà©",
    "„ÇØ„É™„Ç¢„É™„É≥„Ç∞": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„ÇØ„É™„Ç¢",
    "„ÇØ„É™„Ç¢": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„ÇØ„É™„Ç¢",
    "„Ç§„É≥„Çø„Éº„Çª„Éó„Éà": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç§„É≥„Çø„Éº„Çª„Éó„Éà",
    "„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ": "„Éá„Ç£„Éï„Çß„É≥„Çπ:„Ç≠„Éº„Éë„Éº„Çª„Éº„Éñ",

    "Áõ∏Êâã„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Åß„ÅÆ„Çø„ÉÉ„ÉÅ": "„Éë„Çπ:Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ",
    "Áõ∏Êâã„Éú„ÉÉ„ÇØ„ÇπÂÜÖ„Çø„ÉÉ„ÉÅ": "„Éë„Çπ:Áõ∏Êâã„Éú„ÉÉ„ÇØ„Çπ„Çø„ÉÉ„ÉÅ",
    "„Éë„Çπ": "„Éë„Çπ:Á∑è„Éë„ÇπÊï∞",
    "„É≠„É≥„Ç∞„Éë„Çπ": "„Éë„Çπ:„É≠„É≥„Ç∞„Éë„Çπ",
    "„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Åß„ÅÆ„Éë„Çπ": "„Éë„Çπ:„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ",
    "„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„ÅÆ„Éë„Çπ": "„Éë„Çπ:„Éï„Ç°„Ç§„Éä„É´„Çµ„Éº„Éâ„Éë„Çπ",
    "„ÇØ„É≠„Çπ": "„Éë„Çπ:„ÇØ„É≠„Çπ",
}

SECTION_PREFER = ["‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ", "„Ç∑„É•„Éº„Éà", "„Ç¢„Çø„ÉÉ„ÇØ", "„Éë„Çπ", "„Éá„Ç£„Éï„Çß„É≥„Çπ", "„Ç¥„Éº„É´„Ç≠„Éº„Éë„Éº"]
def _section_rank(section: str) -> int:
    try:
        return SECTION_PREFER.index(section)
    except ValueError:
        return 999


# =========================
# ÂØæË±°„É™„Éº„Ç∞„Éï„Ç£„É´„ÇøÔºàË¶™„Åß‰ΩøÁî®Ôºâ
# =========================
CONTAINS_LIST = [
    "„Ç±„Éã„Ç¢: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Ç≥„É≠„É≥„Éì„Ç¢: „Éó„É™„É°„Éº„É© A", "„Çø„É≥„Ç∂„Éã„Ç¢: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞",
    "„Ç§„É≥„Ç∞„É©„É≥„Éâ: EFL „ÉÅ„É£„É≥„Éî„Ç™„É≥„Ç∑„ÉÉ„Éó", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: EFL „É™„Éº„Ç∞ 1", "„Ç®„ÉÅ„Ç™„Éî„Ç¢: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Ç≥„Çπ„Çø„É™„Ç´: „É™„Éº„Ç¨ FPD",
    "„Ç∏„É£„Éû„Ç§„Ç´: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Çπ„Éö„Ç§„É≥: „É©„Éª„É™„Éº„Ç¨", "„Éñ„É©„Ç∏„É´: „Çª„É™„Ç® A „Éô„Çø„Éº„Éé", "„Éñ„É©„Ç∏„É´: „Çª„É™„Ç® B", "„Éâ„Ç§„ÉÑ: „Éñ„É≥„Éá„Çπ„É™„Éº„Ç¨",
    "ÈüìÂõΩ: K „É™„Éº„Ç∞ 1", "‰∏≠ÂõΩ: ‰∏≠ÂõΩ„Çπ„Éº„Éë„Éº„É™„Éº„Ç∞", "Êó•Êú¨: J1 „É™„Éº„Ç∞", "Êó•Êú¨: J2 „É™„Éº„Ç∞", "Êó•Êú¨: J3 „É™„Éº„Ç∞", "„Ç§„É≥„Éâ„Éç„Ç∑„Ç¢: „Çπ„Éº„Éë„Éº„É™„Éº„Ç∞",
    "„Ç™„Éº„Çπ„Éà„É©„É™„Ç¢: A „É™„Éº„Ç∞„Éª„É°„É≥", "„ÉÅ„É•„Éã„Ç∏„Ç¢: „ÉÅ„É•„Éã„Ç∏„Ç¢ÔΩ•„Éó„É≠„É™„Éº„Ç∞", "„Ç¶„Ç¨„É≥„ÉÄ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„É°„Ç≠„Ç∑„Ç≥: „É™„Éº„Ç¨ MX",
    "„Éï„É©„É≥„Çπ: „É™„Éº„Ç∞„Éª„Ç¢„É≥", "„Çπ„Ç≥„ÉÉ„Éà„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„Ç∑„ÉÉ„Éó", "„Ç™„É©„É≥„ÉÄ: „Ç®„Éº„É´„Éá„Ç£„Éì„Ç∏", "„Ç¢„É´„Çº„É≥„ÉÅ„É≥: „Éà„É´„Éç„Ç™„Éª„Éô„Çø„Éº„Éé",
    "„Ç§„Çø„É™„Ç¢: „Çª„É™„Ç® A", "„Ç§„Çø„É™„Ç¢: „Çª„É™„Ç® B", "„Éù„É´„Éà„Ç¨„É´: „É™„Éº„Ç¨„Éª„Éù„É´„Éà„Ç¨„É´", "„Éà„É´„Ç≥: „Çπ„É•„Éö„É´„Éª„É™„Ç∞", "„Çª„É´„Éì„Ç¢: „Çπ„Éº„Éö„É´„É™„Éº„Ç¨",
    "Êó•Êú¨: WE„É™„Éº„Ç∞", "„Éú„É™„Éì„Ç¢: LFPB", "„Éñ„É´„Ç¨„É™„Ç¢: „Éë„É´„É¥„Ç°„Éª„É™„Éº„Ç¨", "„Ç´„É°„É´„Éº„É≥: „Ç®„É™„Éº„Éà 1", "„Éö„É´„Éº: „É™„Éº„Ç¨ 1",
    "„Ç®„Çπ„Éà„Éã„Ç¢: „É°„Çπ„Çø„É™„É™„Éº„Ç¨", "„Ç¶„ÇØ„É©„Ç§„Éä: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", "„Éô„É´„ÇÆ„Éº: „Ç∏„É•„Éî„É©„ÉºÔΩ•„Éó„É≠„É™„Éº„Ç∞", "„Ç®„ÇØ„Ç¢„Éâ„É´: „É™„Éº„Ç¨„Éª„Éó„É≠",
    "Êó•Êú¨: YBC „É´„É¥„Ç°„É≥„Ç´„ÉÉ„Éó", "Êó•Êú¨: Â§©ÁöáÊùØ"
]
UNDER_LIST  = ["U17", "U18", "U19", "U20", "U21", "U22", "U23", "U24", "U25"]
GENDER_LIST = ["Â•≥Â≠ê"]
EXP_LIST    = ["„Éù„É´„Éà„Ç¨„É´: „É™„Éº„Ç¨„Éª„Éù„É´„Éà„Ç¨„É´ 2", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞ 2", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞ U18"]


# =========================
# VERIFY
# =========================
def verify_header_and_stat_map():
    bad = []
    for k, (hcol, acol) in STAT_KEY_MAP.items():
        if hcol not in HEADER:
            bad.append(("HOME", k, hcol))
        if acol not in HEADER:
            bad.append(("AWAY", k, acol))
    if bad:
        log("‚ùå [VERIFY] STAT_KEY_MAP „ÅåÂèÇÁÖß„Åó„Å¶„ÅÑ„ÇãÂàó„Åå HEADER „Å´ÁÑ°„ÅÑ")
        for side, k, col in bad:
            log(f"   - {side} key='{k}' col='{col}'")
        raise RuntimeError("STAT_KEY_MAP column mismatch with HEADER")
    log("‚úÖ [VERIFY] STAT_KEY_MAP „ÅÆÂàóÂêç„ÅØ HEADER „Å®Êï¥Âêà„Åó„Å¶„ÅÑ„Åæ„Åô")

def verify_stats_mapping(keys_title: str, stats_pairs: Dict[str, Tuple[str, str]]):
    scraped_keys = set(stats_pairs.keys())
    mapped_keys  = set(STAT_KEY_MAP.keys())
    direct_hit   = sorted(scraped_keys & mapped_keys)
    miss_scraped = sorted(scraped_keys - mapped_keys)

    log(f"‚úÖ [VERIFY:{keys_title}] Áõ¥Êé•‰∏ÄËá¥„Ç≠„ÉºÊï∞: {len(direct_hit)}")
    for k in direct_hit[:40]:
        hv, av = stats_pairs.get(k, ("",""))
        log(f"   ‚úì {k} = ({hv}, {av})")

    log(f"‚ö†Ô∏è [VERIFY:{keys_title}] „Éû„ÉÉ„Éó„Å´ÁÑ°„ÅÑ„Çπ„ÇØ„É¨„Ç§„Éó„Ç≠„ÉºÊï∞: {len(miss_scraped)}")
    for k in miss_scraped[:60]:
        hv, av = stats_pairs.get(k, ("",""))
        log(f"   ? {k} = ({hv}, {av})")

def verify_row_filled(row: Dict[str, Any], limit: int = 120):
    filled = []
    for col in HEADER:
        v = row.get(col, "")
        if v not in ("", None):
            filled.append((col, v))
    log(f"‚úÖ [VERIFY] row „Å´ÂÄ§„ÅåÂÖ•„Å£„ÅüÂàóÊï∞: {len(filled)}")
    for col, v in filled[:limit]:
        log(f"   ‚Ä¢ {col} = {v}")


# =========================
# URL builders
# =========================
_MATCH_ROOT_RE = re.compile(r"^(/match/[^/]+/[^/]+/[^/]+/)")

def text_clean(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def extract_mid(any_url: str) -> Optional[str]:
    if not any_url:
        return None
    qs = parse_qs(urlparse(any_url).query)
    return qs.get("mid", [None])[0]

def _match_root_from_any_url(any_url: str) -> Tuple[Tuple[str, str, str], str]:
    parts = urlsplit(any_url)
    path = parts.path or ""
    m = _MATCH_ROOT_RE.search(path)
    if not m:
        raise ValueError(f"match root not found: {any_url}")
    match_root_path = m.group(1)
    mid = extract_mid(any_url) or ""
    return (parts.scheme, parts.netloc, match_root_path), mid

def build_stats_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "summary/stats/overall/"   # ‚Üê „Åì„Åì„Çí 0 „Åã„Çâ overall „Å´
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_summary_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "summary/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_live_standings_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "standings/live-standings/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))

def build_overall_standings_url(any_url: str) -> str:
    (scheme, netloc, root), mid = _match_root_from_any_url(any_url)
    path = root + "standings/standings/overall/"
    query = f"mid={mid}" if mid else ""
    return urlunsplit((scheme, netloc, path, query, ""))


# =========================
# STOPÂØæÁ≠ñÔºösafe_goto
# =========================
def safe_goto(pg, url: str, timeout_ms: int = NAV_TIMEOUT_MS, tag: str = "") -> bool:
    ttag = f" {tag}" if tag else ""
    log(f"üß≠ [GOTO]{ttag} try1 commit: {url}")
    try:
        pg.goto(url, timeout=timeout_ms, wait_until="commit")
        return True
    except PWTimeout as e:
        log(f"‚è±Ô∏è [GOTO]{ttag} try1 timeout: {e}")
    except Exception as e:
        log(f"‚ö†Ô∏è [GOTO]{ttag} try1 error: {type(e).__name__}: {e}")

    try:
        log(f"üõë [GOTO]{ttag} window.stop()")
        pg.evaluate("() => window.stop()")
    except Exception as e:
        log(f"‚ö†Ô∏è [GOTO]{ttag} window.stop error: {type(e).__name__}: {e}")

    log(f"üß≠ [GOTO]{ttag} try2 domcontentloaded: {url}")
    try:
        pg.goto(url, timeout=8000, wait_until="domcontentloaded")
        return True
    except PWTimeout as e:
        log(f"‚è±Ô∏è [GOTO]{ttag} try2 timeout: {e}")
    except Exception as e:
        log(f"‚ö†Ô∏è [GOTO]{ttag} try2 error: {type(e).__name__}: {e}")

    try:
        log(f"üßº [GOTO]{ttag} about:blank")
        pg.goto("about:blank", timeout=3000, wait_until="commit")
    except Exception:
        pass

    log(f"üß≠ [GOTO]{ttag} try3 domcontentloaded(after blank): {url}")
    try:
        pg.goto(url, timeout=8000, wait_until="domcontentloaded")
        return True
    except Exception as e:
        log(f"‚ùå [GOTO]{ttag} try3 failed: {type(e).__name__}: {e}")
        return False


# =========================
# consent / bot wall
# =========================
def kill_onetrust(page):
    try:
        btn = page.locator("#onetrust-accept-btn-handler")
        if btn.count():
            btn.click(timeout=1200, force=True)
    except:
        pass
    try:
        page.evaluate("""
        () => {
          const ids = ["onetrust-consent-sdk", "onetrust-banner-sdk"];
          ids.forEach(id => document.getElementById(id)?.remove());
          document.querySelectorAll(".ot-sdk-container, .ot-sdk-row, .otOverlay, .ot-pc-footer, .ot-pc-header")
            .forEach(el => el.remove());
        }""", timeout=1200)
    except:
        pass

def kill_consent_banners(page):
    kill_onetrust(page)
    candidates = [
        "button:has-text('„Åô„Åπ„Å¶ÊãíÂê¶„Åô„Çã')",
        "button:has-text('ÂÖ®„Å¶ÊãíÂê¶„Åô„Çã')",
        "button:has-text('ÊãíÂê¶„Åô„Çã')",
        "button:has-text('ÂêåÊÑè„Åó„Åæ„Åô')",
        "button:has-text('ÂêåÊÑè„Åô„Çã')",
        "button:has-text('Reject all')",
        "button:has-text('Accept all')",
        "[role='button']:has-text('„Åô„Åπ„Å¶ÊãíÂê¶„Åô„Çã')",
        "[role='button']:has-text('ÂêåÊÑè„Åó„Åæ„Åô')",
    ]
    for sel in candidates:
        try:
            b = page.locator(sel).first
            if b.count() and b.is_visible(timeout=500):
                b.click(timeout=1200, force=True)
                break
        except:
            pass

    try:
        page.evaluate("""
        () => {
          const kill = [
            "#qc-cmp2-container",
            "#didomi-host",
            "#sp_message_container_",
            ".fc-consent-root",
            ".message-component",
            ".pmConsentWall"
          ];
          kill.forEach(s => document.querySelectorAll(s).forEach(el => el.remove()));
        }""", timeout=1200)
    except:
        pass

def is_bot_wall(pg) -> bool:
    try:
        return pg.locator(f"text=/{BOT_WALL_PAT}/i").first.is_visible(timeout=900)
    except:
        return False


# =========================
# route blockingÔºàËªΩÈáèÂåñÔºâ
# =========================
def setup_route_blocking(ctx):
    def _route(route):
        try:
            rtype = route.request.resource_type
            url = (route.request.url or "").lower()
            if rtype in ("image", "media", "font"):
                return route.abort()
            if any(x in url for x in ("doubleclick", "googlesyndication", "adservice", "adsystem", "taboola", "outbrain")):
                return route.abort()
        except:
            pass
        return route.continue_()
    ctx.route("**/*", _route)


# =========================
# match page: teams/scores/time
# =========================
def get_home_away_names(pg) -> Tuple[str, str]:
    try:
        cont = pg.locator("div.duelParticipant__container").first
        if not cont.count():
            cont = pg
        h = cont.locator(".duelParticipant__home a.participant__participantName").first
        a = cont.locator(".duelParticipant__away a.participant__participantName").first
        home = text_clean(h.text_content()) if h.count() else ""
        away = text_clean(a.text_content()) if a.count() else ""
        if not home:
            img = cont.locator(".duelParticipant__home img.participant__image").first
            home = text_clean(img.get_attribute("alt") or "")
        if not away:
            img = cont.locator(".duelParticipant__away img.participant__image").first
            away = text_clean(img.get_attribute("alt") or "")
        return home, away
    except:
        return "", ""

def get_scores(pg) -> Tuple[str, str]:
    def _clean(s: str) -> str:
        return re.sub(r"\s+", "", (s or "").replace("\u00A0", " ")).strip()

    def _from_container(el):
        try:
            spans = el.locator("span")
            vals = []
            for i in range(spans.count()):
                sp = spans.nth(i)
                cls = (sp.get_attribute("class") or "")
                if "divider" in cls:
                    continue
                txt = _clean(sp.text_content() or "")
                if re.fullmatch(r"\d+", txt):
                    vals.append(txt)
            if len(vals) >= 2:
                return vals[0], vals[-1]
        except:
            pass

        try:
            t = _clean(el.inner_text() or "")
            m = re.search(r"(\d+)\s*[\-\u2212\u2012\u2013\u2014\u2015]\s*(\d+)", t)
            if m:
                return m.group(1), m.group(2)
        except:
            pass
        return "", ""

    try:
        fx = pg.locator(".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedScore").first
        if fx.count():
            h, a = _from_container(fx)
            if h and a:
                return h, a
    except:
        pass

    try:
        live = pg.locator("div.detailScore__wrapper.detailScore__live").first
        if live.count():
            h, a = _from_container(live)
            if h and a:
                return h, a
    except:
        pass

    try:
        wrap = pg.locator("div.detailScore__wrapper").first
        if wrap.count():
            h, a = _from_container(wrap)
            if h and a:
                return h, a
    except:
        pass

    return "", ""

def get_match_time_text(pg) -> str:
    def _txt(locator) -> str:
        try:
            if locator.count():
                t = (locator.first.text_content() or "").strip().replace("\u00A0", " ")
                return t
        except:
            pass
        return ""

    # 1) „Åæ„Åö„ÄåË©¶ÂêàÊôÇË®à(eventTime)„Äç„ÇíÊúÄÂÑ™ÂÖà„ÅßÊé¢„ÅôÔºà„ÅÇ„Å™„Åü„ÅÆDOM‰æã„ÅÆ57:33Ôºâ
    #    fixed header / detailScore „Å©„Å°„Çâ„Åß„ÇÇÊãæ„Åà„Çã„Çà„ÅÜ„Å´Ë§áÊï∞ÂÄôË£ú
    event_selectors = [
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .eventAndAddedTime .eventTime",
        "div.detailScore__status .eventAndAddedTime .eventTime",
        ".eventAndAddedTime .eventTime",
    ]
    for s in event_selectors:
        t = _txt(pg.locator(f"{s} >> visible=true"))
        if t:
            return t

    # 2) Ê¨°„Å´ data-testid Á≥ªÔºàÁí∞Â¢É„Å´„Çà„Å£„Å¶„ÅØ„Åì„Åì„Å´ÊôÇÂàª„ÅåÂá∫„ÇãÔºâ
    testid_selectors = [
        "[data-testid='wcl-time']",
    ]
    for s in testid_selectors:
        t = _txt(pg.locator(f"{s} >> visible=true"))
        if t:
            return t

    # 3) ÊúÄÂæå„Å´„ÄåÁä∂ÊÖã„ÉÜ„Ç≠„Çπ„ÉàÔºàÁ¨¨‰∫å„Éè„Éº„ÉïÁ≠âÔºâ„Äç„Çí„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„ÅßÂèñ„Çã
    status_selectors = [
        ".fixedHeaderDuel:not(.fixedHeaderDuel--isHidden) .fixedHeaderDuel__detailStatus",
        "div.detailScore__status .fixedHeaderDuel__detailStatus",
        "div.detailScore__status",
    ]
    for s in status_selectors:
        t = _txt(pg.locator(f"{s} >> visible=true"))
        if t:
            return t

    return ""

def parse_live_time_to_seconds(tstr: str) -> int:
    if not tstr:
        return 0
    t = tstr.strip()
    if "ÁµÇ‰∫Ü" in t or "FT" in t.upper():
        return 5400
    if "ÂâçÂçä" in t:
        num = re.sub(r"[^0-9]", "", t)
        return int(num) * 60 if num else 0
    if "ÂæåÂçä" in t:
        num = re.sub(r"[^0-9]", "", t)
        return 2700 + int(num) * 60 if num else 2700
    if re.match(r"^\d+\+\d+$", t):
        a, b = t.split("+")
        return (int(a) + int(b)) * 60
    if t.isdigit():
        return int(t) * 60
    return 0


# =========================
# RowBuilderÔºàdictÔºâ
# =========================
class RowBuilder:
    def __init__(self, header: List[str], match_tag: str = ""):
        self.header = header
        self.d = {col: "" for col in header}
        self.match_tag = match_tag

    def put(self, key: str, value: Any):
        if key not in self.d:
            return
        self.d[key] = "" if value is None else value

    def put_pair(self, home_key: str, away_key: str, home_val: Any, away_val: Any):
        self.put(home_key, home_val)
        self.put(away_key, away_val)

def apply_stats_to_row(rb: RowBuilder, canon_pairs: Dict[str, Tuple[str, str]]):
    for stat_key, (hcol, acol) in STAT_KEY_MAP.items():
        if stat_key in canon_pairs:
            hv, av = canon_pairs[stat_key]
            rb.put_pair(hcol, acol, hv, av)


# =========================
# stats rawÔºàÂä†Â∑•„Å™„ÅóÔºâ
# =========================
STAT_ROW_SELECTOR = "[data-testid='wcl-statistics']"

def goto_statistics_page(pg) -> bool:
    stats_url = build_stats_url(pg.url)

    kill_consent_banners(pg)

    if "/summary/stats/" not in pg.url:
        log(f"‚û°Ô∏è [STATS] goto: {stats_url}")
        ok = safe_goto(pg, stats_url, timeout_ms=NAV_TIMEOUT_MS, tag="STATS")
        if not ok:
            log("‚ö†Ô∏è [STATS] gotoÂ§±ÊïóÔºàsafe_goto„Åß„ÇÇÔºâ")
            return False
        kill_consent_banners(pg)

    if is_bot_wall(pg):
        log("üß± [STATS] BOTÂ£Å„Å£„ÅΩ„ÅÑ ‚Üí stats„Çπ„Ç≠„ÉÉ„Éó")
        return False

    log("‚è≥ [STATS] wait_for_selector wcl-statistics ...")
    for attempt in range(2):
        try:
            pg.wait_for_selector(STAT_ROW_SELECTOR, timeout=WAIT_TIMEOUT_MS)
            log("‚úÖ [STATS] statistics selector appeared")
            return True
        except Exception as e:
            log(f"‚ö†Ô∏è [STATS] wait_for_selector failed({attempt+1}/2): {type(e).__name__}: {e}")
            kill_consent_banners(pg)

    log("‚ö†Ô∏è [STATS] Áµ±Ë®àÂá∫„Å™„ÅÑÔºàÂêåÊÑèÈô§ÂéªÂæå„ÇÇÔºâ")
    return False

def scrape_stats_raw(pg) -> Dict[str, List[str]]:
    """
    out: {"‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ:„Éë„Çπ": ["34%Ôºà31/90Ôºâ","51%Ôºà24/47Ôºâ"], ...}
    ‚Äª ÂÄ§„ÅØÂä†Â∑•„Åó„Å™„ÅÑ
    """
    if not goto_statistics_page(pg):
        return {}

    log("üìä [STATS] JS extraction start")
    try:
        pg.wait_for_selector('[data-testid="wcl-statistics"]', timeout=WAIT_TIMEOUT_MS)
    except Exception:
        log("‚ö†Ô∏è [STATS] statistics selector not found")
        return {}

    stats = pg.evaluate("""
    () => {
      const out = {};
      const wrapper = document.querySelector('.sectionsWrapper');
      if (!wrapper) return out;

      const clean = (s) =>
        (s || '')
          .replace(/\\u00A0/g, ' ')
          .replace(/\\s+/g, ' ')
          .trim();

      const pickValue = (cell) => {
        if (!cell) return '';
        const spans = cell.querySelectorAll("span[data-testid='wcl-scores-simple-text-01']");
        if (!spans || spans.length === 0) return '';
        const parts = [];
        for (const sp of spans) {
          const t = clean(sp.textContent);
          if (t) parts.push(t);
        }
        return parts.join(' ');
      };

      const sections = wrapper.querySelectorAll('.section');
      for (const sec of sections) {
        const sectionTitle = clean(sec.querySelector('.sectionHeader')?.textContent);
        if (!sectionTitle) continue;

        const rows = sec.querySelectorAll('[data-testid="wcl-statistics"]');
        for (const row of rows) {
          const label = clean(
            row.querySelector('[data-testid="wcl-statistics-category"] span')?.textContent
          );
          if (!label) continue;

          const values = row.querySelectorAll('[data-testid="wcl-statistics-value"]');
          if (!values || values.length < 2) continue;

          const home = pickValue(values[0]);
          const away = pickValue(values[values.length - 1]);

          out[`${sectionTitle}:${label}`] = [home, away];
        }
      }
      return out;
    }
    """) or {}

    log(f"üìä [STATS] extracted items = {len(stats)}")
    return stats


def normalize_stats_raw_to_canon(stats_raw: Dict[str, Any]) -> Dict[str, Tuple[str, str]]:
    """
    stats_raw: {"‰∏ª„Å™„Çπ„Çø„ÉÉ„ÉÑ:ÂêàË®à„Ç∑„É•„Éº„Éà":[home,away], ...}
    -> {"„Ç∑„É•„Éº„Éà:„Ç∑„É•„Éº„ÉàÊï∞":(home,away), ...}
    """
    if not stats_raw:
        return {}

    best_by_label: Dict[str, Tuple[int, Tuple[str, str]]] = {}

    for k, v in stats_raw.items():
        if not isinstance(v, (list, tuple)) or len(v) < 2:
            continue
        if ":" not in k:
            continue

        section, label = k.split(":", 1)
        section = (section or "").strip()
        label = (label or "").strip()

        canon = LABEL_TO_CANON.get(label)
        if not canon:
            continue

        hv, av = str(v[0]), str(v[1])
        rank = _section_rank(section)

        if (label not in best_by_label) or (rank < best_by_label[label][0]):
            best_by_label[label] = (rank, (hv, av))

    out: Dict[str, Tuple[str, str]] = {}
    for label, (_, pair) in best_by_label.items():
        canon = LABEL_TO_CANON.get(label)
        if canon:
            out[canon] = pair
    return out


# =========================
# metaÔºàsummaryÔºâ
# =========================
def get_match_meta(pg) -> Dict[str, str]:
    meta: Dict[str, str] = {}
    summary_url = build_summary_url(pg.url)

    if "/summary/" not in pg.url or "/summary/stats" in pg.url:
        log(f"‚û°Ô∏è [META] goto: {summary_url}")
        ok = safe_goto(pg, summary_url, timeout_ms=NAV_TIMEOUT_MS, tag="META")
        if not ok:
            meta["Ë©¶ÂêàÊôÇÈñì"] = get_match_time_text(pg)
            meta["ÂèñÂæóÊôÇÂàª"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            return meta
        kill_consent_banners(pg)

    if is_bot_wall(pg):
        meta["Ë©¶ÂêàÊôÇÈñì"] = get_match_time_text(pg)
        meta["ÂèñÂæóÊôÇÂàª"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return meta

    try:
        pg.wait_for_selector(
            "ol li span[itemprop='name'], [data-testid*='breadcrumbs'], .tournamentHeader, .duelParticipant",
            timeout=WAIT_TIMEOUT_MS
        )
    except:
        pass

    country = ""
    league = ""

    try:
        crumb_txts = []
        cand = pg.locator("ol li span[itemprop='name'], [data-testid*='breadcrumbs'] span[itemprop='name']")
        for i in range(cand.count()):
            t = text_clean(cand.nth(i).text_content() or "")
            if t:
                crumb_txts.append(t)
        if len(crumb_txts) >= 2:
            country = crumb_txts[1]
        if len(crumb_txts) >= 3:
            league = crumb_txts[2]
    except:
        pass

    if not country or not league:
        try:
            c = pg.locator(".tournamentHeader__country, .tournamentHeader__category, [class*='tournamentHeader__category']").first
            t = text_clean(c.text_content() or "")
            if t:
                if ":" in t and (not country or not league):
                    a, b = [x.strip() for x in t.split(":", 1)]
                    if not country:
                        country = a
                    if not league:
                        league = b
                elif not country:
                    country = t
        except:
            pass

        try:
            l = pg.locator(".tournamentHeader__name, [class*='tournamentHeader__name']").first
            t = text_clean(l.text_content() or "")
            if t and not league:
                league = t
        except:
            pass

    meta["ÂõΩ"] = country
    meta["„É™„Éº„Ç∞"] = league

    label_aliases = {
        "„É¨„Éï„Çß„É™„Éº": "„É¨„Éï„Çß„É™„Éº",
        "ÂØ©Âà§": "„É¨„Éï„Çß„É™„Éº",
        "‰∏ªÂØ©": "„É¨„Éï„Çß„É™„Éº",
        "ÈñãÂÇ¨Âú∞": "ÈñãÂÇ¨Âú∞",
        "„Çπ„Çø„Ç∏„Ç¢„É†": "ÈñãÂÇ¨Âú∞",
        "‰ºöÂ†¥": "ÈñãÂÇ¨Âú∞",
        "ÂèéÂÆπ‰∫∫Êï∞": "ÂèéÂÆπ‰∫∫Êï∞",
        "„Ç≠„É£„Éë„Ç∑„ÉÜ„Ç£": "ÂèéÂÆπ‰∫∫Êï∞",
        "Ë¶≥ÂÆ¢": "Ë¶≥ÂÆ¢",
        "Ë¶≥ÂÆ¢Êï∞": "Ë¶≥ÂÆ¢",
        "ÂèÇÂä†": "Ë¶≥ÂÆ¢",
    }
    want = set(label_aliases.keys())

    def put_meta(label: str, value: str):
        label = text_clean(label).replace(":", "").replace("Ôºö", "").strip()
        value = text_clean(value)
        if not label or not value:
            return
        norm = label_aliases.get(label, label)
        if norm not in meta:
            meta[norm] = value

    try:
        dts = pg.locator("dl dt")
        n = dts.count()
        for i in range(n):
            dt = dts.nth(i)
            lab = text_clean(dt.text_content() or "")
            if lab not in want:
                continue
            dd = dt.locator("xpath=following-sibling::dd[1]").first
            val = text_clean(dd.text_content() or "") if dd.count() else ""
            put_meta(lab, val)
    except:
        pass

    meta["Ë©¶ÂêàÊôÇÈñì"] = get_match_time_text(pg)
    meta["ÂèñÂæóÊôÇÂàª"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return meta


# =========================
# standingsÔºàÈ†Ü‰ΩçÔºâ
# =========================
def goto_standings_page(pg) -> Optional[str]:
    url1 = build_live_standings_url(pg.url)
    url2 = build_overall_standings_url(pg.url)

    log(f"‚û°Ô∏è [RANK] goto(live): {url1}")
    ok = safe_goto(pg, url1, timeout_ms=NAV_TIMEOUT_MS, tag="RANK-LIVE")
    if ok:
        kill_consent_banners(pg)
        try:
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=WAIT_TIMEOUT_MS)
            return url1
        except Exception as e:
            log(f"‚ö†Ô∏è [RANK] live wait failed: {type(e).__name__}: {e}")

    log(f"‚û°Ô∏è [RANK] goto(overall): {url2}")
    ok = safe_goto(pg, url2, timeout_ms=NAV_TIMEOUT_MS, tag="RANK-OVERALL")
    if ok:
        kill_consent_banners(pg)
        try:
            pg.wait_for_selector(".ui-table__body .ui-table__row", timeout=WAIT_TIMEOUT_MS)
            return url2
        except Exception as e:
            log(f"‚ö†Ô∏è [RANK] overall wait failed: {type(e).__name__}: {e}")

    return None

def get_match_standings(pg, home_name: str, away_name: str) -> Dict[str, Any]:
    url = goto_standings_page(pg)
    if not url:
        return {}

    rows = pg.locator(".ui-table__body .ui-table__row")
    try:
        n = rows.count()
    except:
        n = 0
    if n == 0:
        return {}

    table = {}
    for i in range(n):
        r = rows.nth(i)
        rank_txt = text_clean(r.locator(".table__cell--rank .tableCellRank").first.text_content() or "")
        team_name = text_clean(r.locator(".table__cell--participant .tableCellParticipant__name").first.text_content() or "")
        pts_txt = text_clean(r.locator(".table__cell--value").last.text_content() or "")

        try:
            rank = int(rank_txt.strip().rstrip("."))
        except:
            rank = None
        try:
            pts = int(pts_txt)
        except:
            pts = None

        if team_name:
            table[team_name] = {"rank": rank, "pts": pts}

    h = text_clean(home_name)
    a = text_clean(away_name)

    home = next((v for k, v in table.items() if (h and (h in k or k in h))), None)
    away = next((v for k, v in table.items() if (a and (a in k or k in a))), None)

    return {
        "url": url,
        "home_rank": home["rank"] if home else None,
        "home_pts": home["pts"] if home else None,
        "away_rank": away["rank"] if away else None,
        "away_pts": away["pts"] if away else None,
    }


# =========================
# S3 utils
# =========================
def s3_client():
    return boto3.client("s3", region_name=(S3_REGION or None))

def s3_put_bytes(bucket: str, key: str, data: bytes, content_type: str) -> bool:
    s3 = s3_client()
    try:
        s3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type)
        log(f"‚úÖ S3 put: s3://{bucket}/{key}")
        return True
    except ClientError as e:
        log(f"‚ùå S3 put failed: {e}")
        return False

def s3_get_bytes(bucket: str, key: str) -> Optional[bytes]:
    s3 = s3_client()
    try:
        obj = s3.get_object(Bucket=bucket, Key=key)
        return obj["Body"].read()
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code in ("NoSuchKey", "404", "NotFound"):
            return None
        log(f"‚ùå S3 get failed: {e}")
        return None

# =========================
# b001_country_league.json -> CONTAINS_LIST
# =========================
B001_JSON_BUCKET = "aws-s3-outputs-csv"
B001_JSON_KEY    = "json/b001_country_league.json"

def load_country_league_filter_from_s3() -> Optional[List[str]]:
    """
    S3„ÅÆ b001_country_league.json „ÇíË™≠„ÅøËæº„Åø„ÄÅ
    ["Êó•Êú¨: J1 „É™„Éº„Ç∞", "„Ç§„É≥„Ç∞„É©„É≥„Éâ: „Éó„É¨„Éü„Ç¢„É™„Éº„Ç∞", ...] „ÅÆÂΩ¢„Å´„Åó„Å¶Ëøî„Åô„ÄÇ
    ÂèñÂæó‰∏çÂèØ or Á©∫„Å™„Çâ None „ÇíËøî„Åô„ÄÇ
    """
    b = s3_get_bytes(B001_JSON_BUCKET, B001_JSON_KEY)
    if not b:
        log(f"‚ö†Ô∏è [B001] not found or empty: s3://{B001_JSON_BUCKET}/{B001_JSON_KEY}")
        return None

    try:
        obj = json.loads(b.decode("utf-8"))
    except Exception as e:
        log(f"‚ö†Ô∏è [B001] json parse failed: {e}")
        return None

    if not isinstance(obj, dict):
        log("‚ö†Ô∏è [B001] json is not a dict")
        return None

    out: List[str] = []
    for country, leagues in obj.items():
        if not country:
            continue
        if not isinstance(leagues, (list, tuple, set)):
            continue
        for league in leagues:
            if not league:
                continue
            out.append(f"{str(country).strip()}: {str(league).strip()}")

    # ÈáçË§áÈô§Âéª„Åó„Å§„Å§È†ÜÂ∫èÁ∂≠ÊåÅ
    out = list(dict.fromkeys(out))

    if len(out) == 0:
        log(f"‚ö†Ô∏è [B001] json loaded but empty list: s3://{B001_JSON_BUCKET}/{B001_JSON_KEY}")
        return None

    log(f"‚úÖ [B001] loaded contains list from S3: {len(out)} items")
    log(f"   sample: {out[:10]}")
    return out

# =========================
# SEQMAP (S3)
# =========================
SEQMAP: Dict[str, int] = {}

def load_seqmap_from_s3():
    global SEQMAP
    b = s3_get_bytes(S3_BUCKET_OUTPUTS, SEQMAP_S3_KEY)
    if not b:
        SEQMAP = {}
        log(f"üÜï [SEQ] S3„Å´Êó¢Â≠ò„Å™„Åó: s3://{S3_BUCKET_OUTPUTS}/{SEQMAP_S3_KEY}")
        return
    try:
        SEQMAP = pickle.loads(b) or {}
        log(f"üîÅ [SEQ] S3„Åã„ÇâË™≠„ÅøËæº„Åø: ‰ª∂Êï∞={len(SEQMAP)}")
    except Exception as e:
        log(f"‚ö†Ô∏è [SEQ] S3 loadÂ§±Êïó: {e}")
        SEQMAP = {}

def save_seqmap_to_s3():
    try:
        data = pickle.dumps(SEQMAP)
        ok = s3_put_bytes(
            S3_BUCKET_OUTPUTS,
            SEQMAP_S3_KEY,
            data,
            content_type="application/octet-stream"
        )
        if ok:
            log(f"üíæ [SEQ] S3„Å∏‰øùÂ≠ò: ‰ª∂Êï∞={len(SEQMAP)}")
    except Exception as e:
        log(f"‚ö†Ô∏è [SEQ] S3 saveÂ§±Êïó: {e}")


# =========================
# row -> CSV bytes, key builder
# =========================
def row_dict_to_csv_bytes(row: Dict[str, Any], include_header: bool = True) -> bytes:
    buf = io.StringIO()
    w = csv.writer(buf, lineterminator="\n")
    if include_header:
        w.writerow(HEADER)
    w.writerow([row.get(col, "") for col in HEADER])
    return buf.getvalue().encode("utf-8")

def _normalize_prefix(prefix: str) -> str:
    if not prefix:
        return ""
    return prefix if prefix.endswith("/") else (prefix + "/")

def build_row_s3_key(target_date: str, mid: str, seq: int) -> str:
    # timestamp„ÅØUTCË°®Ë®ò„Å´„Åó„Å¶Ë°ùÁ™ÅÂõûÈÅøÔºà‰∏≠Ë∫´„ÅØJSTÈÅãÁî®„Åß„ÇÇOKÔºâ
    ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    prefix = _normalize_prefix(S3_PREFIX)
    # ‰æã: outputs/2026-01-20/mid=XXXX/seq=000001_20260120T010203Z.csv
    return f"{prefix}{target_date}/mid={mid}/seq={seq:06d}_{ts}.csv"

def upload_row_csv_to_s3(row: Dict[str, Any], mid: str, seq: int) -> bool:
    # target_date „ÅØ JST „ÅÆÂÆüË°åÊó•Ôºà„É©„Ç§„Éñ„Çπ„ÇØ„É¨„Ç§„Éó„ÅÆÂá∫ÂäõÂçò‰ΩçÔºâ
    target_date = datetime.date.today().strftime("%Y-%m-%d")
    key = build_row_s3_key(target_date, mid, seq)
    body = row_dict_to_csv_bytes(row, include_header=INCLUDE_HEADER_IN_EACH_ROW)
    return s3_put_bytes(
        S3_BUCKET_OUTPUTS,
        key,
        body,
        content_type="text/csv; charset=utf-8"
    )


# =========================
# Ë¶™Ôºö„É©„Ç§„ÉñURLÂàóÊåô
# =========================
def collect_live_links_filtered() -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(
            user_agent=USER_AGENT,
            locale=LOCALE,
            timezone_id=TIMEZONE_ID
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        page = ctx.new_page()

        log("üåê Flashscore„Éà„ÉÉ„Éó„ÇíÈñã„Åç„Åæ„Åô...")
        ok = safe_goto(page, FLASHCORE_URL, timeout_ms=45000, tag="TOP")
        if not ok:
            log("‚ùå „Éà„ÉÉ„Éó„Éö„Éº„Ç∏„ÅåÈñã„Åë„Åæ„Åõ„Çì„Åß„Åó„Åü")
            try:
                browser.close()
            except:
                pass
            return []

        kill_consent_banners(page)

        # „É©„Ç§„Éñ„Çø„Éñ
        try:
            live_sel = (
                "div.filters__tab:has(div.filters__text--short:has-text('„É©„Ç§„Éñ')),"
                " div.filters__tab:has(div.filters__text--long:has-text('ÈñãÂÇ¨‰∏≠„ÅÆË©¶Âêà'))"
            )
            page.locator(live_sel).first.click(timeout=4000)
            log("‚úÖ [TOP] „É©„Ç§„Éñ„Çø„Éñ click")
        except Exception as e:
            log(f"‚ö†Ô∏è [TOP] „É©„Ç§„Éñ„Çø„Éñ click Â§±Êïó: {type(e).__name__}: {e}")

        try:
            page.wait_for_selector("div.event__match.event__match--live", timeout=20000)
        except:
            pass

        # „Ç¢„Ç≥„Éº„Éá„Ç£„Ç™„É≥Â±ïÈñãÔºà„ÅÇ„Çå„Å∞Ôºâ
        try:
            buttons = page.locator("button[data-testid='wcl-accordionButton']")
            n_btn = buttons.count()
            opened = 0
            for i in range(n_btn):
                btn = buttons.nth(i)
                aria = btn.get_attribute("aria-label") or ""
                if "ÈùûË°®Á§∫" in aria:
                    continue
                try:
                    btn.scroll_into_view_if_needed(timeout=1000)
                    btn.click(timeout=1500)
                    opened += 1
                    time.sleep(0.12)
                except:
                    pass
            if opened:
                time.sleep(0.2)
        except:
            pass

        items = page.evaluate("""
        () => {
          const results = [];
          const headers = Array.from(document.querySelectorAll("[data-testid='wcl-headerLeague']"));
          for (const h of headers) {
            const league =
              (h.querySelector("[data-testid='wcl-scores-simple-text-01']")?.textContent || "").trim() ||
              (h.querySelector("a.headerLeague__title")?.getAttribute("title") || "").trim();

            const country =
              (h.querySelector(".headerLeague__category-text")?.textContent || "").trim() ||
              (h.querySelector(".headerLeague__flag")?.getAttribute("title") || "").trim();

            const category = [country, league].filter(Boolean).join(": ").trim();

            const wrapper = h.closest(".headerLeague__wrapper");
            if (!wrapper) continue;

            let cur = wrapper.nextElementSibling;
            while (cur) {
              if (cur.querySelector?.("[data-testid='wcl-headerLeague']")) break;

              const links = cur.querySelectorAll(
                "div.event__match.event__match--live a.eventRowLink[href*='/match/'][href*='?mid=']"
              );
              for (const a of links) {
                results.push({ href: a.href, category });
              }
              cur = cur.nextElementSibling;
            }
          }
          return results;
        }
        """) or []

        log(f"üß± headerLeague„Éñ„É≠„ÉÉ„ÇØÂèñÂæó: {len(items)} ‰ª∂")

        seen_mid = set()
        for it in items:
            href = it.get("href", "") or ""
            cat  = it.get("category", "") or ""
            mid  = extract_mid(href)

            if not mid or mid in seen_mid:
                continue
            seen_mid.add(mid)

            if not cat:
                continue

            if not any(c in cat for c in CONTAINS_LIST):
                continue
            if any(x in cat for x in UNDER_LIST) or any(x in cat for x in GENDER_LIST) or any(x in cat for x in EXP_LIST):
                continue

            out.append({"url": href, "category": cat})

        log(f"üéØ „Éï„Ç£„É´„ÇøÂæåURL: {len(out)} ‰ª∂")

        try:
            browser.close()
        except:
            pass

    return out


# =========================
# WorkerÔºö1Ë©¶ÂêàÂá¶ÁêÜ
# =========================
def process_one_match_in_worker(url: str, top_category: str = "") -> Dict[str, Any]:
    result: Dict[str, Any] = {"ok": False, "url": url, "top_category": top_category}
    mid = extract_mid(url) or ""

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO_MS,
            args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"],
        )
        ctx = browser.new_context(
            user_agent=USER_AGENT,
            locale=LOCALE,
            timezone_id=TIMEZONE_ID
        )
        ctx.set_default_timeout(15000)
        ctx.set_default_navigation_timeout(15000)
        setup_route_blocking(ctx)
        pg = ctx.new_page()

        try:
            rb = RowBuilder(HEADER, match_tag=mid)

            rb.put("Ë©¶ÂêàID", mid)
            rb.put("Ë©¶Âêà„É™„É≥„ÇØÊñáÂ≠óÂàó", url)

            log("üß© [WORKER] open match page")
            ok = safe_goto(pg, url, timeout_ms=NAV_TIMEOUT_MS, tag="MATCH")
            if not ok:
                return {"ok": False, "url": url, "top_category": top_category, "error": "match_goto_failed"}

            kill_consent_banners(pg)
            if is_bot_wall(pg):
                return {"ok": False, "url": url, "top_category": top_category, "error": "bot_wall"}

            log("üîé [WORKER] read teams/scores/time")
            home, away = get_home_away_names(pg)
            hs, aw = get_scores(pg)
            ttxt = get_match_time_text(pg)

            rb.put("„Éõ„Éº„É†„ÉÅ„Éº„É†", home)
            rb.put("„Ç¢„Ç¶„Çß„Éº„ÉÅ„Éº„É†", away)
            rb.put("„Éõ„Éº„É†„Çπ„Ç≥„Ç¢", hs)
            rb.put("„Ç¢„Ç¶„Çß„Éº„Çπ„Ç≥„Ç¢", aw)
            rb.put("Ë©¶ÂêàÊôÇÈñì", ttxt)

            # meta
            meta = get_match_meta(pg)
            meta_category = ""
            if meta.get("ÂõΩ") and meta.get("„É™„Éº„Ç∞"):
                meta_category = f"{meta['ÂõΩ']}: {meta['„É™„Éº„Ç∞']}".strip()
            final_category = meta_category or (top_category or "")

            rb.put("Ë©¶ÂêàÂõΩÂèä„Å≥„Ç´„ÉÜ„Ç¥„É™", final_category)
            rb.put("„Çπ„Ç≥„Ç¢ÊôÇÈñì", meta.get("ÂèñÂæóÊôÇÂàª", ""))

            if meta.get("ÈñãÂÇ¨Âú∞"):
                rb.put("„Çπ„Çø„Ç∏„Ç¢„É†", meta.get("ÈñãÂÇ¨Âú∞"))
            if meta.get("ÂèéÂÆπ‰∫∫Êï∞"):
                rb.put("ÂèéÂÆπ‰∫∫Êï∞", meta.get("ÂèéÂÆπ‰∫∫Êï∞"))
            if meta.get("Ë¶≥ÂÆ¢"):
                rb.put("Ë¶≥ÂÆ¢Êï∞", meta.get("Ë¶≥ÂÆ¢"))
            if meta.get("„É¨„Éï„Çß„É™„Éº"):
                rb.put("ÂØ©Âà§Âêç", meta.get("„É¨„Éï„Çß„É™„Éº"))

            # standings
            log("üìå [WORKER] standings")
            st = get_match_standings(pg, home, away)
            if st:
                if st.get("home_rank") is not None:
                    rb.put("„Éõ„Éº„É†È†Ü‰Ωç", st.get("home_rank"))
                if st.get("away_rank") is not None:
                    rb.put("„Ç¢„Ç¶„Çß„ÉºÈ†Ü‰Ωç", st.get("away_rank"))

            # stats
            log("üìà [WORKER] scrape stats")
            stats_raw = scrape_stats_raw(pg)
            log(f"[DEBUG] stats_raw keys sample: {list(stats_raw.keys())[:20]}")
            canon_pairs = normalize_stats_raw_to_canon(stats_raw)

            verify_stats_mapping("CANON", canon_pairs)
            apply_stats_to_row(rb, canon_pairs)

            rb.put("ÈÄöÁï™", "")  # Ë¶™„ÅßÁ¢∫ÂÆö
            rb.put("„ÇΩ„Éº„ÉàÁî®Áßí", parse_live_time_to_seconds(rb.d.get("Ë©¶ÂêàÊôÇÈñì", "")))

            verify_row_filled(rb.d)

            result.update({
                "ok": True,
                "mid": mid,
                "meta_category": meta_category,
                "final_category": final_category,
                "row": rb.d,
            })
            return result

        except Exception as e:
            result["error"] = f"{type(e).__name__}: {e}"
            result["trace"] = traceback.format_exc(limit=10)
            log(f"üí• [WORKER] exception: {result['error']}")
            log(result["trace"])
            return result

        finally:
            try:
                pg.close()
            except:
                pass
            try:
                browser.close()
            except:
                pass


def _worker_entry(url: str, top_category: str, q: "mp.Queue"):
    res = process_one_match_in_worker(url, top_category=top_category)
    try:
        q.put(res)
    except:
        pass

def run_match_with_timeout(url: str, top_category: str, timeout_sec: int = WORKER_TIMEOUT_SEC) -> Dict[str, Any]:
    q: mp.Queue = mp.Queue(maxsize=1)
    p = mp.Process(target=_worker_entry, args=(url, top_category, q), daemon=True)
    p.start()
    p.join(timeout=timeout_sec)

    if p.is_alive():
        log(f"üß® [TIMEOUT] Â≠ê„Éó„É≠„Çª„ÇπÂº∑Âà∂ÁµÇ‰∫Ü: {url} ({timeout_sec}s)")
        try:
            p.terminate()
        except:
            pass
        p.join(3)
        return {"ok": False, "url": url, "top_category": top_category, "error": f"timeout({timeout_sec}s)"}

    try:
        return q.get(timeout=2)
    except pyqueue.Empty:
        return {"ok": False, "url": url, "top_category": top_category, "error": "no_result_from_worker"}


# =========================
# Main
# =========================
def main():
    global CONTAINS_LIST  # ‚Üê „Åì„ÇåÈáçË¶ÅÔºà‰∏äÊõ∏„Åç„Åô„Çã„ÅÆ„ÅßÔºâ
    
    # mp start method
    try:
        if FORCE_SPAWN:
            mp.set_start_method("spawn", force=True)
    except RuntimeError:
        # „Åô„Åß„Å´Ë®≠ÂÆöÊ∏à„Åø
        pass

    # ‚òÖ ËøΩÂä†ÔºöS3„ÅÆb001_country_league.json„ÅåÂèñ„Çå„Çå„Å∞ CONTAINS_LIST „ÇíÂãïÁöÑ„Å´ÁΩÆÊèõ
    dyn = load_country_league_filter_from_s3()
    if dyn is not None:
        CONTAINS_LIST = dyn
        log("‚úÖ [FILTER] CONTAINS_LIST is replaced by b001_country_league.json")
    else:
        log("‚ÑπÔ∏è [FILTER] fallback to hard-coded CONTAINS_LIST")

    log(f"ENV HEADLESS={os.environ.get('HEADLESS')} -> {HEADLESS}, SLOW_MO_MS={os.environ.get('SLOW_MO_MS')} -> {SLOW_MO_MS}")
    log(f"S3_BUCKET_OUTPUTS={S3_BUCKET_OUTPUTS}, S3_PREFIX='{S3_PREFIX}', SEQMAP_S3_KEY={SEQMAP_S3_KEY}")

    verify_header_and_stat_map()
    load_seqmap_from_s3()

    items = collect_live_links_filtered()
    log(f"üéØ „É©„Ç§„ÉñË©¶Âêà: {len(items)} ‰ª∂")

    for i, it in enumerate(items, 1):
        url = it.get("url", "") or ""
        top_category = it.get("category", "") or ""

        log("\n==============================")
        log(f"[{i}/{len(items)}] {url}")
        log(f"TOP category = {top_category}")
        log("==============================")

        res = run_match_with_timeout(url, top_category=top_category, timeout_sec=WORKER_TIMEOUT_SEC)
        if not res.get("ok"):
            log(f"‚ö†Ô∏è [WORKER] Â§±Êïó: {res.get('error','')} url={url}")
            continue

        mid = res.get("mid", "") or ""
        row = res.get("row", {}) or {}
        final_category = (res.get("final_category", "") or "").strip()

        # Ë¶™„Åß„ÇÇÂÜç„Éï„Ç£„É´„ÇøÔºàÊúÄÁµÇ„Ç´„ÉÜ„Ç¥„É™„ÅßÂà§ÂÆöÔºâ
        if not any(c in final_category for c in CONTAINS_LIST):
            log(f"‚è≠Ô∏è „Çπ„Ç≠„ÉÉ„ÉóÔºà„É™„Çπ„ÉàÂ§ñÔºâ: {final_category}")
            continue
        if any(x in final_category for x in UNDER_LIST) or any(x in final_category for x in GENDER_LIST) or any(x in final_category for x in EXP_LIST):
            log(f"üö´ Èô§Â§ñ: {final_category}")
            continue

        # ÈÄöÁï™Á¢∫ÂÆöÔºàSEQMAP„ÅØË¶™„ÅÆ„Åø„ÅåÊõ¥Êñ∞Ôºâ
        last_seq = int(SEQMAP.get(mid, 0)) if mid else 0
        seq = last_seq + 1
        if mid:
            SEQMAP[mid] = seq

        row["ÈÄöÁï™"] = seq
        row["Ë©¶ÂêàID"] = mid

        ok = upload_row_csv_to_s3(row, mid=mid, seq=seq)
        if not ok:
            log("‚ùå row„ÅÆS3‰øùÂ≠ò„Å´Â§±ÊïóÔºà„Åì„ÅÆË©¶Âêà„ÅØ„Çπ„Ç≠„ÉÉ„ÉóÊâ±„ÅÑÔºâ")
            continue

    # SEQMAP‰øùÂ≠òÔºàÊúÄÂæå„Å´„Åæ„Å®„ÇÅ„Å¶Ôºâ
    save_seqmap_to_s3()
    log("üéâ ÂÆå‰∫Ü")


if __name__ == "__main__":
    main()
