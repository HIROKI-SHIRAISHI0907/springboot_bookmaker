# -*- coding: utf-8 -*-
"""
FutureData_aws_S3.py (ECS + S3)
- Flashscore.co.jp ã®ã€Œã‚µãƒƒã‚«ãƒ¼ > é–‹å‚¬äºˆå®šã€ã‹ã‚‰ FUTURE_DAYS æ—¥åˆ†ã®è©¦åˆã‚’å–å¾—
- å„è©¦åˆã®é †ä½è¡¨(standings)ã‹ã‚‰ã€Œå›½/ãƒªãƒ¼ã‚°ã€ã€Œãƒ›ãƒ¼ãƒ é †ä½/ã‚¢ã‚¦ã‚§ãƒ¼é †ä½ã€ã‚’åŸ‹ã‚ã‚‹
- å¯¾è±¡ãƒªãƒ¼ã‚°ã®ã¿æŠ½å‡ºï¼ˆCONTAINS_LIST / UNDER_LIST / GENDER_LIST / EXP_LISTï¼‰
- future_N.xlsx ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¿å­˜ï¼ˆMAX_ROWS_PER_FILE_SCHEDULED è¡Œã”ã¨ï¼‰
- future_*.xlsx -> future_*.csv å¤‰æ›
- S3ï¼ˆaws-s3-future-csvï¼‰ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
  - S3å†…ã® future_*.csv æœ€å¤§é€£ç•ªã‚’å–å¾—ã—ã€æ¬¡ã®ç•ªå·ã‹ã‚‰æ¡ç•ªã—ã¦ä¸Šæ›¸ãå›é¿
"""

from playwright.sync_api import sync_playwright
import time
import re, os
import datetime
from typing import List, Dict, Optional
import pandas as pd
from pathlib import Path
import glob

try:
    import openpyxl
except ImportError:
    raise RuntimeError("openpyxl ãŒå¿…è¦ã§ã™ã€‚`pip install openpyxl` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# âœ… S3
import boto3

# =========================
# ç’°å¢ƒå¤‰æ•°
# =========================
FUTURE_DAYS = int(os.environ.get("FUTURE_DAYS", "7"))
SAVE_DIR_SCHEDULED = os.environ.get("FUTURE_SAVE_DIR", "/tmp/bookmaker/future")

S3_BUCKET_FUTURE = os.environ.get("S3_BUCKET_FUTURE", "aws-s3-future-csv")
S3_PREFIX_FUTURE = os.environ.get("S3_PREFIX_FUTURE", "")  # ä¾‹: "future/"

# ============== å–å¾—æ¡ä»¶ ====================

HEADER_SCHEDULED = [
    "è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª", "è©¦åˆäºˆå®šæ™‚é–“", "ãƒ›ãƒ¼ãƒ é †ä½", "ã‚¢ã‚¦ã‚§ãƒ¼é †ä½", "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ æœ€å¤§å¾—ç‚¹å–å¾—è€…",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹", "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¾—ç‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ãƒ›ãƒ¼ãƒ å¤±ç‚¹",
    "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹", "ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹",
    "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¾—ç‚¹", "ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ã‚¢ã‚¦ã‚§ãƒ¼å¤±ç‚¹", "è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—", "ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚é–“"
]

MAX_ROWS_PER_FILE_SCHEDULED = 10
SHEET_NAME_SCHEDULED = "Sheet1"
FILE_PREFIX_SCHEDULED = "future_"
FILE_SUFFIX_SCHEDULED = ".xlsx"

# S3ã®æœ€å¤§é€£ç•ª+1 ã‚’èµ·ç‚¹ã«ã™ã‚‹
SERIAL_BASE_SCHEDULED = 1

# ============== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==============

VERBOSE = True

def log(msg: str):
    if VERBOSE:
        print(msg)

def text_clean(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def extract_mid(s: str) -> Optional[str]:
    """URLã‹ã‚‰ mid ã‚’æŠ½å‡º"""
    if not s:
        return None
    s = str(s).strip()
    m = re.search(r"[?&#]mid=([A-Za-z0-9]+)", s)
    if m:
        return m.group(1)
    m = re.search(r"/match/([A-Za-z0-9]{6,20})(?:/|$)", s)
    if m:
        return m.group(1)
    return None

# ==========================================
# âœ… å¯¾è±¡ãƒªãƒ¼ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ãƒªã‚¹ãƒˆ
# ==========================================
CONTAINS_LIST = [
    "ã‚±ãƒ‹ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚³ãƒ­ãƒ³ãƒ“ã‚¢: ãƒ—ãƒªãƒ¡ãƒ¼ãƒ© A", "ã‚¿ãƒ³ã‚¶ãƒ‹ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°",
    "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: EFL ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã‚·ãƒƒãƒ—", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: EFL ãƒªãƒ¼ã‚° 1", "ã‚¨ãƒã‚ªãƒ”ã‚¢: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚³ã‚¹ã‚¿ãƒªã‚«: ãƒªãƒ¼ã‚¬ FPD",
    "ã‚¸ãƒ£ãƒã‚¤ã‚«: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ã‚¹ãƒšã‚¤ãƒ³: ãƒ©ãƒ»ãƒªãƒ¼ã‚¬", "ãƒ–ãƒ©ã‚¸ãƒ«: ã‚»ãƒªã‚¨ A ãƒ™ã‚¿ãƒ¼ãƒ", "ãƒ–ãƒ©ã‚¸ãƒ«: ã‚»ãƒªã‚¨ B", "ãƒ‰ã‚¤ãƒ„: ãƒ–ãƒ³ãƒ‡ã‚¹ãƒªãƒ¼ã‚¬",
    "éŸ“å›½: K ãƒªãƒ¼ã‚° 1", "ä¸­å›½: ä¸­å›½ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J1 ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J2 ãƒªãƒ¼ã‚°", "æ—¥æœ¬: J3 ãƒªãƒ¼ã‚°", "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢: ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ¼ã‚°",
    "ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢: A ãƒªãƒ¼ã‚°ãƒ»ãƒ¡ãƒ³", "ãƒãƒ¥ãƒ‹ã‚¸ã‚¢: ãƒãƒ¥ãƒ‹ã‚¸ã‚¢ï½¥ãƒ—ãƒ­ãƒªãƒ¼ã‚°", "ã‚¦ã‚¬ãƒ³ãƒ€: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ãƒ¡ã‚­ã‚·ã‚³: ãƒªãƒ¼ã‚¬ MX",
    "ãƒ•ãƒ©ãƒ³ã‚¹: ãƒªãƒ¼ã‚°ãƒ»ã‚¢ãƒ³", "ã‚¹ã‚³ãƒƒãƒˆãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ã‚·ãƒƒãƒ—", "ã‚ªãƒ©ãƒ³ãƒ€: ã‚¨ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ“ã‚¸", "ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³: ãƒˆãƒ«ãƒã‚ªãƒ»ãƒ™ã‚¿ãƒ¼ãƒ",
    "ã‚¤ã‚¿ãƒªã‚¢: ã‚»ãƒªã‚¨ A", "ã‚¤ã‚¿ãƒªã‚¢: ã‚»ãƒªã‚¨ B", "ãƒãƒ«ãƒˆã‚¬ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ«", "ãƒˆãƒ«ã‚³: ã‚¹ãƒ¥ãƒšãƒ«ãƒ»ãƒªã‚°", "ã‚»ãƒ«ãƒ“ã‚¢: ã‚¹ãƒ¼ãƒšãƒ«ãƒªãƒ¼ã‚¬",
    "æ—¥æœ¬: WEãƒªãƒ¼ã‚°", "ãƒœãƒªãƒ“ã‚¢: LFPB", "ãƒ–ãƒ«ã‚¬ãƒªã‚¢: ãƒ‘ãƒ«ãƒ´ã‚¡ãƒ»ãƒªãƒ¼ã‚¬", "ã‚«ãƒ¡ãƒ«ãƒ¼ãƒ³: ã‚¨ãƒªãƒ¼ãƒˆ 1", "ãƒšãƒ«ãƒ¼: ãƒªãƒ¼ã‚¬ 1",
    "ã‚¨ã‚¹ãƒˆãƒ‹ã‚¢: ãƒ¡ã‚¹ã‚¿ãƒªãƒªãƒ¼ã‚¬", "ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠ: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚°", "ãƒ™ãƒ«ã‚®ãƒ¼: ã‚¸ãƒ¥ãƒ”ãƒ©ãƒ¼ï½¥ãƒ—ãƒ­ãƒªãƒ¼ã‚°", "ã‚¨ã‚¯ã‚¢ãƒ‰ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒ—ãƒ­",
    "æ—¥æœ¬: YBC ãƒ«ãƒ´ã‚¡ãƒ³ã‚«ãƒƒãƒ—", "æ—¥æœ¬: å¤©çš‡æ¯"
]
UNDER_LIST  = ["U17", "U18", "U19", "U20", "U21", "U22", "U23", "U24", "U25"]
GENDER_LIST = ["å¥³å­"]
EXP_LIST    = ["ãƒãƒ«ãƒˆã‚¬ãƒ«: ãƒªãƒ¼ã‚¬ãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ« 2", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° 2", "ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰: ãƒ—ãƒ¬ãƒŸã‚¢ãƒªãƒ¼ã‚° U18"]

# ==========================================
# âœ… S3 é€£æºï¼ˆæœ€å¤§é€£ç•ªå–å¾—ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
# ==========================================

def _s3_client():
    return boto3.client("s3")

def get_max_future_csv_seq_in_s3(bucket: str, prefix: str = "") -> int:
    """
    S3ä¸Šã® prefix + future_*.csv ã®æœ€å¤§é€£ç•ªã‚’è¿”ã™ï¼ˆç„¡ã‘ã‚Œã°0ï¼‰
    """
    s3 = _s3_client()
    max_seq = 0
    token = None

    pattern = re.compile(rf"^{re.escape(prefix)}{re.escape(FILE_PREFIX_SCHEDULED)}(\d+)\.csv$")

    while True:
        kwargs = {"Bucket": bucket}
        if prefix:
            kwargs["Prefix"] = prefix
        if token:
            kwargs["ContinuationToken"] = token

        resp = s3.list_objects_v2(**kwargs)
        for obj in resp.get("Contents", []):
            key = obj.get("Key", "")
            m = pattern.match(key)
            if m:
                try:
                    max_seq = max(max_seq, int(m.group(1)))
                except:
                    pass

        if resp.get("IsTruncated"):
            token = resp.get("NextContinuationToken")
        else:
            break

    return max_seq

def upload_file_to_s3(local_path: str, bucket: str, key: str):
    s3 = _s3_client()
    s3.upload_file(local_path, bucket, key)
    log(f"â˜ï¸ S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: s3://{bucket}/{key}")

def init_serial_base_from_s3():
    """
    S3ä¸Šã® future_*.csv æœ€å¤§é€£ç•ª+1 ã‚’ SERIAL_BASE_SCHEDULED ã«ã‚»ãƒƒãƒˆ
    """
    global SERIAL_BASE_SCHEDULED
    try:
        max_seq = get_max_future_csv_seq_in_s3(S3_BUCKET_FUTURE, S3_PREFIX_FUTURE)
        SERIAL_BASE_SCHEDULED = max_seq + 1
        log(f"ğŸ”¢ S3æœ€å¤§é€£ç•ª: {max_seq} â†’ ä»Šå›é–‹å§‹: {SERIAL_BASE_SCHEDULED}")
    except Exception as e:
        log(f"âš ï¸ S3ã‹ã‚‰é€£ç•ªå–å¾—å¤±æ•—ï¼ˆ{e}ï¼‰ã€‚SERIAL_BASE=1ã§ç¶šè¡Œï¼ˆä¸Šæ›¸ãæ³¨æ„ï¼‰")
        SERIAL_BASE_SCHEDULED = 1

# ==========================================
# âœ… Excel é€æ¬¡æ›¸ãè¾¼ã¿ï¼ˆé–‹å‚¬äºˆå®šï¼‰
# ==========================================

def _existing_serials_scheduled(output_dir: str) -> List[int]:
    p = Path(output_dir)
    nums = []
    for f in p.glob(f"{FILE_PREFIX_SCHEDULED}*{FILE_SUFFIX_SCHEDULED}"):
        m = re.match(rf"^{re.escape(FILE_PREFIX_SCHEDULED)}(\d+){re.escape(FILE_SUFFIX_SCHEDULED)}$", f.name)
        if m:
            nums.append(int(m.group(1)))
    return sorted(nums)

def _next_serial_scheduled(output_dir: str) -> int:
    nums = _existing_serials_scheduled(output_dir)
    if nums:
        return max(nums) + 1
    return SERIAL_BASE_SCHEDULED

def _current_file_path_scheduled(output_dir: str) -> Path:
    p = Path(output_dir)
    nums = _existing_serials_scheduled(output_dir)
    if not nums:
        return p / f"{FILE_PREFIX_SCHEDULED}{SERIAL_BASE_SCHEDULED}{FILE_SUFFIX_SCHEDULED}"
    return p / f"{FILE_PREFIX_SCHEDULED}{max(nums)}{FILE_SUFFIX_SCHEDULED}"

def _data_rows_in_scheduled(path: Path) -> int:
    if not path.exists():
        return 0
    wb = openpyxl.load_workbook(path, read_only=True, data_only=True)
    ws = wb[SHEET_NAME_SCHEDULED] if SHEET_NAME_SCHEDULED in wb.sheetnames else wb.active
    total = ws.max_row or 0
    wb.close()
    return max(0, total - 1)

def _create_new_workbook_scheduled(path: Path):
    df = pd.DataFrame(columns=HEADER_SCHEDULED)
    path.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=SHEET_NAME_SCHEDULED)

def append_scheduled_row_to_excel(
    row_dict: Dict[str, str],
    output_dir: str = SAVE_DIR_SCHEDULED,
    max_rows_per_file: int = MAX_ROWS_PER_FILE_SCHEDULED
):
    output_dir_path = Path(output_dir)
    output_dir_path.mkdir(parents=True, exist_ok=True)

    cur = _current_file_path_scheduled(str(output_dir_path))
    if not cur.exists():
        _create_new_workbook_scheduled(cur)

    current_rows = _data_rows_in_scheduled(cur)

    if current_rows >= max_rows_per_file:
        next_serial = _next_serial_scheduled(str(output_dir_path))
        cur = output_dir_path / f"{FILE_PREFIX_SCHEDULED}{next_serial}{FILE_SUFFIX_SCHEDULED}"
        _create_new_workbook_scheduled(cur)
        current_rows = 0

    df = pd.DataFrame([row_dict])
    for col in HEADER_SCHEDULED:
        if col not in df.columns:
            df[col] = ""
    df = df[HEADER_SCHEDULED]

    with pd.ExcelWriter(cur, engine="openpyxl", mode="a", if_sheet_exists="overlay") as w:
        startrow = current_rows + 1
        df.to_excel(w, index=False, header=False, sheet_name=SHEET_NAME_SCHEDULED, startrow=startrow)

    log(f"ğŸ’¾ [FUTURE] è¿½è¨˜: {cur.name}ï¼ˆ{current_rows}â†’{current_rows+1}ï¼‰")

def save_scheduled_to_excel(match_results: List[Dict[str, str]], output_dir: str = SAVE_DIR_SCHEDULED):
    if not match_results:
        log("âœ‹ Excelã«æ›¸ãè¾¼ã‚€é–‹å‚¬äºˆå®šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    os.makedirs(output_dir, exist_ok=True)

# ==========================================
# âœ… Flashscoreã€Œé–‹å‚¬äºˆå®šã€ãƒŠãƒ“
# ==========================================

def goto_football_top(page):
    """ã‚µãƒƒã‚«ãƒ¼ â†’ é–‹å‚¬äºˆå®šã‚¿ãƒ–ã¸é·ç§»"""
    log("ğŸŒ Flashscore ãƒˆãƒƒãƒ—ã¸ã‚¢ã‚¯ã‚»ã‚¹...")
    page.goto("https://www.flashscore.co.jp/", timeout=45000, wait_until="domcontentloaded")

    # CookieãƒãƒŠãƒ¼ãªã©
    try:
        page.locator("#onetrust-accept-btn-handler").click(timeout=2000)
        log("âœ… CookieãƒãƒŠãƒ¼ã‚’é–‰ã˜ã¾ã—ãŸ")
    except:
        pass

    # ã‚µãƒƒã‚«ãƒ¼ãŒé¸ã°ã‚Œã¦ã„ãªã„å ´åˆã«å‚™ãˆã¦ã€Œã‚µãƒƒã‚«ãƒ¼ã€ã‚¯ãƒªãƒƒã‚¯ï¼ˆä¿é™ºï¼‰
    try:
        soccer_btn = page.locator("a,button").filter(has_text="ã‚µãƒƒã‚«ãƒ¼").first
        if soccer_btn and soccer_btn.count():
            soccer_btn.click(timeout=4000)
            time.sleep(0.8)
    except:
        pass

    # ã€Œé–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
    try:
        tab = page.locator("div.filters__tab[data-analytics-alias='scheduled']").first
        if tab and tab.count():
            tab.click(timeout=4000)
        else:
            tab = page.locator("div.filters__tab").filter(has_text=re.compile(r"(é–‹å‚¬äºˆå®š)")).first
            tab.click(timeout=4000)
        log("âœ… ã€é–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
    except Exception as e:
        log(f"âš ï¸ é–‹å‚¬äºˆå®šã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆå¤±æ•—: {e}")

    try:
        page.wait_for_timeout(1000)
        page.wait_for_load_state("networkidle", timeout=8000)
    except:
        pass

def expand_all_collapsed_leagues(page):
    """
    Flashscoreã€é–‹å‚¬äºˆå®šã€ã‚¿ãƒ–ã§ã€æŠ˜ã‚ŠãŸãŸã¾ã‚Œã¦ã„ã‚‹ãƒªãƒ¼ã‚°ã‚’ã™ã¹ã¦å±•é–‹ã™ã‚‹ã€‚
    """
    print("ğŸ“‚ æŠ˜ã‚ŠãŸãŸã¿ãƒªãƒ¼ã‚°ï¼ˆéè¡¨ç¤ºï¼‰ã‚’å±•é–‹ã—ã¾ã™...")

    btn_selector = (
        "button[data-testid='wcl-accordionButton']"
        "[aria-label='ãƒªãƒ¼ã‚°å…¨è©¦åˆ è¡¨ç¤º']"
    )

    max_loops = 200
    for _ in range(max_loops):
        btns = page.locator(btn_selector)
        count = btns.count()

        if count == 0:
            print("   âœ… ã™ã¹ã¦ã®æŠ˜ã‚ŠãŸãŸã¿ãƒªãƒ¼ã‚°ã‚’å±•é–‹ã—ã¾ã—ãŸ")
            return

        print(f"   æ®‹ã‚Šã€è¡¨ç¤ºã€ãƒœã‚¿ãƒ³æ•°: {count}")

        btn = btns.first
        try:
            btn.scroll_into_view_if_needed()
        except Exception:
            pass

        try:
            btn.click(timeout=2000)
        except Exception as e:
            print(f"   âš ï¸ ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
            break

        page.wait_for_timeout(200)

    print("   âš ï¸ ãƒ«ãƒ¼ãƒ—ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã¾ã éè¡¨ç¤ºãƒªãƒ¼ã‚°ãŒæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

# ==========================================
# âœ… è©¦åˆè¡Œã®åŸºæœ¬æƒ…å ±å–å¾—
# ==========================================

def _get_match_row_teams_and_time(row):
    """
    1ã¤ã®è©¦åˆè¡Œã‹ã‚‰
      - è©¦åˆäºˆå®šæ™‚é–“
      - ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ å
      - ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ å
    ã‚’æŠ½å‡ºï¼ˆæ—§UI / æ–°UI ä¸¡å¯¾å¿œï¼‰
    """
    ktime = ""

    # æ—§UI: .event__time
    try:
        ktime = text_clean(row.locator(".event__time").first.text_content() or "")
    except Exception:
        pass

    # æ–°UI: data-testid ãƒ™ãƒ¼ã‚¹
    if not ktime:
        try:
            ktime = text_clean(
                row.locator(
                    "[data-testid='wcl-time'], "
                    "[data-testid='wcl-start-time'], "
                    "[data-testid='wcl-time-status']"
                ).first.text_content() or ""
            )
        except Exception:
            pass

    home = ""
    away = ""

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘ : æ—§UI (.event__participant--home / --away)
    try:
        h = row.locator(".event__participant--home .event__participant--name").first
        a = row.locator(".event__participant--away .event__participant--name").first
        if h.count():
            home = text_clean(h.text_content() or "")
        if a.count():
            away = text_clean(a.text_content() or "")
    except Exception:
        pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘¡: æ—§UI ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (.event__participant)
    if not home or not away:
        try:
            ps = row.locator(".event__participant .event__participant--name")
            if ps.count() >= 2:
                if not home:
                    home = text_clean(ps.first.text_content() or "")
                if not away:
                    away = text_clean(ps.last.text_content() or "")
        except Exception:
            pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘¢: æ–°UI æ˜ç¤ºçš„ã‚»ãƒ¬ã‚¯ã‚¿
    if not home or not away:
        try:
            h = row.locator(
                ".event__homeParticipant span.wcl-name_jjfMf, "
                ".event__homeParticipant [data-testid='wcl-scores-simple-text-01']"
            ).first
            a = row.locator(
                ".event__awayParticipant span.wcl-name_jjfMf, "
                ".event__awayParticipant [data-testid='wcl-scores-simple-text-01']"
            ).first

            if h and h.count():
                if not home:
                    home = text_clean(h.text_content() or "")
            if a and a.count():
                if not away:
                    away = text_clean(a.text_content() or "")
        except Exception:
            pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘£: æ–°UI ã‚†ã‚‹ã‚
    if not home or not away:
        try:
            ps = row.locator(
                "[data-testid='wcl-matchRow-participant'] span.wcl-name_jjfMf, "
                "[data-testid='wcl-matchRow-participant'] [data-testid='wcl-scores-simple-text-01']"
            )
            n = ps.count()
            if n >= 2:
                if not home:
                    home = text_clean(ps.nth(0).text_content() or "")
                if not away:
                    away = text_clean(ps.nth(n - 1).text_content() or "")
        except Exception:
            pass

    # ãƒ‘ã‚¿ãƒ¼ãƒ³â‘¤: <img alt="ãƒãƒ¼ãƒ å">
    if not home or not away:
        try:
            imgs = row.locator("[data-testid='wcl-matchRow-participant'] img[data-testid='wcl-participantLogo']")
            n_img = imgs.count()
            if n_img >= 2:
                if not home:
                    alt0 = imgs.nth(0).get_attribute("alt") or ""
                    home = text_clean(alt0)
                if not away:
                    alt1 = imgs.nth(n_img - 1).get_attribute("alt") or ""
                    away = text_clean(alt1)
        except Exception:
            pass

    if not home or not away:
        try:
            snippet = (row.inner_text() or "").strip().replace("\n", " ")[:200]
        except Exception:
            snippet = "<inner_text å–å¾—å¤±æ•—>"
        print(f"âš ï¸ ãƒãƒ¼ãƒ åå–å¾—å¤±æ•—: time={ktime}, snippet={snippet}")

    return ktime, home, away

def _get_match_row_link(row) -> str:
    """è©¦åˆè¡Œã‹ã‚‰ matchURL ã‚’æŠ½å‡º"""
    try:
        a = row.locator("a.eventRowLink[href*='/match/'][href*='?mid=']").first
        if a and a.count():
            href = a.get_attribute("href") or ""
            if href.startswith("http"):
                return href
            return "https://www.flashscore.co.jp" + href
    except:
        pass
    return ""

# ==========================================
# âœ… æ—¥ä»˜
# ==========================================

def get_current_match_date(page) -> Optional[datetime.date]:
    """
    Flashscore ãƒˆãƒƒãƒ—ã®æ—¥ä»˜ãƒœã‚¿ãƒ³ï¼ˆä¾‹: '05/12 é‡‘'ï¼‰ã‹ã‚‰ datetime.date ã‚’ä½œã£ã¦è¿”ã™ã€‚
    """
    try:
        btn = page.locator("button[data-testid='wcl-dayPickerButton']").first
        if not btn or not btn.count():
            return None

        txt = text_clean(btn.inner_text() or "")
        m = re.search(r"(\d{2})/(\d{2})", txt)
        if not m:
            return None

        day = int(m.group(1))
        month = int(m.group(2))

        # å¹´ã¯ä»Šå¹´ï¼ˆå¹´ã¾ãŸãã¯ç°¡æ˜“å¯¾å¿œï¼‰
        year = datetime.datetime.now().year
        return datetime.date(year, month, day)

    except Exception:
        return None

# ==========================================
# âœ… é–‹å‚¬äºˆå®šã‚¿ãƒ–ã‹ã‚‰åŸºæœ¬æƒ…å ±ã ã‘é›†ã‚ã‚‹
# ==========================================

def collect_scheduled_matches_on_current_day(page) -> List[Dict[str, str]]:
    """
    ç¾åœ¨è¡¨ç¤ºä¸­ã®æ—¥ä»˜ï¼ˆé–‹å‚¬äºˆå®šã‚¿ãƒ–ï¼‰ã‹ã‚‰è©¦åˆæƒ…å ±ã‚’åé›†ã€‚
    ã“ã“ã§ã¯ã€Œæ™‚é–“ãƒ»ãƒ›ãƒ¼ãƒ ãƒ»ã‚¢ã‚¦ã‚§ãƒ¼ãƒ»ãƒªãƒ³ã‚¯ã€ã ã‘ã‚’é›†ã‚ã€
    å›½ãƒªãƒ¼ã‚°ï¼†é †ä½ã¯å¾Œã§è©¦åˆãƒšãƒ¼ã‚¸ã‹ã‚‰åŸ‹ã‚ã‚‹ã€‚
    """
    try:
        page.wait_for_selector("div.event__match", timeout=12000)
    except:
        log("âš ï¸ event__match ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¾ã¾ç¶šè¡Œ")

    expand_all_collapsed_leagues(page)

    match_date = get_current_match_date(page)

    rows = page.locator("div.event__match.event__match--scheduled")
    if rows.count() == 0:
        rows = page.locator("div.event__match")

    n = rows.count()
    log(f"ğŸ¯ é–‹å‚¬äºˆå®šè©¦åˆ è¡Œæ•°: {n}")

    results: List[Dict[str, str]] = []
    seen_mids = set()

    now_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for i in range(n):
        row = rows.nth(i)
        try:
            ktime, home, away = _get_match_row_teams_and_time(row)
            link = _get_match_row_link(row)

            if match_date and ktime:
                match_dt_str = f"{match_date.strftime('%Y-%m-%d')} {ktime}"
            else:
                match_dt_str = ktime

            mid = extract_mid(link)
            if mid and mid in seen_mids:
                log(f"   â­ï¸ é‡è¤‡è©¦åˆ(mid={mid})ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                continue
            if mid:
                seen_mids.add(mid)

            d = {k: "" for k in HEADER_SCHEDULED}
            d["è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª"] = ""  # å¾Œã§åŸ‹ã‚ã‚‹
            d["è©¦åˆäºˆå®šæ™‚é–“"]       = match_dt_str
            d["ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ "]       = home
            d["ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ "]     = away
            d["è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—"]   = link
            d["ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚é–“"]     = now_str

            results.append(d)
            log(f"   [{i+1}/{n}] | {match_dt_str} | {home} vs {away} | mid={mid}")

        except Exception as e:
            log(f"   âš ï¸ è¡Œ{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    log(f"âœ… å½“æ—¥åˆ† å–å¾—ä»¶æ•°: {len(results)}")
    return results

def click_next_day(page) -> bool:
    """
    ã€Œç¿Œæ—¥ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æ—¥ä»˜ã‚’1æ—¥é€²ã‚ã‚‹ã€‚
    """
    try:
        btn = page.locator("button.wcl-arrow_YpdN4[data-day-picker-arrow='next']").first
        if not btn or not btn.count():
            log("âš ï¸ ç¿Œæ—¥ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return False
        btn.click(timeout=3000)
        log("â¡ï¸ ã€ç¿Œæ—¥ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã—ãŸ")
        time.sleep(1.0)
        try:
            page.wait_for_load_state("networkidle", timeout=8000)
        except:
            pass
        return True
    except Exception as e:
        log(f"âš ï¸ ç¿Œæ—¥ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")
        return False

# ==========================================
# âœ… é †ä½è¡¨ & å›½ãƒªãƒ¼ã‚°å–å¾—
# ==========================================

def build_standings_url_from_match_url(match_url: str) -> str:
    """
    è©¦åˆãƒšãƒ¼ã‚¸URLã‚’é †ä½è¡¨ã‚¿ãƒ–ã®URLã«å¤‰æ›ã™ã‚‹ã€‚
    """
    if not match_url:
        return ""
    if "/standings/" in match_url:
        return match_url
    return re.sub(r"/(\?mid=)", r"/standings/\1", match_url, count=1)

def get_team_ranks_from_standings_table(page, home_name: str, away_name: str):
    """
    ã™ã§ã«ã€Œé †ä½è¡¨ã€ã‚¿ãƒ–ï¼ˆã‚ªãƒ¼ãƒãƒ¼ã‚ªãƒ¼ãƒ«ï¼‰ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ page ã‹ã‚‰é †ä½ã‚’å–å¾—ã€‚
    """
    home_name_norm = text_clean(home_name)
    away_name_norm = text_clean(away_name)

    home_rank = ""
    away_rank = ""

    rows = page.locator("div.ui-table__body > div.ui-table__row")
    n_rows = rows.count()

    for i in range(n_rows):
        row = rows.nth(i)
        try:
            team_elem = row.locator(".tableCellParticipant__name").first
            if not team_elem.count():
                continue
            team_name = text_clean(team_elem.text_content() or "")

            rank_elem = row.locator(".tableCellRank").first
            if not rank_elem.count():
                continue
            rank_text = text_clean(rank_elem.text_content() or "")
            rank_text = rank_text.rstrip(".").strip()

            if not home_rank and team_name == home_name_norm:
                home_rank = rank_text
            if not away_rank and team_name == away_name_norm:
                away_rank = rank_text

            if home_rank and away_rank:
                break
        except:
            continue

    return home_rank, away_rank

def get_country_and_league_from_match_page(page):
    """
    è©¦åˆãƒšãƒ¼ã‚¸ï¼ˆã‚µãƒãƒªãƒ¼ / é †ä½è¡¨ã‚¿ãƒ–ï¼‰ã®ãƒ‘ãƒ³ããšã‹ã‚‰å›½å/ãƒªãƒ¼ã‚°åã‚’å–å¾—ã™ã‚‹ã€‚
    """
    country = ""
    league = ""

    try:
        try:
            page.wait_for_selector(
                "nav[data-testid='wcl-breadcrumbs'] span[data-testid='wcl-scores-overline-03']",
                timeout=3000
            )
        except Exception:
            pass

        spans = page.locator(
            "nav[data-testid='wcl-breadcrumbs'] span[data-testid='wcl-scores-overline-03']"
        )
        count = spans.count()

        if count == 0:
            return "", ""

        texts = []
        for i in range(count):
            txt = text_clean(spans.nth(i).text_content() or "")
            if txt:
                texts.append(txt)

        start_idx = 0
        if texts and texts[0] == "ã‚µãƒƒã‚«ãƒ¼":
            start_idx = 1

        if len(texts) > start_idx:
            country = texts[start_idx]
        if len(texts) > start_idx + 1:
            league = texts[start_idx + 1]

    except Exception:
        pass

    return country, league

def fetch_ranks_for_match(page, match_url: str, home_name: str, away_name: str):
    """
    è©¦åˆURLã‹ã‚‰é †ä½è¡¨ã‚¿ãƒ–ã«é£›ã³ã€
      - ãƒ›ãƒ¼ãƒ é †ä½
      - ã‚¢ã‚¦ã‚§ãƒ¼é †ä½
      - å›½å
      - ãƒªãƒ¼ã‚°åï¼ˆãƒ©ã‚¦ãƒ³ãƒ‰ä»˜ãï¼‰
    ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    if not match_url:
        return "", "", "", ""

    standings_url = build_standings_url_from_match_url(match_url)

    try:
        log(f"   ğŸ“Š é †ä½è¡¨å–å¾—: {standings_url}")
        page.goto(standings_url, timeout=25000, wait_until="domcontentloaded")

        # å¿µã®ãŸã‚ standings ã‚¿ãƒ–é¸æŠï¼ˆUIå¤‰åŒ–ã«å‚™ãˆã‚‹ï¼‰
        try:
            standings_tab = page.locator(
                "a[data-analytics-alias='stats-detail'] button, "
                "a[href*='/standings/'] button"
            ).first
            if standings_tab and standings_tab.count():
                selected = standings_tab.get_attribute("data-selected")
                if selected != "true":
                    standings_tab.click(timeout=3000)
                    page.wait_for_timeout(500)
        except:
            pass

        country, league = get_country_and_league_from_match_page(page)

        page.wait_for_selector("div.ui-table__body div.ui-table__row", timeout=12000)

        home_rank, away_rank = get_team_ranks_from_standings_table(page, home_name, away_name)
        log(f"      â†’ rank: home={home_rank}, away={away_rank}, country={country}, league={league}")
        return home_rank, away_rank, country, league

    except Exception as e:
        log(f"   âš ï¸ é †ä½è¡¨å–å¾—å¤±æ•—: {e}")
        return "", "", "", ""

# ==========================================
# âœ… å…¨è©¦åˆã«å¯¾ã—ã¦ å›½ãƒªãƒ¼ã‚° & é †ä½ & ãƒ•ã‚£ãƒ«ã‚¿
# ==========================================

def fill_ranks_for_matches(ctx, matches: List[Dict[str, str]]):
    """
    ã€Œé–‹å‚¬äºˆå®šã€ã§åé›†ã—ãŸè©¦åˆãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ã€
    å„è©¦åˆãƒšãƒ¼ã‚¸ã®é †ä½è¡¨ã‹ã‚‰é †ä½/å›½ãƒªãƒ¼ã‚°ã‚’åŸ‹ã‚ã€å¯¾è±¡ãƒªãƒ¼ã‚°ã ã‘æ®‹ã™ã€‚
    """
    if not matches:
        return

    page = ctx.new_page()
    filtered: List[Dict[str, str]] = []

    for idx, m in enumerate(matches):
        url  = m.get("è©¦åˆãƒªãƒ³ã‚¯æ–‡å­—åˆ—") or ""
        home = m.get("ãƒ›ãƒ¼ãƒ ãƒãƒ¼ãƒ ") or ""
        away = m.get("ã‚¢ã‚¦ã‚§ãƒ¼ãƒãƒ¼ãƒ ") or ""

        if not url or not home or not away:
            log(f"â­ï¸ URL/ãƒãƒ¼ãƒ åä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {home} vs {away}")
            continue

        log(f"=== é †ä½å–å¾— {idx+1}/{len(matches)} ===")
        home_rank, away_rank, country, league = fetch_ranks_for_match(page, url, home, away)

        game_category = ""
        if country and league:
            game_category = f"{country}: {league}"
        elif country or league:
            game_category = country or league

        if not game_category:
            log("â­ï¸ ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡: ï¼ˆã‚«ãƒ†ã‚´ãƒªå–å¾—å¤±æ•—ï¼‰")
            continue

        # 1) CONTAINS_LIST ã«å«ã¾ã‚Œã‚‹ãƒªãƒ¼ã‚°ã ã‘å¯¾è±¡
        if not any(c in game_category for c in CONTAINS_LIST):
            log(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡: {game_category}ï¼ˆãƒªã‚¹ãƒˆå¤–ï¼‰")
            continue

        # 2) å¹´ä»£ï¼ˆUxxï¼‰ãƒ»å¥³å­ãƒ»ä¾‹å¤–ãƒªãƒ¼ã‚°ã‚’å«ã‚€å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if (any(x in game_category for x in UNDER_LIST) or
            any(x in game_category for x in GENDER_LIST) or
            any(x in game_category for x in EXP_LIST)):
            log(f"ğŸš« é™¤å¤–å¯¾è±¡: {game_category}")
            continue

        if home_rank:
            m["ãƒ›ãƒ¼ãƒ é †ä½"] = home_rank
        if away_rank:
            m["ã‚¢ã‚¦ã‚§ãƒ¼é †ä½"] = away_rank
        m["è©¦åˆå›½åŠã³ã‚«ãƒ†ã‚´ãƒª"] = game_category

        # âœ… ã“ã“ã§å³ Excel ã«1è¡Œè¿½è¨˜
        append_scheduled_row_to_excel(m, output_dir=SAVE_DIR_SCHEDULED)

        filtered.append(m)
        log(f"âœ… å¯¾è±¡è©¦åˆ: {game_category} | {home} vs {away}")

    page.close()
    matches[:] = filtered

# ==========================================
# âœ… ãƒ¡ã‚¤ãƒ³å…¥å£ï¼ˆé–‹å‚¬äºˆå®šå–å¾—ï¼‰
# ==========================================

def fetch_scheduled_matches(days: int) -> List[Dict[str, str]]:
    """
    Flashscore é–‹å‚¬äºˆå®šã‚¿ãƒ–ã‹ã‚‰ã€
      - ä»Šæ—¥ï¼ˆè¡¨ç¤ºä¸­ã®æ—¥ä»˜ï¼‰
      - ç¿Œæ—¥ä»¥é™ï¼ˆdays-1å›ã€ç¿Œæ—¥ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰
    ã®è©¦åˆæƒ…å ±ã‚’å–å¾—ã—ã¦ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚

    days=2 â†’ ä»Šæ—¥ï¼‹ç¿Œæ—¥
    """
    all_results: List[Dict[str, str]] = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, slow_mo=70)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"),
            locale="ja-JP",
            timezone_id="Asia/Tokyo",
        )
        page = ctx.new_page()

        goto_football_top(page)

        # â‘  é–‹å‚¬äºˆå®šã‚¿ãƒ–ã‹ã‚‰å…¨è©¦åˆã‚’é›†ã‚ã‚‹ï¼ˆã‚«ãƒ†ã‚´ãƒªã¯ã¾ã ç©ºï¼‰
        for day_idx in range(days):
            if day_idx > 0:
                ok = click_next_day(page)
                if not ok:
                    log("â­ï¸ ç¿Œæ—¥ã¸ã®é·ç§»ãŒã§ããªã‹ã£ãŸãŸã‚ã€ä»¥é™ã®å–å¾—ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    break
            log(f"==================== æ—¥ä»˜ã‚ªãƒ•ã‚»ãƒƒãƒˆ {day_idx} æ—¥ç›® ====================")
            day_results = collect_scheduled_matches_on_current_day(page)
            all_results.extend(day_results)

        page.close()

        # â‘¡ å„è©¦åˆãƒšãƒ¼ã‚¸ã®ã€Œé †ä½è¡¨ã€ã‹ã‚‰åŸ‹ã‚ã¤ã¤å¯¾è±¡ãƒªãƒ¼ã‚°ã®ã¿æŠ½å‡º
        fill_ranks_for_matches(ctx, all_results)

        browser.close()

    log(f"ğŸ‰ ç·å–å¾—ä»¶æ•°: {len(all_results)}")
    return all_results

# ==========================================
# âœ… future_*.xlsx â†’ future_*.csv å¤‰æ› + S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# ==========================================

def get_existing_future_xlsx_seqs(base_dir: str = SAVE_DIR_SCHEDULED) -> List[int]:
    pattern = os.path.join(base_dir, "future_*.xlsx")
    xlsx_files = glob.glob(pattern)
    seqs: List[int] = []

    for path in xlsx_files:
        basename = os.path.basename(path)
        if not basename.startswith("future_") or not basename.endswith(".xlsx"):
            continue
        try:
            num = int(basename.replace("future_", "").replace(".xlsx", ""))
            seqs.append(num)
        except:
            pass

    return sorted(seqs)

def excel_to_csv_and_upload(excel_file: str, csv_file: str, delete_local: bool = True):
    """
    1ã¤ã® Excel (future_N.xlsx) ã‚’ CSV (future_N.csv) ã«å¤‰æ›ã—ã€S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚
    æˆåŠŸã—ãŸã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã®Excel/CSVã‚’å‰Šé™¤ï¼ˆdelete_local=Trueï¼‰ã€‚
    """
    df = pd.read_excel(excel_file)
    df.to_csv(csv_file, index=False)
    log(f"âœ… Excel->CSV: {os.path.basename(excel_file)} â†’ {os.path.basename(csv_file)}")

    key = f"{S3_PREFIX_FUTURE}{os.path.basename(csv_file)}"
    upload_file_to_s3(csv_file, S3_BUCKET_FUTURE, key)

    if delete_local:
        try:
            os.remove(excel_file)
            log(f"ğŸ—‘ ãƒ­ãƒ¼ã‚«ãƒ«Excelå‰Šé™¤: {os.path.basename(excel_file)}")
        except:
            pass
        try:
            os.remove(csv_file)
            log(f"ğŸ—‘ ãƒ­ãƒ¼ã‚«ãƒ«CSVå‰Šé™¤: {os.path.basename(csv_file)}")
        except:
            pass

def convert_all_future_excels_to_csv_and_upload(base_dir: str = SAVE_DIR_SCHEDULED):
    """
    base_dir é…ä¸‹ã® future_*.xlsx ã‚’ã™ã¹ã¦ CSV ã¸å¤‰æ›ã—ã€S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    seq_list_all = get_existing_future_xlsx_seqs(base_dir)
    if not seq_list_all:
        log("å¤‰æ›å¯¾è±¡ã® future_*.xlsx ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    for seq in seq_list_all:
        xlsx_path = os.path.join(base_dir, f"future_{seq}.xlsx")
        csv_path  = os.path.join(base_dir, f"future_{seq}.csv")
        if not os.path.exists(xlsx_path):
            continue
        excel_to_csv_and_upload(xlsx_path, csv_path, delete_local=True)

# ==========================================
# âœ… ECSèµ·å‹•ã‚¨ãƒ³ãƒˆãƒª
# ==========================================

def main():
    # S3ã®æœ€å¤§é€£ç•ªã‚’è¦‹ã¦æ¡ç•ªèµ·ç‚¹ã‚’æ±ºå®š
    init_serial_base_from_s3()

    os.makedirs(SAVE_DIR_SCHEDULED, exist_ok=True)

    matches = fetch_scheduled_matches(days=FUTURE_DAYS)
    log(f"ç·ä»¶æ•°: {len(matches)}")

    # ä»Šå›ã®å®Ÿè£…ã¯ append_scheduled_row_to_excel ã§é€æ¬¡æ›¸ã„ã¦ã„ã‚‹ãŸã‚ã€
    # save_scheduled_to_excel ã¯ãƒ­ã‚°ç”¨é€”/å°†æ¥æ‹¡å¼µç”¨ï¼ˆç¾çŠ¶ã¯ä½•ã‚‚ã—ãªã„ï¼‰
    save_scheduled_to_excel(matches, output_dir=SAVE_DIR_SCHEDULED)

    # ç”Ÿæˆã•ã‚ŒãŸ future_*.xlsx ã‚’CSVåŒ–ã—ã¦S3ã¸
    convert_all_future_excels_to_csv_and_upload(base_dir=SAVE_DIR_SCHEDULED)

if __name__ == "__main__":
    main()
